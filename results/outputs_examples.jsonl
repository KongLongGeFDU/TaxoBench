{"id": 1, "survey": "A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications", "pdfs": [{"title": "Decoding-Time Language Model Alignment with Multiple Objectives", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Alignment through Feedback/Inference-time Alignment/Decoding-Time Language Model Alignment with Multiple Objectives.pdf", "summary": {"paper_id": "survey-1_paper-0_Decoding-Time Language Model Alignment with Multiple Objectives", "core_task": {"text": "core task of the paper (unspecified)", "query_variants": ["core task of the paper (unspecified)", "primary objective of the study", "main research problem addressed in the work"]}, "contributions": [{"name": "Limited adaptability of language models to multiple objectives", "description": "Current alignment approaches like PPO and DPO optimize language models for only one reward function at a time, making them unable to flexibly adapt to different combinations of objectives that users may prefer in practice.", "author_claim_text": "Existing methods primarily focus on optimizing LMs for a single reward function, limiting their adaptability to varied objectives.", "source_hint": "Abstract"}, {"name": "Need for on-the-fly adaptation to diverse user preferences", "description": "Different applications and users require different trade-offs among objectives (e.g., helpfulness vs. harmlessness in dialogue agents), necessitating methods that can dynamically adjust to various preference weightings without expensive retraining for each combination.", "author_claim_text": "In reality, different use cases and users may prefer different weightings of various alignment objectives. Therefore, there is a growing need for methods of adapting LMs on-the-fly toward different combinations of objectives.", "source_hint": "Introduction paragraph 2"}, {"name": "Multi-objective decoding (MOD) algorithm", "description": "MOD is a training-free decoding algorithm that combines predictions from multiple base models (each trained for a single objective) at inference time. It uses Legendre transform to derive a closed-form solution for token-level decoding that optimally balances multiple objectives according to user-specified preference weightings.", "author_claim_text": "We propose multi-objective decoding (MOD), a decoding-time algorithm that outputs the next token from a linear combination of predictions of all base models, for any given weighting over different objectives.", "source_hint": "Abstract"}, {"name": "Closed-form solution via Legendre transform", "description": "The method leverages the mathematical structure of f-divergence regularized alignment objectives and applies Legendre transform from convex optimization to obtain an efficient closed-form decoding rule that combines base model predictions without requiring normalization factors.", "author_claim_text": "We exploit a common form among a family of f-divergence regularized alignment approaches (such as PPO, DPO, and their variants) to identify a closed-form solution by Legendre transform, and derive an efficient decoding strategy.", "source_hint": "Abstract"}]}, "abs": "Aligning language models (LMs) to human preferences has emerged as a critical pursuit, enabling these models to better serve diverse user needs. Existing methods primarily focus on optimizing LMs for a single reward function, limiting their adaptability to varied objectives. Here, we propose $\\textbf{multi-objective decoding (MOD)}$, a decoding-time algorithm that outputs the next token from a linear combination of predictions of all base models, for any given weightings over different objectives. We exploit a common form among a family of $f$-divergence regularized alignment approaches (such as PPO, DPO, and their variants) to identify a closed-form solution by Legendre transform, and derive an efficient decoding strategy. Theoretically, we show why existing approaches can be sub-optimal even in natural settings and obtain optimality guarantees for our method. Empirical results demonstrate the effectiveness of the algorithm. For example, compared to a parameter-merging baseline, MOD achieves 12.8% overall reward improvement when equally optimizing towards $3$ objectives. Moreover, we experiment with MOD on combining three fully-finetuned LLMs of different model sizes, each aimed at different objectives such as safety, coding, and general user preference. Unlike traditional methods that require careful curation of a mixture of datasets to achieve comprehensive improvement, we can quickly experiment with preference weightings using MOD to find the best combination of models. Our best combination reduces toxicity on Toxigen to nearly 0% and achieves 7.9--33.3% improvement across other three metrics ($\\textit{i.e.}$, Codex@1, GSM-COT, BBH-COT)."}, {"title": "PAD: Personalized Alignment of LLMs at Decoding-Time", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Alignment through Feedback/Inference-time Alignment/PAD: Personalized Alignment of LLMs at Decoding-Time.pdf", "summary": {"paper_id": "survey-1_paper-1_PAD: Personalized Alignment of LLMs at Decoding-Time", "core_task": {"text": "personalized alignment of large language models", "query_variants": ["personalized alignment of large language models", "customized tuning of LLMs to individual user preferences", "tailoring large language model behavior to specific user values"]}, "contributions": [{"name": "High computational cost and data demands of aligning LLMs with diverse personalized preferences", "description": "The paper addresses the challenge that existing alignment methods struggle to accommodate diverse personalized preferences across users due to the high computational costs and extensive data requirements needed for training separate policy models for each preference configuration.", "author_claim_text": "Aligning with personalized preferences, which vary significantly across cultural, educational, and political differences, poses a significant challenge due to the computational costs and data demands of traditional alignment methods.", "source_hint": "Abstract"}, {"name": "Need for real-time personalized alignment without retraining", "description": "The authors are motivated by the observation that user preferences vary substantially across cultural, educational, and political dimensions, and even for the same user across different scenarios. Current frameworks cannot efficiently handle this diversity without expensive retraining and data collection.", "author_claim_text": "users' preferences can diverge significantly based on their different cultures, educational backgrounds, religions, and political stands... there always exists a proportion of human preferences that cannot be unified by the general preference, also known as personalized preferences, which current alignment frameworks struggle to align with", "source_hint": "Introduction §1"}, {"name": "Personalized reward modeling strategy decoupling preferences from MDP dynamics", "description": "The authors introduce a reward modeling approach that separates the dynamics of text generation from personalized preferences by representing rewards as the inner product of state-action features and preference weights. This enables learning generalizable token-level rewards with a single model.", "author_claim_text": "we first propose a personalized reward modeling strategy, which decouples the text generation process (modeled as a Markov Decision Process) from personalized preferences, thereby enabling the acquisition of generalizable token-level personalized rewards", "source_hint": "Abstract and Introduction §1"}, {"name": "PAD algorithm for guided decoding with personalized rewards", "description": "The PAD algorithm performs inference-time alignment by using a personalized reward model to score candidate tokens at each generation step. These scores are combined with the base model's likelihood to guide token selection, dynamically tailoring outputs to personalized preferences without additional training.", "author_claim_text": "we then formulate a personalized reward model (PersRM). During decoding, the PersRM scores the base model's top-K predictions at each token generation step based on the current generation and personalized preferences. Finally, this score is combined with standard decoding likelihoods to adjust the base model's predictions.", "source_hint": "Abstract and Method §3.2"}]}, "abs": "Aligning with personalized preferences, which vary significantly across cultural, educational, and political differences, poses a significant challenge due to the computational costs and data demands of traditional alignment methods. In response, this paper presents Personalized Alignment at Decoding-time (PAD), a novel framework designed to align LLM outputs with diverse personalized preferences during the inference phase, eliminating the need for additional training. By introducing a unique personalized reward modeling strategy, this framework decouples the text generation process from personalized preferences, facilitating the generation of generalizable token-level personalized rewards. The PAD algorithm leverages these rewards to guide the decoding process, dynamically tailoring the base model's predictions to personalized preferences. Extensive experimental results demonstrate that PAD not only outperforms existing training-based alignment methods in terms of aligning with diverse preferences but also shows significant generalizability to preferences unseen during training and scalability across different base models. This work advances the capability of LLMs to meet user needs in real-time applications, presenting a substantial step forward in personalized LLM alignment."}, {"title": "MetaAligner: Towards Generalizable Multi-Objective Alignment of Language Models", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Alignment through Feedback/Inference-time Alignment/MetaAligner: Towards Generalizable Multi-Objective Alignment of Language Models.pdf", "summary": {"paper_id": "survey-1_paper-2_MetaAligner: Towards Generalizable Multi-Objective Alignment of Language Models", "core_task": {"text": "multi-objective preference alignment of language models", "query_variants": ["multi-objective preference alignment of language models", "aligning language models with multiple competing objectives", "preference-based alignment for language models under multiple objectives"]}, "contributions": [{"name": "Policy-dependent and non-generalizable multi-objective alignment", "description": "Current multi-objective alignment methods require retraining for each new policy model and cannot generalize to objectives not seen during training, limiting their practical applicability and scalability.", "author_claim_text": "existing methods are dependent on the policy model parameters, which require high-cost repetition of their alignment algorithms for each new policy model, and they cannot expand to unseen objectives due to their static alignment objectives", "source_hint": "Abstract"}, {"name": "Reducing training costs and enabling generalizable alignment", "description": "The authors are motivated by the need to reduce computational costs associated with aligning each new policy model and to enable flexible alignment to new objectives without retraining, addressing scalability challenges in modern foundation models.", "author_claim_text": "This paradigm inevitably brings two key limitations: (1) they require repetition of their high-cost alignment algorithms for each newly-introduced policy model, which is incompatible with the increasing sizes and fast iteration of current foundation models; (2) all target models are statically aligned on pre-determined objectives, with currently no efforts in expanding and evaluating their capabilities on unseen objectives", "source_hint": "Introduction §1"}, {"name": "Dynamic objectives reformulation algorithm", "description": "An algorithm that restructures alignment datasets by incorporating text descriptions of various objectives in a prompt-based manner, enabling the model to learn flexible alignment across different objective combinations during training.", "author_claim_text": "dynamic objectives reformulation algorithm reorganizes traditional alignment datasets to supervise the model on performing flexible alignment across different objectives", "source_hint": "Abstract"}, {"name": "Conditional weak-to-strong correction paradigm", "description": "A training paradigm that stacks MetaAligner on top of frozen policy models to correct their outputs toward higher-preference responses, using a three-step process (warming up, equal-preference alignment, contrastive-preference alignment) that enables policy-agnostic alignment without accessing model parameters.", "author_claim_text": "conditional weak-to-strong correction paradigm aligns the weak outputs of fixed policy models to approach strong outputs with higher preferences in the corresponding alignment objectives, enabling plug-and-play inferences on any policy models", "source_hint": "Abstract"}]}, "abs": "Recent advancements in large language models (LLMs) focus on aligning to heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are dependent on the policy model parameters, which require high-cost repetition of their alignment algorithms for each new policy model, and they cannot expand to unseen objectives due to their static alignment objectives. In this work, we propose Meta-Objective Aligner (MetaAligner), the first policy-agnostic and generalizable method for multi-objective preference alignment. MetaAligner models multi-objective alignment into three stages: (1) dynamic objectives reformulation algorithm reorganizes traditional alignment datasets to supervise the model on performing flexible alignment across different objectives; (2) conditional weak-to-strong correction paradigm aligns the weak outputs of fixed policy models to approach strong outputs with higher preferences in the corresponding alignment objectives, enabling plug-and-play inferences on any policy models, which significantly reduces training costs and facilitates alignment on close-source policy models; (3) generalizable inference method flexibly adjusts target objectives by updating their text descriptions in the prompts, facilitating generalizable alignment to unseen objectives. Experimental results show that MetaAligner achieves significant and balanced improvements in multi-objective alignments on 10 state-of-the-art policy models, and saves up to 93.63% of GPU training hours compared to previous alignment methods. The model also effectively aligns unseen objectives, marking the first step towards generalizable multi-objective preference alignment."}, {"title": "Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Alignment through Feedback/Multi-objective Optimization/Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment.pdf", "summary": {"paper_id": "survey-1_paper-3_Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment", "core_task": {"text": "controllable multi-objective alignment of large language models", "query_variants": ["controllable multi-objective alignment of large language models", "multi-goal preference tuning for LLMs with control", "steering large language models toward multiple alignment objectives"]}, "contributions": [{"name": "Alignment tax in multi-objective LLM alignment", "description": "The paper addresses the challenge that optimizing large language models for multiple alignment objectives (helpfulness, honesty, harmlessness) simultaneously leads to performance trade-offs, where improving one objective degrades others. Existing alignment techniques are unidirectional and fail to balance these conflicting goals effectively.", "author_claim_text": "the multifaceted nature of human preferences inadvertently introduces what is known as the 'alignment tax'–a compromise where enhancements in alignment within one objective (e.g., harmlessness) can diminish performance in others (e.g., helpfulness)", "source_hint": "Abstract"}, {"name": "Need for controllable multi-objective alignment", "description": "The authors are motivated by the observation that different contexts require different priority balances among alignment objectives. Rather than maximizing all objectives simultaneously, they propose that explicit user-specified preference conditions can guide models to generate contextually appropriate responses while mitigating inherent conflicts.", "author_claim_text": "we recognize controllability as the key to multi-objective alignment. We argue that you can't please all of the people all of the time. For instance, users may prioritize utility for problem-solving questions and concerning moralities for controversial questions.", "source_hint": "Introduction"}, {"name": "Controllable Preference Optimization (CPO) algorithm", "description": "CPO is a two-stage alignment method. CPSFT trains the model to generate responses conditioned on explicit preference tokens (e.g., <Helpfulness:5>). CDPO then optimizes preferences by comparing responses under value conditions using a conditional multi-preference reward, increasing probability of better responses while decreasing others.", "author_claim_text": "we propose a controllable preference optimization (CPO) algorithm, which consists of two stages: (1) controllable preference supervised fine-tuning (CPSFT) which augments the input with explicit preference conditions through preference tokens and (2) controllable direct preference optimization (CDPO)", "source_hint": "Introduction and Method Section 2.2"}, {"name": "Conditional multi-objective optimization formulation", "description": "The authors reformulate the alignment problem by introducing preference conditions into the optimization objective. For controlled objectives, the goal becomes minimizing the distance between model output and the specified condition; for uncontrolled objectives, the goal remains maximizing preference scores.", "author_claim_text": "we propose transforming human value alignment into a conditional multi-objective optimization problem, which is achieved by redefining the learning goal, Ti(θ), to incorporate explicit preference conditions", "source_hint": "Method Section 2.1"}]}, "abs": "Alignment in artificial intelligence pursues the consistency between model responses and human preferences as well as values. In practice, the multifaceted nature of human preferences inadvertently introduces what is known as the \"alignment tax\" -a compromise where enhancements in alignment within one objective (e.g.,harmlessness) can diminish performance in others (e.g.,helpfulness). However, existing alignment techniques are mostly unidirectional, leading to suboptimal trade-offs and poor flexibility over various objectives. To navigate this challenge, we argue the prominence of grounding LLMs with evident preferences. We introduce controllable preference optimization (CPO), which explicitly specifies preference scores for different objectives, thereby guiding the model to generate responses that meet the requirements. Our experimental analysis reveals that the aligned models can provide responses that match various preferences among the \"3H\" (helpfulness, honesty, harmlessness) desiderata. Furthermore, by introducing diverse data and alignment goals, we surpass baseline methods in aligning with single objectives, hence mitigating the impact of the alignment tax and achieving improvements in multi-objective alignment."}, {"title": "Topic-Level Bayesian Surprise and Serendipity for Recommender Systems", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Alignment through Feedback/Multi-objective Optimization/Topic-Level Bayesian Surprise and Serendipity for Recommender Systems.pdf", "summary": {"paper_id": "survey-1_paper-4_Topic-Level Bayesian Surprise and Serendipity for Recommender Systems", "core_task": {"text": "Serendipitous item recommendation using Bayesian surprise", "query_variants": ["Serendipitous item recommendation using Bayesian surprise", "Bayesian surprise for unexpected item recommendation", "Recommending serendipitous items via Bayesian surprise methods"]}, "contributions": [{"name": "Filter bubble creation by recommender systems optimizing solely for user rating history", "description": "The paper addresses the problem that recommender systems focusing exclusively on fitting user rating histories create filter bubbles, preventing users from experiencing items from novel or unseen categories. This is exacerbated by feedback loops between ranking models and user reactions.", "author_claim_text": "A recommender system that optimizes its recommendations solely to fit a user's history of ratings for consumed items can create a filter bubble, wherein the user does not get to experience items from novel, unseen categories.", "source_hint": "Abstract, first sentence"}, {"name": "Mitigating filter bubbles through serendipitous recommendations", "description": "The authors are motivated by the need to break filter bubbles by recommending serendipitous items—those that are both surprising and positively rated—rather than optimizing solely for diversity or relevance. This addresses the limitation that users become insulated from different topics or viewpoints.", "author_claim_text": "One approach to mitigate this undesired behavior is to recommend items with high potential for serendipity, namely surprising items that are likely to be highly rated.", "source_hint": "Abstract, second sentence; Introduction paragraph 2"}, {"name": "Content-based serendipity formulation using Bayesian surprise", "description": "The authors propose a content-based measure of serendipity defined as the product of an item's rating and its Bayesian surprise, computed as the KL divergence between posterior and prior user preference distributions. This post-factum measure captures both the surprising and valuable aspects of serendipity.", "author_claim_text": "we propose a content-based formulation of serendipity that is rooted in Bayesian surprise and use it to measure the serendipity of items after they are consumed and rated by the user.", "source_hint": "Abstract, third sentence; Section 2, Equation 1"}, {"name": "Collaborative filtering integration for serendipity recommendation", "description": "The method integrates the Bayesian surprise-based serendipity measure with a collaborative filtering component. This component identifies similar users who consumed serendipitous items in the past, enabling proactive recommendation of items with high serendipity potential for the target user.", "author_claim_text": "When coupled with a collaborative-filtering component that identifies similar users, this enables recommending items with high potential for serendipity.", "source_hint": "Abstract, fourth sentence; Section 2, Algorithm 1"}]}, "abs": "A recommender system that optimizes its recommendations solely to fit a user's history of ratings for consumed items can create a filter bubble, wherein the user does not get to experience items from novel, unseen categories. One approach to mitigate this undesired behavior is to recommend items with high potential for serendipity, namely surprising items that are likely to be highly rated. In this paper, we propose a content-based formulation of serendipity that is rooted in Bayesian surprise and use it to measure the serendipity of items after they are consumed and rated by the user. When coupled with a collaborative-filtering component that identifies similar users, this enables recommending items with high potential for serendipity. To facilitate the evaluation of topic-level models for surprise and serendipity, we introduce a dataset of book reading histories extracted from Goodreads, containing over 26 thousand users and close to 1.3 million books, where we manually annotate 449 books read by 4 users in terms of their time-dependent, topic-level surprise. Experimental evaluations show that models that use Bayesian surprise correlate much better with the manual annotations of topic-level surprise than distance-based heuristics, and also obtain better serendipitous item recommendation performance."}, {"title": "Eliciting User Preferences for Personalized Multi-Objective Decision Making through Comparative Feedback", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Alignment through Feedback/Multi-objective Optimization/Eliciting User Preferences for Personalized Multi-Objective Decision Making through Comparative Feedback.pdf", "summary": {"paper_id": "survey-1_paper-5_Eliciting User Preferences for Personalized Multi-Objective Decision Making through Comparative Feedback", "core_task": {"text": "Eliciting user preferences for personalized multi-objective decision making", "query_variants": ["Eliciting user preferences for personalized multi-objective decision making", "Extracting individual user preferences for multi-criteria personalized decision support", "Learning user preference models for personalized multi-goal optimization"]}, "contributions": [{"name": "Personalized multi-objective decision making with unknown user preferences", "description": "The paper addresses the challenge of finding optimal personalized policies in multi-objective decision making when user preferences over objectives are unknown and must be learned through interaction. Unlike prior work that assumes known preferences or focuses on Pareto-optimal sets, this work tackles the problem of eliciting unknown preferences efficiently.", "author_claim_text": "Our model consists of a known Markov decision process with a vector-valued reward function, with each user having an unknown preference vector that expresses the relative importance of each objective. The goal is to efficiently compute a near-optimal policy for a given user.", "source_hint": "Abstract"}, {"name": "Practical need for intuitive preference elicitation in personalized systems", "description": "The authors are motivated by the practical difficulty users face in explicitly defining fine-grained preferences over multiple objectives. They argue that comparative feedback (comparing two policies) is more natural and intuitive than providing numerical scores or stochastic feedback on single policies, making it more suitable for real-world personalized systems like navigation or recommendation.", "author_claim_text": "Since the preferences are unknown, we need to elicit users' preferences by requesting feedback on selected policies. The problem of eliciting preferences has been studied in Wang et al. (2022) using a strong query model that provides stochastic feedback on the quality of a single policy. In contrast, our work focuses on a more natural and intuitive query model, comparison queries", "source_hint": "Introduction, paragraph 2-3"}, {"name": "Preference learning via policy comparison queries with basis policies", "description": "The method identifies a set of basis policies with linearly independent values, estimates ratios between personalized values using binary search with comparison queries, and solves a linear program to approximate the user's preference vector. This enables finding a near-optimal personalized policy with query complexity logarithmic in precision.", "author_claim_text": "We suggest an algorithm that finds a nearly optimal policy for the user using a number of comparison queries that scales quasilinearly in the number of objectives.", "source_hint": "Abstract and Section 3"}, {"name": "Weighted trajectory set representation for interpretable policies", "description": "The authors propose representing policies as small weighted sets of representative trajectories (at most k+1 trajectories) rather than explicit state-action mappings. This representation preserves multi-objective values while being more interpretable for users, and can be computed efficiently using expanding-compressing or flow decomposition approaches.", "author_claim_text": "We design an algorithm that given an explicit policy representation (as a mapping from states to distributions over actions), returns a weighted set of trajectories of size at most k+1, such that its expected return is identical to the value of the policy.", "source_hint": "Introduction and Section 4"}]}, "abs": "In classic reinforcement learning (RL) and decision making problems, policies are evaluated with respect to a scalar reward function, and all optimal policies are the same with regards to their expected return. However, many real-world problems involve balancing multiple, sometimes conflicting, objectives whose relative priority will vary according to the preferences of each user. Consequently, a policy that is optimal for one user might be sub-optimal for another. In this work, we propose a multi-objective decision making framework that accommodates different user preferences over objectives, where preferences are learned via policy comparisons. Our model consists of a Markov decision process with a vector-valued reward function, with each user having an unknown preference vector that expresses the relative importance of each objective. The goal is to efficiently compute a near-optimal policy for a given user. We consider two user feedback models. We first address the case where a user is provided with two policies and returns their preferred policy as feedback. We then move to a different user feedback model, where a user is instead provided with two small weighted sets of representative trajectories and selects the preferred one. In both cases, we suggest an algorithm that finds a nearly optimal policy for the user using a small number of comparison queries."}, {"title": "Differentiation of Multi-objective Data-driven Decision Pipeline", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Alignment through Feedback/Multi-objective Optimization/Differentiation of Multi-objective Data-driven Decision Pipeline.pdf", "summary": {"paper_id": "survey-1_paper-6_Differentiation of Multi-objective Data-driven Decision Pipeline", "core_task": {"text": "multi-objective data-driven decision optimization", "query_variants": ["multi-objective data-driven decision optimization", "data-driven optimization for multiple objectives in decision making", "multi-criteria decision optimization using data-driven approaches"]}, "contributions": [{"name": "Misalignment between prediction and multi-objective optimization in data-driven decision problems", "description": "The paper addresses the challenge that traditional two-stage approaches, which separate prediction and optimization phases, lead to suboptimal outcomes in multi-objective data-driven problems because prediction models prioritize accuracy without considering the structure of downstream optimization problems, and the impact of predicted coefficients on decisions is non-uniform.", "author_claim_text": "Traditional two-stage methods independently apply a machine learning model to estimate problem coefficients, followed by invoking a solver to tackle the predicted optimization problem. The independent use of optimization solvers and prediction models may lead to suboptimal performance due to mismatches between their objectives.", "source_hint": "Abstract"}, {"name": "Extending decision-focused learning to multi-objective optimization problems", "description": "The authors are motivated by the fact that existing decision-focused learning methods concentrate on single-objective problems, while numerous real-world scenarios involve multi-objective data-driven optimization with conflicting objectives and unknown coefficients, necessitating specialized approaches that account for Pareto fronts and gradient conflicts.", "author_claim_text": "However, these methods have primarily focused on single-objective optimization problems, thus limiting their applicability. We aim to propose a multi-objective decision-focused approach to address this gap.", "source_hint": "Abstract and Introduction"}, {"name": "Multi-Objective Decision-Focused Learning (MoDFL) framework", "description": "The authors propose MoDFL, an end-to-end framework that integrates predictive models with multi-objective optimization problems for unified training. The framework employs differentiation via reparameterization and transforms multi-objective problems into single-objective ones using the weighted sum method to enable gradient-based training.", "author_claim_text": "we aim to propose a novel Multi-Objective Decision-Focused Learning (MoDFL) model to address these gaps. To the best of our knowledge, this represents the inaugural effort to apply the decision-focused paradigm to the realm of multi-objective optimization.", "source_hint": "Introduction and Section V"}, {"name": "Three novel loss functions for multi-objective decision-focused learning", "description": "The authors introduce three complementary loss functions: landscape loss (using sample rank maximum mean discrepancy to measure objective space distance), Pareto set loss (directly measuring distance between Pareto sets in solution space), and decision loss (evaluating decision quality of representative solutions obtained via weighted sum method).", "author_claim_text": "we propose a set of novel loss functions. These loss functions are designed to capture the discrepancies between predicted and true decision problems, considering solution space, objective space, and decision quality, named landscape loss, Pareto set loss, and decision loss, respectively.", "source_hint": "Abstract and Section V"}]}, "abs": "Real-world scenarios frequently involve multi-objective data-driven optimization problems, characterized by unknown problem coefficients and multiple conflicting objectives. Traditional two-stage methods independently apply a machine learning model to estimate problem coefficients, followed by invoking a solver to tackle the predicted optimization problem. The independent use of optimization solvers and prediction models may lead to suboptimal performance due to mismatches between their objectives. Recent efforts have focused on end-to-end training of predictive models that use decision loss derived from the downstream optimization problem. However, these methods have primarily focused on single-objective optimization problems, thus limiting their applicability. We aim to propose a multi-objective decision-focused approach to address this gap. In order to better align with the inherent properties of multi-objective optimization problems, we propose a set of novel loss functions. These loss functions are designed to capture the discrepancies between predicted and true decision problems, considering solution space, objective space, and decision quality, named landscape loss, Pareto set loss, and decision loss, respectively. Our experimental results demonstrate that our proposed method significantly outperforms traditional two-stage methods and most current decision-focused methods."}, {"title": "Conditional Language Policy: A General Framework for Steerable Multi-Objective Finetuning", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Alignment through Feedback/Multi-objective Optimization/Conditional Language Policy: A General Framework for Steerable Multi-Objective Finetuning.pdf", "summary": {"paper_id": "survey-1_paper-7_Conditional Language Policy: A General Framework for Steerable Multi-Objective Finetuning", "core_task": {"text": "steerable multi-objective finetuning of language models", "query_variants": ["steerable multi-objective finetuning of language models", "controllable multi-goal adaptation of LMs", "guided multiple objective tuning for language models"]}, "contributions": [{"name": "Steerable multi-objective finetuning of language models", "description": "The paper addresses the difficulty of training language models that can flexibly balance multiple, often conflicting objectives (such as creativity versus safety, or succinctness versus completeness) without requiring separate models for each trade-off configuration.", "author_claim_text": "A key challenge is to develop steerable language models that trade-off multiple (conflicting) objectives in a flexible and efficient manner.", "source_hint": "Abstract"}, {"name": "Pluralistic alignment and decision support", "description": "The authors are motivated by the need for pluralistic alignment that respects diverse human preferences and enables decision support systems that can generate multiple diverse outputs covering a wide space of interests, allowing users to personalize models to their preferences.", "author_claim_text": "the model still fails to capture pluralistic alignment since the weightings are fixed. A solution we explore in this paper is to frame pluralistic alignment as a multi-objective RL problem", "source_hint": "Introduction §1"}, {"name": "Conditional Language Policy (CLP) framework", "description": "CLP is a general framework that learns a set of parameters which can be conditioned on reward and KL weightings via parameter-averaging mechanisms. It employs multi-task training across a continuum of weightings to maximize the multi-objective finetuning objective, enabling flexible trade-offs at inference time without maintaining multiple models.", "author_claim_text": "This paper presents Conditional Language Policy (CLP), a general framework for finetuning language models on multiple objectives. Building on techniques from multi-task training and parameter-efficient finetuning, CLP learn steerable models", "source_hint": "Abstract and Introduction §1"}, {"name": "Parameter-based conditioning mechanism", "description": "The conditioning mechanism maintains multiple sets of parameters for different rewards and linearly combines them based on reward weightings, then mixes with reference parameters based on KL weightings. This produces a single conditioned language model for any given weighting without requiring multiple forward passes.", "author_claim_text": "We maintain (1) m sets of conditioned parameters indexed by S, and (2) one set of unconditioned parameters. To condition the S-part on (α,w), we linearly combine them conditioning parameters with weightings w", "source_hint": "Section 3.1"}]}, "abs": "Reward-based finetuning is crucial for aligning language policies with intended behaviors (e.g., creativity and safety). A key challenge is to develop steerable language models that trade-off multiple (conflicting) objectives in a flexible and efficient manner. This paper presents Conditional Language Policy (CLP), a general framework for finetuning language models on multiple objectives. Building on techniques from multi-task training and parameter-efficient finetuning, CLP learn steerable models that effectively trade-off conflicting objectives at inference time. Notably, this does not require training or maintaining multiple models to achieve different trade-offs between the objectives. Through extensive experiments and ablations on two summarization datasets, we show that CLP learns steerable language models that outperform and Pareto-dominate the existing approaches for multi-objective finetuning."}, {"title": "Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Alignment through Feedback/Training-time Alignment/Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging.pdf", "summary": {"paper_id": "survey-1_paper-8_Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging", "core_task": {"text": "Aligning large language models with personalized human preferences", "query_variants": ["Aligning large language models with personalized human preferences", "Customizing large language models to individual human preferences", "Adapting large language models for personalized human alignment"]}, "contributions": [{"name": "Suboptimal alignment of LLMs to individual preferences", "description": "The paper addresses the challenge of aligning large language models (LLMs) to diverse and potentially conflicting individual human preferences, which is not effectively handled by existing RLHF methods.", "author_claim_text": "While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language Models (LLMs) with general, aggregate human preferences, it is suboptimal for learning diverse, individual perspectives.", "source_hint": "Abstract"}, {"name": "Need for personalized alignment in LLMs", "description": "The authors are motivated by the need to provide users with control over the alignment of LLMs to their specific preferences, enabling more personalized and adaptable interactions.", "author_claim_text": "We aim to support a wider range of multifaceted preferences that are explicitly declared as desirable by the user—giving the user control over the facets of output text they want to see as well as the personal data they wish to reveal to the model.", "source_hint": "Introduction §1"}, {"name": "Reinforcement Learning from Personalized Human Feedback (RLPHF)", "description": "The authors propose RLPHF, a multi-objective reinforcement learning framework that aligns LLMs with diverse and potentially conflicting human preferences by treating alignment as a multi-objective optimization problem.", "author_claim_text": "In this work, we instead propose Reinforcement Learning from Personalized Human Feedback (RLPHF), a new, multi-objective formulation of the human preference alignment problem, where Large Language Models (LLMs) are trained to be efficiently aligned with a range of different, potentially personalized combinations of human preferences.", "source_hint": "Introduction §1"}, {"name": "PERSONALIZED SOUPS", "description": "The authors introduce PERSONALIZED SOUPS, a method that trains separate policy models for individual preferences and combines their parameters dynamically during inference, enabling efficient and scalable alignment.", "author_claim_text": "Next, we propose PERSONALIZED SOUPS, a method that circumvents simultaneously optimizing multiple preferences by first optimizing multiple policy models each with distinct preferences with Proximal Policy Optimization (PPO) and merging the parameters of the policy models whose preferences we want to composite together on the fly during inference.", "source_hint": "Introduction §1"}]}, "abs": "While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language Models (LLMs) with general, aggregate human preferences, it is suboptimal for learning diverse, individual perspectives. In this work, we study Reinforcement Learning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are aligned to multiple (sometimes conflicting) preferences by modeling alignment as a Multi-Objective Reinforcement Learning (MORL) problem. Compared to strong single-objective baselines, we show that we can achieve personalized alignment by decomposing preferences into multiple dimensions. These dimensions are defined based on personalizations that are declared as desirable by the user. In this work, we show that they can be efficiently trained independently in a distributed manner and combined effectively post-hoc through parameter merging. The code is available at this https URL."}, {"title": "AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Alignment through Feedback/Training-time Alignment/AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback.pdf", "summary": {"paper_id": "survey-1_paper-9_AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback", "core_task": {"text": "building adaptable modular knowledge agents through process feedback", "query_variants": ["building adaptable modular knowledge agents through process feedback", "constructing flexible compositional knowledge systems using procedural feedback signals", "developing modular adaptive agents for knowledge tasks via process-level feedback"]}, "contributions": [{"name": "Building adaptable knowledge agents with controllable reasoning and human supervision", "description": "Existing language agents struggle to simultaneously provide robust reasoning logic, adaptive mechanisms for specific environments, and amenable processes for human intervention through direct feedback, especially when built on less powerful open-source LLMs.", "author_claim_text": "few, if any, can fulfill all the required criteria due to their uncontrollable reasoning logic, static model capability, or sparse/missing feedback signals", "source_hint": "Introduction paragraph 2"}, {"name": "Need for agents with structured reasoning, adaptability, and human-steerable processes", "description": "The authors identify three critical desiderata for knowledge agents: robust reasoning logic for precise problem-solving pathways, adaptive mechanisms for environment-specific adjustment, and reasoning processes that allow human steering through direct process feedback rather than only outcome feedback.", "author_claim_text": "the agent should possess a robust reasoning logic about the task to solve individual problems with precise pathways. Secondly, the agent should maintain an adaptive mechanism to adjust to specific environments, rather than staying static. Thirdly, the reasoning process should be amenable to human interventions", "source_hint": "Introduction paragraph 1"}, {"name": "FSM-based reasoning logic with modular architecture", "description": "AMOR employs a finite state machine framework that decomposes complex tasks into disentangled modules (LLM modules and tool modules) with explicit inter-step dependencies, enabling structured thinking, skill disentanglement, and intervenable workflows for knowledge-intensive tasks.", "author_claim_text": "AMOR's reasoning logic is formalized as a finite state machine (FSM) that solves problems via a series of executions and transitions over a set of modules", "source_hint": "Introduction paragraph 4 and Section 3.1"}, {"name": "Two-stage fine-tuning strategy with warm-up and adaptation", "description": "The training approach consists of a warm-up stage that fine-tunes open-source LLMs on 50k examples from multiple public datasets to enable generalization, followed by an adaptation stage that tailors the agent to target domains through exploration, process feedback collection, and exploitation via further fine-tuning.", "author_claim_text": "we develop AMOR through two-stage fine-tuning: warm-up and adaptation. The former fine-tunes the LLM with examples automatically constructed from various public datasets, enabling AMOR to generalize across different knowledge environments, while the latter tailors AMOR to specific domains using process feedback", "source_hint": "Abstract and Introduction paragraph 5"}]}, "abs": "The notable success of large language models (LLMs) has sparked an upsurge in building language agents to complete various complex tasks. We present AMOR, an agent framework based on open-source LLMs, which reasons with external knowledge bases and adapts to specific domains through human supervision to the reasoning process. AMOR builds reasoning logic over a finite state machine (FSM) that solves problems through autonomous executions and transitions over disentangled modules. This allows humans to provide direct feedback to the individual modules, and thus naturally forms process supervision. Based on this reasoning and feedback framework, we develop AMOR through two-stage fine-tuning: warm-up and adaptation. The former fine-tunes the LLM with examples automatically constructed from various public datasets, enabling AMOR to generalize across different knowledge environments, while the latter tailors AMOR to specific domains using process feedback. Extensive experiments across multiple domains demonstrate the advantage of AMOR to strong baselines, thanks to its FSM-based reasoning and process feedback mechanism. The code and data are publicly available at \\url{this https URL}."}, {"title": "FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Alignment through Feedback/Training-time Alignment/FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning.pdf", "summary": {"paper_id": "survey-1_paper-10_FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning", "core_task": {"text": "federated fine-tuning of large language models", "query_variants": ["federated fine-tuning of large language models", "distributed adaptation of large-scale language models", "collaborative tuning of LLMs across decentralized clients"]}, "contributions": [{"name": "Inadequate support for fine-tuning LLMs in federated learning settings", "description": "Existing federated learning frameworks do not adequately support fine-tuning large language models due to challenges in managing high communication and computational costs, diverse data preparation requirements across tasks, and varying information protection needs. This gap prevents effective collaborative training of LLMs across multiple entities with privacy constraints.", "author_claim_text": "fine-tuning LLMs in federated learning settings still lacks adequate support from the existing FL frameworks because it has to deal with optimizing the consumption of significant communication and computational resources, various data preparation for different tasks, and distinct information protection demands", "source_hint": "Abstract"}, {"name": "Enabling collaborative LLM fine-tuning without direct data sharing", "description": "The authors are motivated by the need to allow multiple entities to improve LLM performance on domain-specific tasks using their private data without direct sharing. Federated learning provides a solution that respects privacy regulations while enabling collaborative model improvement and can achieve rigorous data privacy protection and model customization.", "author_claim_text": "When several entities have similar interested tasks, but their local data cannot be shared directly because of privacy concerns regulations, federated learning (FL) is a mainstream solution to leverage the data of different entities", "source_hint": "Abstract"}, {"name": "Complete end-to-end benchmarking pipeline for federated LLM fine-tuning", "description": "The authors develop an automated pipeline that handles dataset preprocessing, execution or simulation of federated fine-tuning, and performance evaluation. This pipeline includes diverse federated datasets from various domains with tunable heterogeneity levels and corresponding evaluation tasks to benchmark algorithms under different FL scenarios.", "author_claim_text": "we build a complete end-to-end benchmarking pipeline, automizing the processes of dataset preprocessing, federated fine-tuning execution or simulation, and performance evaluation on federated LLM fine-tuning", "source_hint": "Abstract, Introduction"}, {"name": "Comprehensive federated parameter-efficient fine-tuning algorithm implementations", "description": "The package provides ready-to-use implementations of PEFT algorithms (LoRA, prefix-tuning, P-tuning, prompt tuning) adapted for federated settings, along with flexible programming interfaces. These enable fine-tuning LLMs with low communication and computation costs, even supporting scenarios where clients cannot access the full model such as closed-source LLMs.", "author_claim_text": "we provide comprehensive and off-the-shelf federated parameter-efficient fine-tuning (PEFT) algorithm implementations and versatile programming interfaces for future extension", "source_hint": "Abstract, Introduction"}]}, "abs": "LLMs have demonstrated great capabilities in various NLP tasks. Different entities can further improve the performance of those LLMs on their specific downstream tasks by fine-tuning LLMs. When several entities have similar interested tasks, but their data cannot be shared because of privacy concerns regulations, federated learning (FL) is a mainstream solution to leverage the data of different entities. However, fine-tuning LLMs in federated learning settings still lacks adequate support from existing FL frameworks because it has to deal with optimizing the consumption of significant communication and computational resources, data preparation for different tasks, and distinct information protection demands. This paper first discusses these challenges of federated fine-tuning LLMs, and introduces our package FS-LLM as a main contribution, which consists of the following components: (1) we build an end-to-end benchmarking pipeline, automizing the processes of dataset preprocessing, federated fine-tuning execution, and performance evaluation on federated LLM fine-tuning; (2) we provide comprehensive federated parameter-efficient fine-tuning algorithm implementations and versatile programming interfaces for future extension in FL scenarios with low communication and computation costs, even without accessing the full model; (3) we adopt several accelerating and resource-efficient operators for fine-tuning LLMs with limited resources and the flexible pluggable sub-routines for interdisciplinary study. We conduct extensive experiments to validate the effectiveness of FS-LLM and benchmark advanced LLMs with state-of-the-art parameter-efficient fine-tuning algorithms in FL settings, which also yields valuable insights into federated fine-tuning LLMs for the research community. To facilitate further research and adoption, we release FS-LLM at this https URL."}, {"title": "FedRLHF: A Convergence-Guaranteed Federated Framework for Privacy-Preserving and Personalized RLHF", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Alignment through Feedback/Training-time Alignment/FedRLHF: A Convergence-Guaranteed Federated Framework for Privacy-Preserving and Personalized RLHF.pdf", "summary": {"paper_id": "survey-1_paper-11_FedRLHF: A Convergence-Guaranteed Federated Framework for Privacy-Preserving and Personalized RLHF", "core_task": {"text": "federated reinforcement learning with human feedback for privacy-preserving personalization", "query_variants": ["federated reinforcement learning with human feedback for privacy-preserving personalization", "decentralized RL from human preferences for private personalized systems", "distributed reinforcement learning using human feedback preserving user privacy and personalization"]}, "contributions": [{"name": "Privacy and personalization challenges in centralized RLHF", "description": "Centralized RLHF systems aggregate user data and feedback, creating privacy risks (e.g., violating HIPAA, GDPR) and limiting personalization, as a single global policy cannot effectively cater to diverse user preferences.", "author_claim_text": "traditional Reinforcement Learning with Human Feedback (RLHF) frameworks face significant challenges due to their reliance on centralized data", "source_hint": "Abstract"}, {"name": "Need for privacy-preserving and personalized RLHF", "description": "The authors are motivated by growing privacy regulations and the need for personalized AI systems that respect individual user preferences without centralizing sensitive data or feedback.", "author_claim_text": "In the era of increasing privacy concerns and demand for personalized experiences, traditional Reinforcement Learning with Human Feedback (RLHF) frameworks face significant challenges", "source_hint": "Abstract"}, {"name": "FedRLHF framework", "description": "FedRLHF decentralizes RLHF by enabling clients to perform local policy updates using private human feedback and intrinsic rewards, sharing only model updates (not raw data) with a central server for aggregation.", "author_claim_text": "We introduce Federated Reinforcement Learning with Human Feedback (FedRLHF), a novel framework that decentralizes the RLHF process", "source_hint": "Abstract"}, {"name": "Client-specific reward shaping with human feedback", "description": "Each client shapes its reward function as Rk = R0_k + λHk, where R0_k is intrinsic reward, Hk is local human feedback, and λ controls personalization, allowing diverse client preferences without sharing feedback.", "author_claim_text": "each client integrates human feedback locally into reward functions and updates their policies through personalized RLHF processes", "source_hint": "Abstract and Section 3.1"}]}, "abs": "In the era of increasing privacy concerns and demand for personalized experiences, traditional Reinforcement Learning with Human Feedback (RLHF) frameworks face significant challenges due to their reliance on centralized data. We introduce Federated Reinforcement Learning with Human Feedback (FedRLHF), a novel framework that decentralizes the RLHF process. FedRLHF enables collaborative policy learning across multiple clients without necessitating the sharing of raw data or human feedback, thereby ensuring robust privacy preservation. Leveraging federated reinforcement learning, each client integrates human feedback locally into their reward functions and updates their policies through personalized RLHF processes. We establish rigorous theoretical foundations for FedRLHF, providing convergence guarantees, and deriving sample complexity bounds that scale efficiently with the number of clients. Empirical evaluations on the MovieLens and IMDb datasets demonstrate that FedRLHF not only preserves user privacy but also achieves performance on par with centralized RLHF, while enhancing personalization across diverse client environments."}, {"title": "Collective Constitutional AI: Aligning a Language Model with Public Input", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Alignment through Feedback/Training-time Alignment/Collective Constitutional AI: Aligning a Language Model with Public Input.pdf", "summary": {"paper_id": "survey-1_paper-12_Collective Constitutional AI: Aligning a Language Model with Public Input", "core_task": {"text": "Aligning language models with public input through participatory constitutional principles", "query_variants": ["Aligning language models with public input through participatory constitutional principles", "Constitutional AI using collective feedback for language model alignment", "Participatory governance frameworks for aligning LLMs with community values"]}, "contributions": [{"name": "Lack of public input in language model alignment", "description": "The paper addresses the challenge that current language model development lacks systematic methods for incorporating public preferences and values into model behavior, with developers making normative decisions that affect diverse populations without their meaningful input.", "author_claim_text": "There is growing consensus that language model (LM) developers should not be the sole deciders of LM behavior, creating a need for methods that enable the broader public to collectively shape the behavior of LM systems that affect them.", "source_hint": "Abstract, first sentence"}, {"name": "Democratic legitimacy and harm mitigation in AI systems", "description": "The authors are motivated by the need to ensure AI systems reflect diverse societal values and to mitigate harms through collaboration with affected communities, particularly as language models become more widely deployed and impact varied populations.", "author_claim_text": "This question is increasingly urgent as LMs are deployed more broadly and in increasingly diverse contexts, making it more likely that varied risks and harms will manifest; anticipating and mitigating risks and harms is done most effectively in collaboration with affected communities.", "source_hint": "Introduction, paragraph 1"}, {"name": "Collective Constitutional AI (CCAI) framework", "description": "The authors propose CCAI, a multi-stage framework that combines the Polis deliberation platform to collect and aggregate public input into constitutional principles, then uses Constitutional AI training methods to fine-tune language models according to these collectively-sourced principles.", "author_claim_text": "To address this, we develop a method called Collective Constitutional AI (CCAI). CCAI is a multi-stage process for (1) sourcing and integrating public preferences into a 'constitution' using the Polis platform for online deliberation and (2) fine-tuning a language model to adhere to this set of preferences using Constitutional AI.", "source_hint": "Introduction, paragraph 2"}, {"name": "Group-aware consensus (GAC) for principle selection", "description": "The authors use a group-aware consensus metric to select constitutional principles that have broad support across different opinion groups, protecting against tyranny of the majority by favoring statements with cross-group agreement rather than those with strong support from only one group.", "author_claim_text": "We decided to choose the statements that had the highest group-aware consensus (GAC) as defined in Small et al. for inclusion in the final constitution. The idea of the GAC metric is to identify the statements that are favorably viewed across opinion groups (identified via clustering), such that statements that all groups tend to agree with are more popular than ones for which one small group strongly dissents.", "source_hint": "Methods, Section 3.3, Statement selection"}]}, "abs": "There is growing consensus that language model (LM) developers should not be the sole deciders of LM behavior, creating a need for methods that enable the broader public to collectively shape the behavior of LM systems that affect them. To address this need, we present Collective Constitutional AI (CCAI): a multi-stage process for sourcing and integrating public input into LMs-from identifying a target population to sourcing principles to training and evaluating a model. We demonstrate the real-world practicality of this approach by creating what is, to our knowledge, the first LM fine-tuned with collectively sourced public input and evaluating this model against a baseline model trained with established principles from a LM developer. Our quantitative evaluations demonstrate several benefits of our approach: the CCAI-trained model shows lower bias across nine social dimensions compared to the baseline model, while maintaining equivalent performance on language, math, and helpful-harmless evaluations. Qualitative comparisons of the models suggest that the models differ on the basis of their respective constitutions, e.g., when prompted with contentious topics, the CCAI-trained model tends to generate responses that reframe the matter positively instead of a refusal. These results demonstrate a promising, tractable pathway toward publicly informed development of language models."}, {"title": "PERSOMA: PERsonalized SOft ProMpt Adapter Architecture for Personalized Language Prompting", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Alignment through Feedback/Training-time Alignment/PERSOMA: PERsonalized SOft ProMpt Adapter Architecture for Personalized Language Prompting.pdf", "summary": {"paper_id": "survey-1_paper-13_PERSOMA: PERsonalized SOft ProMpt Adapter Architecture for Personalized Language Prompting", "core_task": {"text": "personalized language generation using user interaction history", "query_variants": ["personalized language generation using user interaction history", "user-specific text generation based on past interaction records", "tailored natural language production leveraging historical user engagement data"]}, "contributions": [{"name": "Ineffective personalization in LLMs with extensive user histories", "description": "Existing text-based prompting methods for personalizing large language models struggle with long user histories due to context length limitations, information loss in lengthy prompts, and distraction from irrelevant context, raising questions about the effectiveness of natural language prompting for personalization at scale.", "author_claim_text": "LLMs often forget information when faced with long prompts. LLMs tend to leave out information in the middle of the prompt. LLMs are also often distracted by irrelevant context when faced with large prompts.", "source_hint": "Introduction paragraph 4"}, {"name": "Need for efficient personalization with large-scale user histories", "description": "The authors are motivated by the need to build personalized natural language systems that can effectively leverage extensive user interaction histories to adapt to evolving preferences, addressing limitations in current LLM personalization approaches that rely on lengthy text prompts.", "author_claim_text": "Understanding the nuances of a user's extensive interaction history is key to building accurate and personalized natural language systems that can adapt to evolving user preferences.", "source_hint": "Abstract"}, {"name": "PERSOMA soft prompt adapter architecture", "description": "PERSOMA is a novel architecture that uses a soft prompt adapter network to compress and resample user historical interactions into compact expressive prompt embeddings. These embeddings are mapped to the LLM vocabulary space and concatenated with task prompts to steer a frozen LLM toward personalized outputs without sacrificing in-context learning capabilities.", "author_claim_text": "We introduce PERSOMA, Personalized Soft Prompt Adapter architecture. PERSOMA offers a novel approach to efficiently capture user history. It achieves this by resampling and compressing interactions as free form text into expressive soft prompt embeddings.", "source_hint": "Abstract and Introduction paragraph 5"}, {"name": "History encoder with SentenceT5 and adapter networks", "description": "The method employs SentenceT5 to encode natural language user interactions into embeddings, which are then processed by soft prompt adapter networks (including Perceiver, Transformer, or MLP variants) to generate personalized soft prompt tokens that jointly compress and resample the user history.", "author_claim_text": "We first encode each natural language user interaction using a SentenceT5 text embedding model, our history encoder. We then feed the set into our soft prompt adapter network to generate the set of personalized soft prompt tokens.", "source_hint": "Method section 3.1"}]}, "abs": "Understanding the nuances of a user's extensive interaction history is key to building accurate and personalized natural language systems that can adapt to evolving user preferences. To address this, we introduce PERSOMA, Personalized Soft Prompt Adapter architecture. Unlike previous personalized prompting methods for large language models, PERSOMA offers a novel approach to efficiently capture user history. It achieves this by resampling and compressing interactions as free form text into expressive soft prompt embeddings, building upon recent research utilizing embedding representations as input for LLMs. We rigorously validate our approach by evaluating various adapter architectures, first-stage sampling strategies, parameter-efficient tuning techniques like LoRA, and other personalization methods. Our results demonstrate PERSOMA's superior ability to handle large and complex user histories compared to existing embedding-based and text-prompt-based techniques."}, {"title": "FDLoRA: Personalized Federated Learning of Large Language Model via Dual LoRA Tuning", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Alignment through Feedback/Training-time Alignment/FDLoRA: Personalized Federated Learning of Large Language Model via Dual LoRA Tuning.pdf", "summary": {"paper_id": "survey-1_paper-14_FDLoRA: Personalized Federated Learning of Large Language Model via Dual LoRA Tuning", "core_task": {"text": "personalized federated learning of large language models", "query_variants": ["personalized federated learning of large language models", "customized collaborative training of LLMs across distributed clients", "federated adaptation of large-scale language models for individual users"]}, "contributions": [{"name": "Training LLMs for individual users with limited labeled data and high costs", "description": "The paper addresses the challenge of training large language models for individual users or clients when computational resources are limited and labeled data is scarce, making traditional training approaches prohibitively expensive and impractical.", "author_claim_text": "Large language models (LLMs) have emerged as important components across various fields, yet their training requires substantial computation resources and abundant labeled data. It poses a challenge to robustly training LLMs for individual users (clients).", "source_hint": "Abstract, paragraph 1"}, {"name": "Leveraging distributed private data while maintaining privacy and reducing costs", "description": "The authors are motivated by the need to collaboratively train models on distributed private data across clients while maintaining data privacy, improving individual client performance, and avoiding the high communication and computational costs associated with existing federated learning methods for LLMs.", "author_claim_text": "This enables FDLoRA to make effective use of private data distributed across different clients, thereby improving performance on the client without incurring high communication and computing costs.", "source_hint": "Abstract, final paragraph"}, {"name": "Dual LoRA modules for personalized and global knowledge", "description": "The framework initializes two separate LoRA modules on each client: a personalized LoRA module that captures client-specific knowledge from local data without sharing parameters, and a global LoRA module that uploads parameters to the server for aggregation to integrate knowledge across different clients.", "author_claim_text": "FDLoRA sets dual LoRA modules on each client to capture personalized and global knowledge, respectively, and only the global LoRA module uploads parameters to the central server to aggregate cross-client knowledge.", "source_hint": "Abstract, paragraph 2"}, {"name": "Adaptive fusion approach for combining dual LoRAs", "description": "The method employs a gradient-free optimization technique called AdaFusion to adaptively combine the parameters of both the personalized and global LoRA modules, dynamically adjusting fusion weights to enhance performance on each client's local task.", "author_claim_text": "Finally, an adaptive fusion approach is employed to combine the of the dual LoRAs.", "source_hint": "Abstract, paragraph 2; Section 3.5"}]}, "abs": "Large language models (LLMs) have emerged as important components across various fields, yet their training requires substantial computation resources and abundant labeled data. It poses a challenge to robustly training LLMs for individual users (clients). To tackle this challenge, the intuitive idea is to introduce federated learning (FL), which can collaboratively train models on distributed private data. However, existing methods suffer from the challenges of data heterogeneity, system heterogeneity, and model size, resulting in suboptimal performance and high costs. In this work, we proposed a variant of personalized federated learning (PFL) framework, namely FDLoRA, which allows the client to be a single device or a cluster and adopts low-rank adaptation (LoRA) tuning. FDLoRA sets dual LoRA modules on each client to capture personalized and global knowledge, respectively, and only the global LoRA module uploads parameters to the central server to aggregate cross-client knowledge. Finally, an adaptive fusion approach is employed to combine the parameters of the dual LoRAs. This enables FDLoRA to make effective use of private data distributed across different clients, thereby improving performance on the client without incurring high communication and computing costs. We conducted extensive experiments in two practice scenarios. The results demonstrate that FDLoRA outperforms six baselines in terms of performance, stability, robustness, computation cost, and communication cost."}, {"title": "Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Alignment through Feedback/Training-time Alignment/Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning.pdf", "summary": {"paper_id": "survey-1_paper-15_Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning", "core_task": {"text": "core task of the paper (unspecified)", "query_variants": ["core task of the paper (unspecified)", "primary objective of the study", "main research problem addressed in the work"]}, "contributions": [{"name": "Lack of model ownership and poor generalization to user behavior shifts in LLM personalization", "description": "Current LLM personalization methods rely on centralized models with prompt-based approaches, which limit users' ability to own and customize their models while raising privacy concerns. Additionally, these methods struggle to generalize when user behavior patterns shift or when retrieved history is not highly relevant to current queries.", "author_claim_text": "existing methods faced limitations due to a lack of model ownership, resulting in constrained customization and privacy issues, and often failed to capture complex, dynamic user behavior patterns", "source_hint": "Abstract"}, {"name": "Enabling democratized, privacy-preserving personalized LLMs with better adaptation to behavior changes", "description": "The authors are motivated to address the ownership and behavior shift challenges in LLM personalization. They aim to enable users to possess their own personalized models through lightweight PEFT modules, enhancing both privacy and the model's ability to adapt to evolving user preferences without heavy reliance on highly relevant retrieved examples.", "author_claim_text": "By plugging in personal PEFT parameters, users can own and use their LLMs individually. OPPU integrates parametric user knowledge in the personal PEFT parameters with non-parametric knowledge from retrieval and profiles, adapting LLMs to user behavior shifts", "source_hint": "Abstract and Introduction §1"}, {"name": "One PEFT Per User (OPPU) framework", "description": "OPPU equips each user with a personal PEFT module (e.g., LoRA) that is fine-tuned on the user's behavior history. This module encapsulates parametric user knowledge and can be plugged into base LLMs, allowing users to own their personalized models. The framework supports integration with non-parametric knowledge via retrieval and profile augmentation.", "author_claim_text": "we introduce One PEFT Per User (OPPU), employing personalized parameter-efficient fine-tuning (PEFT) modules to store user-specific behavior patterns and preferences", "source_hint": "Abstract and §3"}, {"name": "Integration of parametric and non-parametric user knowledge", "description": "The method combines user-specific patterns stored in PEFT parameters (parametric knowledge) with dynamically retrieved behavior history and textual user profiles (non-parametric knowledge). This hybrid approach enhances personalization by leveraging both learned representations and contextual information.", "author_claim_text": "OPPU integrates parametric user knowledge in the personal PEFT parameters with non-parametric knowledge from retrieval and profiles", "source_hint": "Abstract and §3"}]}, "abs": "Personalization in large language models (LLMs) is increasingly important, aiming to align the LLMs' interactions, content, and recommendations with individual user preferences. Recent advances have highlighted effective prompt design by enriching user queries with non-parametric knowledge through behavior history retrieval and textual profiles. However, these methods faced limitations due to a lack of model ownership, resulting in constrained customization and privacy issues, and often failed to capture complex, dynamic user behavior patterns. To address these shortcomings, we introduce One PEFT Per User (OPPU), employing personalized parameter-efficient fine-tuning (PEFT) modules to store user-specific behavior patterns and preferences. By plugging in personal PEFT parameters, users can own and use their LLMs individually. OPPU integrates parametric user knowledge in the personal PEFT parameters with non-parametric knowledge from retrieval and profiles, adapting LLMs to user behavior shifts. Experimental results demonstrate that OPPU significantly outperforms existing prompt-based methods across seven diverse tasks in the LaMP benchmark. Further studies reveal OPPU's enhanced capabilities in handling user behavior shifts, modeling users at different activity levels, maintaining robustness across various user history formats, and displaying versatility with different PEFT methods."}, {"title": "RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Alignment through Feedback/Training-time Alignment/RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation.pdf", "summary": {"paper_id": "survey-1_paper-16_RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation", "core_task": {"text": "reinforcement learning from heterogeneous human feedback", "query_variants": ["reinforcement learning from heterogeneous human feedback", "RL training using diverse human preference signals", "learning from varied human evaluative input in reinforcement learning"]}, "contributions": [{"name": "Heterogeneity in human preferences in RLHF", "description": "Existing RLHF methods assume homogeneous human preferences that can be captured by a single reward model, but this assumption fails when users have diverse preferences. The paper addresses the challenge of learning reward models from heterogeneous user preference data, where individual feedback may be insufficient and users may behave strategically.", "author_claim_text": "Most existing RLHF paradigms make the underlying assumption that human preferences are relatively homogeneous, and can be encoded by a single reward model. In this paper, we focus on addressing the issues due to the inherent heterogeneity in human preferences", "source_hint": "Abstract"}, {"name": "Aligning AI systems with diverse human values", "description": "The authors are motivated by the need to align increasingly powerful AI models with human preferences, particularly when the target user group is heterogeneous. Accurately approximating diverse user preferences is critical but becomes significantly more challenging with heterogeneous users.", "author_claim_text": "As AI models are becoming more powerful, there is greater emphasis on aligning their performance and priorities with the preferences of human users. In this context, reinforcement learning from human feedback (RLHF) has emerged as a promising approach", "source_hint": "Introduction"}, {"name": "Personalization via representation learning", "description": "The authors propose learning personalized reward models for each user by assuming individual reward functions share a common representation structure. They model each reward as the inner product of a shared representation and an individual parameter vector, leveraging data from all users to learn the representation while estimating individual parameters.", "author_claim_text": "For the former, we propose two approaches based on representation learning and clustering, respectively, for learning multiple reward models that trade-off the bias (due to preference heterogeneity) and variance (due to the use of fewer data for learning each model by personalization)", "source_hint": "Abstract and Introduction"}, {"name": "Personalization via clustering", "description": "The authors develop a clustering-based approach that groups users into clusters and learns a reward model for each cluster. This method addresses the bias-variance tradeoff by reducing variance through pooling data within clusters while managing bias from preference heterogeneity across clusters.", "author_claim_text": "The second approach segments user preferences into clusters and learns a reward model for each cluster. This approach is useful when individual reward functions might not be available due to insufficient data", "source_hint": "Introduction and Section 3.2"}]}, "abs": "Reinforcement learning from human feedback (RLHF) has been an effective technique for aligning AI systems with human values, with remarkable successes in fine-tuning large-language models recently. Most existing RLHF paradigms make the underlying assumption that human preferences are relatively homogeneous, and can be encoded by a single reward model. In this paper, we focus on addressing the issues due to the inherent heterogeneity in human preferences, as well as their potential strategic behavior in providing feedback. Specifically, we propose two frameworks to address heterogeneous human feedback in principled ways: personalization-based one and aggregation-based one. For the former, we propose two approaches based on representation learning and clustering, respectively, for learning multiple reward models that trades off the bias (due to preference heterogeneity) and variance (due to the use of fewer data for learning each model by personalization). We then establish sample complexity guarantees for both approaches. For the latter, we aim to adhere to the single-model framework, as already deployed in the current RLHF paradigm, by carefully aggregating diverse and truthful preferences from humans. We propose two approaches based on reward and preference aggregation, respectively: the former utilizes both utilitarianism and Leximin approaches to aggregate individual reward models, with sample complexity guarantees; the latter directly aggregates the human feedback in the form of probabilistic opinions. Under the probabilistic-opinion-feedback model, we also develop an approach to handle strategic human labelers who may bias and manipulate the aggregated preferences with untruthful feedback. Based on the ideas in mechanism design, our approach ensures truthful preference reporting, with the induced aggregation rule maximizing social welfare functions."}, {"title": "MACRec: a Multi-Agent Collaboration Framework for Recommendation", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Personalized Generation and Rewarding/Agent-based/MACRec: a Multi-Agent Collaboration Framework for Recommendation.pdf", "summary": {"paper_id": "survey-1_paper-17_MACRec: a Multi-Agent Collaboration Framework for Recommendation", "core_task": {"text": "multi-agent collaboration for recommendation systems", "query_variants": ["multi-agent collaboration for recommendation systems", "collaborative multi-agent approaches in recommender systems", "recommendation systems using multiple collaborative agents"]}, "contributions": [{"name": "Gap in leveraging multi-agent collaboration for recommendation tasks", "description": "Existing work primarily uses agents for simulating user or item behaviors rather than directly tackling recommendation tasks. Single-agent approaches fail to handle complex recommendation scenarios effectively, and prior multi-agent frameworks have limited agent types and fixed collaboration modes.", "author_claim_text": "Recognizing the current gap in leveraging agent capabilities for multi-agent collaboration in recommendation systems", "source_hint": "Abstract"}, {"name": "Unleashing multi-agent collaboration potential for complex recommendation tasks", "description": "Multi-agent collaboration, mirroring human workflows, is believed to accomplish complex recommendation tasks better through collective intelligence. The authors aim to harness diverse agent capabilities to build recommenders directly rather than just simulating behaviors.", "author_claim_text": "To better unleash the potential of multi-agent collaboration for recommendation tasks", "source_hint": "Introduction paragraph 3"}, {"name": "MACRec multi-agent collaboration framework with specialized agents", "description": "MACRec provides customizable agents with different roles powered by LLMs and tools, including Manager (plans and manages task execution), Reflector (reflects on errors), User/Item Analysts (analyze characteristics), Searcher (searches information), and Task Interpreter (translates dialogs into tasks). These agents work collaboratively to tackle specific recommendation tasks.", "author_claim_text": "we introduce MACRec, a novel framework designed to enhance recommendation systems through multi-agent collaboration", "source_hint": "Abstract and Introduction paragraph 3"}, {"name": "Application examples on diverse recommendation scenarios", "description": "The framework demonstrates how to select and customize agents for different recommendation tasks. Each scenario requires different agent combinations: rating prediction uses User and Item Analysts, sequential recommendation adds Reflector, explanation generation includes Searcher, and conversational recommendation employs Task Interpreter.", "author_claim_text": "we provide application examples of how developers can easily use MACRec on various recommendation tasks, including rating prediction, sequential recommendation, conversational recommendation, and explanation generation", "source_hint": "Abstract and Section 4"}]}, "abs": "LLM-based agents have gained considerable attention for their decision-making skills and ability to handle complex tasks. Recognizing the current gap in leveraging agent capabilities for multi-agent collaboration in recommendation systems, we introduce MACRec, a novel framework designed to enhance recommendation systems through multi-agent collaboration. Unlike existing work on using agents for user/item simulation, we aim to deploy multi-agents to tackle recommendation tasks directly. In our framework, recommendation tasks are addressed through the collaborative efforts of various specialized agents, including Manager, User/Item Analyst, Reflector, Searcher, and Task Interpreter, with different working flows. Furthermore, we provide application examples of how developers can easily use MACRec on various recommendation tasks, including rating prediction, sequential recommendation, conversational recommendation, and explanation generation of recommendation results. The framework and demonstration video are publicly available at this https URL."}, {"title": "JumpStarter: Human-AI Planning with Task-Structured Context Curation", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Personalized Generation and Rewarding/Agent-based/JumpStarter: Human-AI Planning with Task-Structured Context Curation.pdf", "summary": {"paper_id": "survey-1_paper-18_JumpStarter: Human-AI Planning with Task-Structured Context Curation", "core_task": {"text": "human-AI collaborative planning for complex personal goals", "query_variants": ["human-AI collaborative planning for complex personal goals", "joint human-agent goal planning for intricate personal objectives", "collaborative AI-human planning for multi-faceted individual goals"]}, "contributions": [{"name": "Challenges in human-AI planning with current LLMs", "description": "Current LLMs struggle to maintain context over long interactions for complex goal planning. Users must actively manage information through intensive prompt engineering, manually curating chat histories and resupplying information, resulting in high cognitive burden and generic outputs misaligned with prior interactions.", "author_claim_text": "Human-AI planning for complex goals remains challenging with current large language models (LLMs), which rely on linear chat histories and simplistic memory mechanisms. Despite advances in long-context prompting, users still manually manage information, leading to a high cognitive burden.", "source_hint": "Abstract"}, {"name": "Need for structured, transparent, and efficient human-AI planning", "description": "The authors are motivated by the need to reduce user burden in managing context while enabling personalized, goal-oriented planning. They aim to create systems where tasks and context are treated as dynamic, composable units rather than requiring manual curation through opaque memory mechanisms.", "author_claim_text": "To support complex, personalized workflows, LLM-based systems need mechanisms that treat both tasks and context as dynamic, composable units, enabling more modular, transparent, and efficient human-AI planning.", "source_hint": "Introduction paragraph 2"}, {"name": "Task-structured context curation framework", "description": "A framework that decomposes user goals into hierarchical actionable subtasks and scopes context to localized decision points. It enables finer-grained personalization and reuse through three core mechanisms: context elicitation (prompting for missing information), context selection (surfacing relevant prior inputs), and context reuse (incorporating approved drafts into future subtasks).", "author_claim_text": "we propose task-structured context curation, a novel framework that enables LLMs to collaborate with humans on complex goals by dynamically decomposing tasks to help users manage context. By decomposing a user's goal into a hierarchy of actionable subtasks, our system scopes context to localized decision points", "source_hint": "Abstract and Introduction paragraph 3"}, {"name": "JumpStarter system implementation", "description": "An interactive system that operationalizes task-structured context curation. It takes user goals, elicits context, decomposes goals into subtasks, helps users select relevant context, generates answer drafts, and supports iterative refinement. The system uses hierarchical task trees and enables task forking for parallel subtasks.", "author_claim_text": "We specifically operationalize this method in JumpStarter, a human-AI planning system that supports open-ended user goals through structured task planning and personalized output generation.", "source_hint": "Introduction paragraph 4"}]}, "abs": "Human-AI planning for complex goals remains challenging with current large language models (LLMs), which rely on linear chat histories and simplistic memory mechanisms. Despite advances in long-context prompting, users still manually manage information, leading to a high cognitive burden. Hence, we propose JumpStarter, a system that enables LLMs to collaborate with humans on complex goals by dynamically decomposing tasks to help users manage context. We specifically introduce task-structured context curation, a novel framework that breaks down a user's goal into a hierarchy of actionable subtasks, and scopes context to localized decision points, enabling finer-grained personalization and reuse. The framework is realized through three core mechanisms: context elicitation, selection, and reuse. We demonstrate that task-structured context curation significantly improves plan quality by 16% over ablations. Our user study shows that JumpStarter helped users generate plans with 79% higher quality compared to ChatGPT."}, {"title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Personalized Generation and Rewarding/Agent-based/TravelPlanner: A Benchmark for Real-World Planning with Language Agents.pdf", "summary": {"paper_id": "survey-1_paper-19_TravelPlanner: A Benchmark for Real-World Planning with Language Agents", "core_task": {"text": "real-world travel planning with language agents", "query_variants": ["real-world travel planning with language agents", "language agents for practical trip planning tasks", "natural language models applied to real-world itinerary generation"]}, "contributions": [{"name": "Limitations of language agents in complex real-world planning", "description": "The paper addresses whether current language agents powered by large language models can handle complex, multi-constraint planning tasks in realistic settings, which have been beyond the capabilities of earlier AI agents that operated in constrained environments.", "author_claim_text": "Are language agents capable of planning in more complex settings that are out of the reach of prior AI agents? Planning tasks of such complexity are out of the reach of prior AI agents.", "source_hint": "Abstract and Introduction"}, {"name": "Need for realistic evaluation of language agent planning capabilities", "description": "Existing evaluation methods for language agents focus on single objectives with fixed ground truths, failing to capture the complexity of real-world planning scenarios that involve multiple interdependent constraints and require strong agency for information gathering and decision-making.", "author_claim_text": "the complexity of the real-world implies that previous evaluation methods, which focus on single objective and fixed ground truths, may fall short of capturing the full scope of agents' capabilities.", "source_hint": "Introduction and Related Work §2.3"}, {"name": "TravelPlanner benchmark with sandbox environment and diverse tools", "description": "The authors introduce TravelPlanner, a benchmark centered on travel planning that includes a static sandbox environment with approximately four million data entries accessible through six tools, along with 1,225 diverse queries and human-annotated reference plans to evaluate language agents.", "author_claim_text": "we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans.", "source_hint": "Abstract and §3.1"}, {"name": "Multi-constraint evaluation framework with three constraint types", "description": "The benchmark evaluates agents across three constraint categories: environment constraints testing adaptability to dynamic conditions, commonsense constraints assessing implicit reasoning, and hard constraints measuring satisfaction of explicit user needs like budget and preferences.", "author_claim_text": "we include three types of constraints: Environment Constraints, Commonsense Constraints, and Hard Constraints.", "source_hint": "§3.2"}]}, "abs": "Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of 0.6%. Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints. However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress. TravelPlanner provides a challenging yet meaningful testbed for future language agents."}, {"title": "Prospect Personalized Recommendation on Large Language Model-based Agent Platform", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Personalized Generation and Rewarding/Agent-based/Prospect Personalized Recommendation on Large Language Model-based Agent Platform.pdf", "summary": {"paper_id": "survey-1_paper-20_Prospect Personalized Recommendation on Large Language Model-based Agent Platform", "core_task": {"text": "personalized recommendation on large language model-based agent platforms", "query_variants": ["personalized recommendation on large language model-based agent platforms", "tailored suggestions in LLM-driven agent systems", "customized recommendations for agent frameworks built on large language models"]}, "contributions": [{"name": "Adapting recommender systems to LLM-based Agent platforms", "description": "The paper addresses the challenge of redesigning recommender system infrastructure to support Agent-level information processing on LLM-based Agent platforms, which exhibit novel characteristics like interactivity, intelligence, and proactiveness that differ fundamentally from traditional passive items.", "author_claim_text": "The new kind of Agent-oriented information system, exemplified by GPTs, urges us to inspect the information system infrastructure to support Agent-level information processing and to adapt to the characteristics of Large Language Model (LLM)-based Agents, such as interactivity.", "source_hint": "Abstract"}, {"name": "Leveraging Agent properties for personalized information services", "description": "The authors are motivated by the opportunity to exploit the unique properties of LLM-based Agents—such as interactivity and proactiveness—to enable deeper collaboration between recommended items and recommender systems, thereby enhancing personalized information services beyond traditional user-recommender feedback loops.", "author_claim_text": "Such properties make it possible for Agent Items and Agent Recommender to collaborate and share user information, facilitating personalized information delivery.", "source_hint": "Abstract"}, {"name": "Rec4Agentverse paradigm with Agent Items and Agent Recommender", "description": "The authors propose Rec4Agentverse, a new recommendation paradigm where LLM-based Agents serve as recommended items (Agent Items) and an Agent Recommender provides personalized agent recommendations. This paradigm emphasizes collaboration and information exchange between Agent Items and Agent Recommender.", "author_claim_text": "we introduce a novel recommendation paradigm called Rec4Agentverse, comprised of Agent Items and Agent Recommender. Rec4Agentverse emphasizes the collaboration between Agent Items and Agent Recommender", "source_hint": "Abstract"}, {"name": "Three-stage evolution framework for Rec4Agentverse", "description": "The authors conceptualize the development of Rec4Agentverse into three progressive stages: User-Agent Interaction (users interact with Agent Items), Agent-Recommender Collaboration (Agent Items and Agent Recommender exchange information), and Agents Collaboration (multiple Agent Items collaborate to serve users).", "author_claim_text": "we prospect the evolution of Rec4Agentverse and conceptualize it into three stages based on the enhancement of the interaction and information exchange among Agent Items, Agent Recommender, and the user.", "source_hint": "Abstract"}]}, "abs": "The new kind of Agent-oriented information system, exemplified by GPTs, urges us to inspect the information system infrastructure to support Agent-level information processing and to adapt to the characteristics of Large Language Model (LLM)-based Agents, such as interactivity. In this work, we envisage the prospect of the recommender system on LLM-based Agent platforms and introduce a novel recommendation paradigm called Rec4Agentverse, comprised of Agent Items and Agent Recommender. Rec4Agentverse emphasizes the collaboration between Agent Items and Agent Recommender, thereby promoting personalized information services and enhancing the exchange of information beyond the traditional user-recommender feedback loop. Additionally, we prospect the evolution of Rec4Agentverse and conceptualize it into three stages based on the enhancement of the interaction and information exchange among Agent Items, Agent Recommender, and the user. A preliminary study involving several cases of Rec4Agentverse validates its significant potential for application. Lastly, we discuss potential issues and promising directions for future research."}, {"title": "User-LLM: Efficient LLM Contextualization with User Embeddings", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Personalized Generation and Rewarding/Encoding-based/User-LLM: Efficient LLM Contextualization with User Embeddings.pdf", "summary": {"paper_id": "survey-1_paper-21_User-LLM: Efficient LLM Contextualization with User Embeddings", "core_task": {"text": "LLM contextualization with user interaction embeddings", "query_variants": ["LLM contextualization with user interaction embeddings", "language model personalization using user engagement representations", "contextualizing large language models via user interaction encodings"]}, "contributions": [{"name": "Inefficient and incomplete incorporation of user timeline data into LLMs", "description": "Existing methods for integrating user interaction histories into LLMs rely on converting timelines into lengthy text descriptions, which creates computational overhead due to long context windows and fails to capture subtle behavioral patterns essential for personalization.", "author_claim_text": "effectively incorporating complex and potentially noisy user timeline data into LLMs remains a challenge. Current approaches often involve translating user timelines into text descriptions before feeding them to LLMs, which can be inefficient and may not fully capture the nuances of user behavior.", "source_hint": "Abstract"}, {"name": "Enabling personalized AI agents through efficient user understanding", "description": "The authors aim to leverage LLMs' capabilities to build personalized AI systems that can understand and respond to individual user needs by effectively incorporating rich user behavioral data from diverse interaction timelines.", "author_claim_text": "This success has sparked significant interest in their potential to power the next generation of personalized AI agent - systems that understand and respond to individual needs and preferences.", "source_hint": "Introduction §1"}, {"name": "USER-LLM framework with user embeddings and cross-attention integration", "description": "USER-LLM treats user timelines as a distinct modality, using a pretrained Transformer-based user encoder to generate compact embeddings from ID-based interaction sequences. These embeddings are integrated with LLMs via cross-attention layers, enabling dynamic adaptation to user contexts without lengthy text prompts.", "author_claim_text": "we propose USER-LLM, a novel framework that leverages user embeddings to directly contextualize LLMs with user history interactions. These embeddings, generated by a user encoder pretrained using self-supervised learning on diverse user interactions, capture latent user behaviors and interests as well as their evolution over time. We integrate these user embeddings with LLMs through cross-attention", "source_hint": "Abstract and Introduction §1"}, {"name": "Two-stage training framework with encoder pretraining and co-training strategies", "description": "The framework first pretrains the user encoder using self-supervised learning on user interaction sequences, then finetunes the integrated system with four flexible strategies (Full, Enc, LoRA, Proj) to align user embeddings with LLM text representations while preserving pretrained knowledge.", "author_claim_text": "we propose a two-stage training framework: user-encoder pretraining and encoder-LLM finetuning.", "source_hint": "Methodology §3.3"}]}, "abs": "Large language models (LLMs) have achieved remarkable success across various domains, but effectively incorporating complex and potentially noisy user timeline data into LLMs remains a challenge. Current approaches often involve translating user timelines into text descriptions before feeding them to LLMs, which can be inefficient and may not fully capture the nuances of user behavior. Inspired by how LLMs are effectively integrated with images through direct embeddings, we propose User-LLM, a novel framework that leverages user embeddings to directly contextualize LLMs with user history interactions. These embeddings, generated by a user encoder pretrained using self-supervised learning on diverse user interactions, capture latent user behaviors and interests as well as their evolution over time. We integrate these user embeddings with LLMs through cross-attention, enabling LLMs to dynamically adapt their responses based on the context of a user's past actions and preferences.\nOur approach achieves significant efficiency gains by representing user timelines directly as embeddings, leading to substantial inference speedups of up to 78.1X. Comprehensive experiments on MovieLens, Amazon Review, and Google Local Review datasets demonstrate that User-LLM outperforms text-prompt-based contextualization on tasks requiring deep user understanding, with improvements of up to 16.33%, particularly excelling on long sequences that capture subtle shifts in user behavior. Furthermore, the incorporation of Perceiver layers streamlines the integration between user encoders and LLMs, yielding additional computational savings."}, {"title": "Personalized Language Modeling from Personalized Human Feedback", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Personalized Generation and Rewarding/Encoding-based/Personalized Language Modeling from Personalized Human Feedback.pdf", "summary": {"paper_id": "survey-1_paper-22_Personalized Language Modeling from Personalized Human Feedback", "core_task": {"text": "Personalized language modeling from human feedback", "query_variants": ["Personalized language modeling from human feedback", "Language model customization using human preference signals", "Tailoring language models with user feedback"]}, "contributions": [{"name": "Vanilla RLHF assumes uniform user preferences, limiting personalization", "description": "Standard RLHF methods implicitly assume all users have identical preferences, which causes fine-tuned LLMs to fail at generating personalized responses when users exhibit diverse or conflicting preferences. This uniform preference assumption leads to majority-voting behavior that can misalign with minority user preferences.", "author_claim_text": "vanilla RLHF assumes that all human preferences share the same distribution, preventing fine-tuned LLMs from generating personalized content when user preferences are diverse", "source_hint": "Abstract"}, {"name": "Need for efficient personalized LLMs that scale with diverse user bases", "description": "The authors are motivated by the need to personalize LLMs for diverse user preferences without the computational burden of training separate models per user or preference dimension. Existing personalization methods are inefficient and inflexible, especially when user preferences cannot be fully articulated or enumerated.", "author_claim_text": "Personalizing LLMs to align with individual user preferences has become a key research topic. Recent endeavors often require training separate reward models or LLMs for each preference dimension, posing computational and storage challenges", "source_hint": "Introduction §1"}, {"name": "Personalized-RLHF (P-RLHF) framework with lightweight user model", "description": "P-RLHF introduces a general framework consisting of a lightweight user model that extracts user embeddings from user information and a base LLM. The user model captures both explicit preferences (from textual user information) and implicit preferences (from feedback data), enabling efficient personalization without training separate models per user.", "author_claim_text": "we propose Personalized-RLHF (P-RLHF), an efficient framework that utilizes a lightweight user model to capture individual user preferences and jointly learns the user model and the personalized LLM from human feedback", "source_hint": "Abstract"}, {"name": "User model designs encoding preference assumptions", "description": "The authors propose multiple user model architectures (uniform, individualized, cluster-based) that encode different structural assumptions about user preference relationships. These designs allow flexible modeling of implicit preferences learned from feedback data, with cluster-based models providing efficient low-rank approximations for large user bases.", "author_claim_text": "we structure fim_P to encode specific preference assumptions regarding how different users' preferences are related to each other", "source_hint": "Section 4.3"}]}, "abs": "Personalized large language models (LLMs) are designed to tailor responses to individual user preferences. While Reinforcement Learning from Human Feedback (RLHF) is a commonly used framework for aligning LLMs with human preferences, vanilla RLHF assumes that all human preferences share the same distribution, preventing fine-tuned LLMs from generating personalized content when user preferences are diverse. In this work, we propose Personalized-RLHF (P-RLHF), an efficient framework that utilizes a lightweight user model to capture individual user preferences and jointly learns the user model and the personalized LLM from human feedback. P-RLHF exhibits the following three characteristics: (1) It enables an LLM to generate personalized content and scale efficiently with growing number of users. (2) It handles both explicit user preferences described as textual input and implicit user preferences encoded in the feedback data. (3) It eliminates the need for users to fully articulate their preferences, which are normally needed for prompting LLMs to generate personalized content yet are often impractical to obtain in real-world scenarios. Our experimental results show that personalized LLMs trained using P-RLHF generate responses that are more closely aligned with individual user preferences, outperforming vanilla, non-personalized RLHF and prompting-based personalization approaches across different tasks. We opensource our code at this https URL."}, {"title": "Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Personalized Generation and Rewarding/Encoding-based/Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF.pdf", "summary": {"paper_id": "survey-1_paper-23_Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF", "core_task": {"text": "Preference learning from human feedback with hidden context", "query_variants": ["Preference learning from human feedback with hidden context", "Learning preferences from human feedback under latent contextual information", "Human feedback preference learning with unobserved context variables"]}, "contributions": [{"name": "Hidden context in preference learning affects RLHF outcomes", "description": "The paper addresses the challenge that preference learning methods, including RLHF, operate on data where important contextual information (such as annotator identity, cognitive states, or evaluation criteria) influences feedback but is not available to the learned model. This hidden context can lead to unexpected and undesirable outcomes.", "author_claim_text": "In practice, preference learning from human feedback depends on incomplete data with hidden context. Hidden context refers to data that affects the feedback received, but which is not represented in the data used to train a preference model.", "source_hint": "Abstract"}, {"name": "Understanding implicit aggregation and its consequences for safe AI deployment", "description": "The authors are motivated by the need to understand how standard preference learning methods implicitly aggregate over hidden contexts, as this aggregation can produce counter-intuitive results, create vulnerabilities (such as incentives for annotators to misreport preferences), and impact the safe deployment of systems trained with RLHF.", "author_claim_text": "Since these challenges are ubiquitous, understanding their implications is crucial for safely deploying RLHF-trained models.", "source_hint": "Introduction, Section 2"}, {"name": "Formal model of preference learning with hidden context", "description": "The authors extend the standard preference learning framework by introducing utility functions u(a,z) that depend on both observed alternatives a and hidden context z. This model encompasses diverse preferences, irrationality, partial observability, and multiple objectives as special cases of hidden context.", "author_claim_text": "In Section 2 we present a formal model of preference learning with hidden context. We show that our model can represent many challenges in preference learning, such as combining data from different users, accounting for irrationality, and optimizing for multiple objectives.", "source_hint": "Introduction, Section 2"}, {"name": "Distributional preference learning (DPL)", "description": "DPL estimates a distribution over utility values for each input rather than a single scalar, allowing the method to detect situations where hidden context influences preferences. The approach can identify hidden context through an explained variance metric and enable risk-averse decision-making to mitigate issues like jailbreaks.", "author_claim_text": "As a step towards mitigating these problems, we introduce a class of methods called distributional preference learning (DPL). DPL methods estimate a distribution of possible score values for each alternative in order to better account for hidden context.", "source_hint": "Abstract, Section 4"}]}, "abs": "In practice, preference learning from human feedback depends on incomplete data with hidden context. Hidden context refers to data that affects the feedback received, but which is not represented in the data used to train a preference model. This captures common issues of data collection, such as having human annotators with varied preferences, cognitive processes that result in seemingly irrational behavior, and combining data labeled according to different criteria. We prove that standard applications of preference learning, including reinforcement learning from human feedback (RLHF), implicitly aggregate over hidden contexts according to a well-known voting rule called Borda count. We show this can produce counter-intuitive results that are very different from other methods which implicitly aggregate via expected utility. Furthermore, our analysis formalizes the way that preference learning from users with diverse values tacitly implements a social choice function. A key implication of this result is that annotators have an incentive to misreport their preferences in order to influence the learned model, leading to vulnerabilities in the deployment of RLHF. As a step towards mitigating these problems, we introduce a class of methods called distributional preference learning (DPL). DPL methods estimate a distribution of possible score values for each alternative in order to better account for hidden context. Experimental results indicate that applying DPL to RLHF for LLM chatbots identifies hidden context in the data and significantly reduces subsequent jailbreak vulnerability. Our code and data are available at this https URL"}, {"title": "Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Personalized Generation and Rewarding/Encoding-based/Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning.pdf", "summary": {"paper_id": "survey-1_paper-24_Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning", "core_task": {"text": "Personalizing reinforcement learning from human feedback with diverse user preferences", "query_variants": ["Personalizing reinforcement learning from human feedback with diverse user preferences", "Adapting RLHF to heterogeneous human preference distributions", "Customizing human feedback reinforcement learning for varied user tastes"]}, "contributions": [{"name": "Inability of current RLHF to account for diverse individual preferences", "description": "Standard RLHF methods assume all users share a single utility function and use a unimodal BTL model, which fails when preferences are multi-modal due to diverse user populations. This model misspecification causes reward models to average divergent preferences, resulting in inaccurate rewards and policies that fail to satisfy any distinct user group.", "author_claim_text": "current RLHF techniques cannot account for the naturally occurring differences in individual human preferences across a diverse population. When these differences arise, traditional RLHF frameworks simply average over them, leading to inaccurate rewards and poor performance for individual subgroups.", "source_hint": "Abstract"}, {"name": "Need for pluralistic alignment to serve diverse populations", "description": "The authors are motivated by the need to democratize RLHF to account for diverse human values and preferences, particularly to avoid disadvantaging minority groups whose preferences would be ignored by majority-driven reward models. This is critical for applications ranging from LLMs to assistive robotics where users have fundamentally different preferences.", "author_claim_text": "To build safe and performant foundation models serving a diverse population, we need methods that can explicitly account for and adapt to the inherent plurality of human preferences.", "source_hint": "Introduction §1"}, {"name": "Variational Preference Learning (VPL) with latent variable formulation", "description": "VPL formulates RLHF as a latent variable problem where a latent z represents hidden user context. The method uses variational inference to learn an encoder that infers user-specific latent distributions from preference annotations, and a latent-conditional reward model that predicts rewards based on both state and latent. This is optimized via an evidence lower bound (ELBO) objective.", "author_claim_text": "we propose a method—Variational Preference Learning (VPL)—for multi-modal reward modeling. Intuitively, given a few preference annotations from a particular user, our approach uses a variational encoder to infer a latent distribution over hidden user context, and a latent conditional reward model", "source_hint": "Abstract and Introduction §1"}, {"name": "Scaled rewards via pairwise classification for multi-task learning", "description": "To address the problem that different latent values produce vastly different reward scales (which harms multi-user policy optimization), VPL replaces raw BTL rewards with expected preference likelihoods. This approach, termed VPL-SPO, ensures rewards are consistently scaled across different user latents by using probabilities normalized in the [0,1] range.", "author_claim_text": "an effective solution to the reward scaling issue is to replace the raw rewards from the BTL model with likelihoods suggested by the pairwise preference likelihood model. In particular, a natural choice of scaled rewards for a state is the expected likelihood that the state is 'preferred' to all other states", "source_hint": "Section 4.1"}]}, "abs": "Reinforcement Learning from Human Feedback (RLHF) is a powerful paradigm for aligning foundation models to human values and preferences. However, current RLHF techniques cannot account for the naturally occurring differences in individual human preferences across a diverse population. When these differences arise, traditional RLHF frameworks simply average over them, leading to inaccurate rewards and poor performance for individual subgroups. To address the need for pluralistic alignment, we develop a class of multimodal RLHF methods. Our proposed techniques are based on a latent variable formulation - inferring a novel user-specific latent and learning reward models and policies conditioned on this latent without additional user-specific data. While conceptually simple, we show that in practice, this reward modeling requires careful algorithmic considerations around model architecture and reward scaling. To empirically validate our proposed technique, we first show that it can provide a way to combat underspecification in simulated control problems, inferring and optimizing user-specific reward functions. Next, we conduct experiments on pluralistic language datasets representing diverse user preferences and demonstrate improved reward function accuracy. We additionally show the benefits of this probabilistic framework in terms of measuring uncertainty, and actively learning user preferences. This work enables learning from diverse populations of users with divergent preferences, an important challenge that naturally occurs in problems from robot learning to foundation model alignment."}, {"title": "Selective Prompting Tuning for Personalized Conversations with LLMs", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Personalized Generation and Rewarding/Parameter-based/Selective Prompting Tuning for Personalized Conversations with LLMs.pdf", "summary": {"paper_id": "survey-1_paper-25_Selective Prompting Tuning for Personalized Conversations with LLMs", "core_task": {"text": "personalized dialogue generation with large language models", "query_variants": ["personalized dialogue generation with large language models", "customized conversational response generation using large-scale language models", "user-specific dialogue synthesis with LLMs"]}, "contributions": [{"name": "Difficulty in personalizing LLMs for diverse conversational settings", "description": "The paper addresses the challenge of effectively personalizing large language models for conversations with diverse settings, including varied persona profiles and conversation lengths, where textual prompting struggles to generate responses aligned with ground truths and direct fine-tuning produces repetitive or generic replies.", "author_claim_text": "designing personalized conversational agents with LLMs faces two main challenges. The primary issue lies in diverse settings in conversations, which encompass a wide array of dialogues, each characterized by unique persona profiles and varying lengths of conversation.", "source_hint": "Introduction paragraph 3"}, {"name": "Overfitting to small-scale persona-based dialogue datasets", "description": "When LLMs are tuned on small datasets like PersonaChat, they risk overfitting to specific persona profiles, resulting in repetitive responses that are overly aligned with the persona at the cost of dynamic and contextually appropriate interactions.", "author_claim_text": "another equally critical challenge arises from the limitations inherent to the datasets used for persona-based dialogue generation. Typically small and lacking in diversity, these datasets can restrict the model's exposure to a wide range of conversational scenarios.", "source_hint": "Introduction paragraph 4"}, {"name": "Enhancing personalization and diversity in LLM-based dialogue systems", "description": "The authors are motivated by the need to create personalized dialogue systems that adapt conversations to individual preferences and contexts, fostering deeper user connections and more intuitive interactions, while addressing the limitations of current LLM approaches in achieving effective persona integration.", "author_claim_text": "In conversational AI, personalizing dialogues with persona profiles and contextual understanding is essential. Despite large language models' improved response coherence, effective persona integration remains a challenge.", "source_hint": "Abstract and Introduction paragraph 1"}, {"name": "Selective Prompt Tuning (SPT) framework", "description": "SPT is a framework that combines a soft prompt group with a trainable dense retriever to adaptively select appropriate soft prompts based on input contexts. The retriever is dynamically updated through feedback from the LLM, enabling the model to discover intrinsic patterns in personalized dialogues without explicit annotations.", "author_claim_text": "we propose Selective Prompt Tuning (SPT), which softly prompts LLMs for personalized conversations in a selective way. Concretely, SPT initializes a set of soft prompts and uses a trainable dense retriever to adaptively select suitable soft prompts for LLMs according to different input contexts", "source_hint": "Abstract and Introduction paragraph 5"}]}, "abs": "In conversational AI, personalizing dialogues with persona profiles and contextual understanding is essential. Despite large language models' (LLMs) improved response coherence, effective persona integration remains a challenge. In this work, we first study two common approaches for personalizing LLMs: textual prompting and direct fine-tuning. We observed that textual prompting often struggles to yield responses that are similar to the ground truths in datasets, while direct fine-tuning tends to produce repetitive or overly generic replies. To alleviate those issues, we propose \\textbf{S}elective \\textbf{P}rompt \\textbf{T}uning (SPT), which softly prompts LLMs for personalized conversations in a selective way. Concretely, SPT initializes a set of soft prompts and uses a trainable dense retriever to adaptively select suitable soft prompts for LLMs according to different input contexts, where the prompt retriever is dynamically updated through feedback from the LLMs. Additionally, we propose context-prompt contrastive learning and prompt fusion learning to encourage the SPT to enhance the diversity of personalized conversations. Experiments on the CONVAI2 dataset demonstrate that SPT significantly enhances response diversity by up to 90\\%, along with improvements in other critical performance indicators. Those results highlight the efficacy of SPT in fostering engaging and personalized dialogue generation. The SPT model code (this https URL) is publicly available for further exploration."}, {"title": "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Personalized Generation and Rewarding/Parameter-based/Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction.pdf", "summary": {"paper_id": "survey-1_paper-26_Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction", "core_task": {"text": "user rating prediction based on historical interactions", "query_variants": ["user rating prediction based on historical interactions", "predicting user ratings from past interaction history", "forecasting user preferences using historical engagement data"]}, "contributions": [{"name": "Understanding user preferences with LLMs for rating prediction", "description": "The paper addresses the open question of whether large language models can effectively understand and predict user preferences based on historical behavior, specifically in the context of user rating prediction tasks.", "author_claim_text": "the extent to which LLMs can comprehend user preferences based on their previous behavior remains an emerging and still unclear research question", "source_hint": "Abstract"}, {"name": "Comparing LLMs with traditional collaborative filtering methods", "description": "The authors are motivated to investigate whether LLMs, which require less data and possess extensive world knowledge, can compete with traditional collaborative filtering methods that rely heavily on large volumes of rating data.", "author_claim_text": "Traditionally, Collaborative Filtering (CF) has been the most effective method for these tasks, predominantly relying on the extensive volume of rating data. In contrast, LLMs typically demand considerably less data while maintaining an exhaustive world knowledge", "source_hint": "Abstract"}, {"name": "Zero-shot and few-shot prompting for rating prediction", "description": "The authors design prompts that encode user rating history and item metadata as text, enabling LLMs to predict ratings in zero-shot and few-shot settings without task-specific training.", "author_claim_text": "We demonstrate the zero-shot and few-shot prompts used for the rating prediction task... the input prompts depict several important features represented as text, including user's past rating history and candidate item features", "source_hint": "Method §3.2"}, {"name": "Fine-tuning LLMs with classification and regression objectives", "description": "The authors propose two fine-tuning approaches for LLMs on rating prediction: treating it as multi-class classification with cross-entropy loss or as regression with MSE loss, adapting the model's output layer accordingly.", "author_claim_text": "we formulate the rating regression task as a 5-way classification task... During training, we use the cross-entropy loss... we apply a mean-squared-error (MSE) loss", "source_hint": "Method §3.3"}]}, "abs": "Large Language Models (LLMs) have demonstrated exceptional capabilities in generalizing to new tasks in a zero-shot or few-shot manner. However, the extent to which LLMs can comprehend user preferences based on their previous behavior remains an emerging and still unclear research question. Traditionally, Collaborative Filtering (CF) has been the most effective method for these tasks, predominantly relying on the extensive volume of rating data. In contrast, LLMs typically demand considerably less data while maintaining an exhaustive world knowledge about each item, such as movies or products. In this paper, we conduct a thorough examination of both CF and LLMs within the classic task of user rating prediction, which involves predicting a user's rating for a candidate item based on their past ratings. We investigate various LLMs in different sizes, ranging from 250M to 540B parameters and evaluate their performance in zero-shot, few-shot, and fine-tuning scenarios. We conduct comprehensive analysis to compare between LLMs and strong CF methods, and find that zero-shot LLMs lag behind traditional recommender models that have the access to user interaction data, indicating the importance of user interaction data. However, through fine-tuning, LLMs achieve comparable or even better performance with only a small fraction of the training data, demonstrating their potential through data efficiency."}, {"title": "P-React: Synthesizing Topic-Adaptive Reactions of Personality Traits via Mixture of Specialized LoRA Experts", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Personalized Generation and Rewarding/Parameter-based/P-React: Synthesizing Topic-Adaptive Reactions of Personality Traits via Mixture of Specialized LoRA Experts.pdf", "summary": {"paper_id": "survey-1_paper-27_P-React: Synthesizing Topic-Adaptive Reactions of Personality Traits via Mixture of Specialized LoRA Experts", "core_task": {"text": "Modeling personality traits in large language models", "query_variants": ["Modeling personality traits in large language models", "Representing personality characteristics within LLMs", "Personality trait representation in large-scale language models"]}, "contributions": [{"name": "Lack of psychological grounding in personalized LLMs", "description": "Current personalized large language models focus on surface-level character attributes (name, skill, experience) rather than deeper psychological personality traits that actually drive human behavior and decision-making, limiting their authenticity and psychological realism.", "author_claim_text": "existing works primarily focus on modeling explicit character profiles, while ignoring the underlying personality traits that truly shape behaviors and decision-making, hampering the development of more anthropomorphic and psychologically-grounded AI systems", "source_hint": "Abstract"}, {"name": "Need for consistent and realistic personality traits in AI applications", "description": "The authors are motivated by applications like psychological counseling and digital tutoring that demand consistent, realistic personality simulation. Additionally, the scarcity of large-scale, high-quality dialogue datasets with personality annotations has hindered progress in this area.", "author_claim_text": "This limitation becomes particularly evident in applications requiring consistent and realistic personality traits, such as psychological counseling and digital tutoring", "source_hint": "Introduction paragraph 2"}, {"name": "P-React: Mixture of LoRA experts with personality-guided routing", "description": "P-React is a mixture-of-experts model where each expert is a LoRA module. A personality-guided routing mechanism assigns unique combinations of expert weights based on the Big Five personality trait being simulated, enabling trait-specific modeling.", "author_claim_text": "we introduce P-React, a personality-customizable model built on the LoRAMoE architecture", "source_hint": "Introduction paragraph 4"}, {"name": "Personality Specialization Loss (PSL)", "description": "PSL is a novel auxiliary loss that encourages different experts to specialize in distinct personality traits by minimizing the similarity of expert weight distributions across traits, ensuring that each expert captures unique trait-specific reactions rather than learning all traits uniformly.", "author_claim_text": "we develop the Personality Specialization Loss (PSL), which guides each expert to focus on modeling reactions to specific personality traits", "source_hint": "Introduction paragraph 4"}]}, "abs": "Personalized large language models (LLMs) have attracted great attention in many applications, such as emotional support and role-playing. However, existing works primarily focus on modeling explicit character profiles, while ignoring the underlying personality traits that truly shape behaviors and decision-making, hampering the development of more anthropomorphic and psychologically-grounded AI systems. In this paper, we explore the modeling of Big Five personality traits, which is the most widely used trait theory in psychology, and propose P-React, a mixture of experts (MoE)-based personalized LLM. Particularly, we integrate a Personality Specialization Loss (PSL) to better capture individual trait expressions, providing a more nuanced and psychologically grounded personality simulacrum. To facilitate research in this field, we curate OCEAN-Chat, a high-quality, human-verified dataset designed to train LLMs in expressing personality traits across diverse topics. Extensive experiments demonstrate the effectiveness of P-React in maintaining consistent and real personality."}, {"title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Personalized Generation and Rewarding/Parameter-based/Fine-Grained Human Feedback Gives Better Rewards for Language Model Training.pdf", "summary": {"paper_id": "survey-1_paper-28_Fine-Grained Human Feedback Gives Better Rewards for Language Model Training", "core_task": {"text": "reinforcement learning from fine-grained human feedback for language model training", "query_variants": ["reinforcement learning from fine-grained human feedback for language model training", "training language models using reinforcement learning with detailed human annotations", "RL from granular human preferences for LM optimization"]}, "contributions": [{"name": "Limited information from holistic feedback in RLHF", "description": "Existing RLHF methods use holistic human preference judgments that provide only a single scalar reward for entire generated sequences. This sparse signal fails to localize errors or distinguish between different types of undesired behaviors, making training unreliable especially for long-form text generation tasks.", "author_claim_text": "holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors", "source_hint": "Abstract"}, {"name": "Improving reward signals through fine-grained feedback", "description": "The authors are motivated by prior research showing that fine-grained evaluation (identifying specific error types and their locations) offers clearer insights than holistic quality comparisons. They aim to leverage this principle to create more informative training signals for language models through RLHF.", "author_claim_text": "categorizing and localizing model errors (i.e., fine-grained evaluation) provides explicit insights about which part of the model output has what type of problem", "source_hint": "Introduction paragraph 1"}, {"name": "FINE-GRAINED RLHF framework", "description": "The proposed framework trains multiple reward models, each targeting a specific error category (e.g., toxicity, factual incorrectness, irrelevance) and providing rewards at different densities (e.g., sub-sentence, sentence, or sequence level). These fine-grained rewards are integrated into PPO training to optimize the policy language model.", "author_claim_text": "We introduce FINE-GRAINED RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types", "source_hint": "Abstract"}, {"name": "QA-FEEDBACK dataset with fine-grained annotations", "description": "The authors create a new dataset for long-form question answering based on ASQA, annotated with fine-grained human feedback across three error categories at different densities: irrelevance/repetition/incoherence (sub-sentence level), incorrect/unverifiable facts (sentence level), and incomplete information (sequence level).", "author_claim_text": "we construct a long-form QA dataset, QA-FEEDBACK, along with our collected human feedback", "source_hint": "Introduction paragraph 3, Section 4.1"}]}, "abs": "Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF) - where human preference judgments on LM outputs are transformed into a learning signal - has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how learning with such reward functions leads to improved performance, supported by both automatic and human evaluation. Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models. We release all data, collected human feedback, and codes at this https URL."}, {"title": "Aligning to Thousands of Preferences via System Message Generalization", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Personalized Generation and Rewarding/Prompting-based/Aligning to Thousands of Preferences via System Message Generalization.pdf", "summary": {"paper_id": "survey-1_paper-29_Aligning to Thousands of Preferences via System Message Generalization", "core_task": {"text": "Aligning large language models to diverse user preferences", "query_variants": ["Aligning large language models to diverse user preferences", "Adapting large language models to varied user preferences", "Customizing large language models for diverse user needs"]}, "contributions": [{"name": "Scalability of individualized LLM alignment", "description": "The paper addresses the challenge of scaling individualized alignment of large language models (LLMs) to diverse user preferences without the need for repeated data collection and retraining.", "author_claim_text": "A major challenge in adopting a more individualized approach to LLM alignment is its lack of scalability, as it involves repeatedly acquiring preference data and training new reward models and LLMs for each individual's preferences.", "source_hint": "Abstract"}, {"name": "Diverse human preferences in LLM alignment", "description": "The authors are motivated by the need to align LLMs with the diverse and nuanced preferences of individuals, which are not adequately captured by current alignment methods focused on general preferences.", "author_claim_text": "However, human preferences cannot simply be categorized into such binary divisions; they are typically diverse and exhibit nuanced variations according to different people and contexts.", "source_hint": "Introduction §1"}, {"name": "System message generalization", "description": "The authors propose using system messages to explicitly encode user preferences, allowing LLMs to adapt their behavior to align with individual user intentions without retraining.", "author_claim_text": "To address these challenges, we propose a new paradigm where users specify what they value most within the system message, steering the LLM's generation behavior to better align with the user's intentions.", "source_hint": "Abstract"}, {"name": "MULTIFACETED COLLECTION dataset", "description": "The authors develop a dataset called MULTIFACETED COLLECTION, which includes a large number of system messages generated by combining hierarchical user value dimensions to train LLMs for diverse preference alignment.", "author_claim_text": "To improve this generalization, we create MULTIFACETED COLLECTION, augmenting 66k user instructions into 197k system messages through hierarchical user value combinations.", "source_hint": "Abstract"}]}, "abs": "Although humans inherently have diverse values, current large language model (LLM) alignment methods often assume that aligning LLMs with the general public's preferences is optimal. A major challenge in adopting a more individualized approach to LLM alignment is its lack of scalability, as it involves repeatedly acquiring preference data and training new reward models and LLMs for each individual's preferences. To address these challenges, we propose a new paradigm where users specify what they value most within the system message, steering the LLM's generation behavior to better align with the user's intentions. However, a naive application of such an approach is non-trivial since LLMs are typically trained on a uniform system message (e.g., \"You are a helpful assistant\") which limits their ability to generalize to diverse, unseen system messages. To improve this generalization, we create the Multifaceted Collection, a preference dataset with 192k combinations of values beyond generic helpfulness and harmlessness, spanning 65k user instructions. Using this dataset, we train a 7B LLM called Janus and test it on 921 prompts from 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct) by adding various unseen system messages that reflect user preferences. Janus achieves tie+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct v0.2, GPT-3.5 Turbo, and GPT-4, respectively. Unexpectedly, on three benchmarks focused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto v0.1), Janus also outperforms LLaMA 3 8B Instruct by a +4.0%, +0.1%, +3.0% margin, underscoring that training with a vast array of system messages could also enhance alignment to the general public's preference as well. Our code, dataset, benchmark, and models are available at this https URL."}, {"title": "SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Personalized Generation and Rewarding/Prompting-based/SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF.pdf", "summary": {"paper_id": "survey-1_paper-30_SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF", "core_task": {"text": "attribute conditioned supervised fine-tuning for language model alignment", "query_variants": ["attribute conditioned supervised fine-tuning for language model alignment", "supervised fine-tuning of language models conditioned on attributes for alignment", "attribute-based supervised tuning to align language models"]}, "contributions": [{"name": "Limitations of RLHF for model alignment", "description": "The paper identifies that RLHF has a complex training configuration and aligns models to implicit values that users cannot adjust during inference. Additionally, reward models in RLHF rely on single-dimensional feedback rather than explicit multi-dimensional signals for attributes like helpfulness, humor, and toxicity.", "author_claim_text": "RLHF faces inherent limitations stemming from a complex training setup and its tendency to align the model with implicit values that end users cannot control at run-time.", "source_hint": "Abstract"}, {"name": "Need for user-steerable AI with explicit multi-dimensional control", "description": "The authors are motivated to create a model alignment approach that allows users to explicitly control multiple response attributes at inference time, enabling customizable and steerable AI that generates helpful, high-quality responses while maintaining flexibility for different use cases.", "author_claim_text": "To address these limitations, we propose STEERLM, a supervised fine-tuning method that empowers end-users to control responses during inference.", "source_hint": "Abstract"}, {"name": "Attribute Prediction Model", "description": "A language model is trained to predict multiple human-annotated attributes (quality, toxicity, humor, etc.) for responses, providing multi-dimensional feedback instead of a single reward signal. This model is used to annotate diverse datasets with attribute values.", "author_claim_text": "Similar to the reward model in RLHF, the Attribute Prediction Model in STEERLM is designed to predict human preference of model responses to improve model alignment.", "source_hint": "Section 3.1"}, {"name": "Attribute Conditioned Supervised Fine-Tuning", "description": "The language model is fine-tuned to generate responses conditioned on both the prompt and desired attribute values. This approach uses only supervised learning with an offline annotated dataset, avoiding the complexity of online sampling and evaluation required in RLHF.", "author_claim_text": "Attribute-conditioned SFT is an extension of regular SFT that enables incorporating reward signal information through attribute labels.", "source_hint": "Section 3.3"}]}, "abs": "Model alignment with human preferences is an essential step in making Large Language Models (LLMs) helpful and consistent with human values. It typically consists of supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) stages. However, RLHF faces inherent limitations stemming from a complex training setup and its tendency to align the model with implicit values that end users cannot control at run-time. Moreover, reward models in RLHF stage commonly rely on single-dimensional feedback as opposed to explicit, multifaceted signals that indicate attributes such as helpfulness, humor, and toxicity. To address these limitations, we propose SteerLM, a supervised fine-tuning method that empowers end-users to control responses during inference. SteerLM conditions responses to conform to an explicitly defined multi-dimensional set of attributes, thereby empowering a steerable AI capable of generating helpful and high-quality responses while maintaining customizability. Experiments show that SteerLM trained on open source datasets generates responses that are preferred by human and automatic evaluators to many state-of-the-art baselines trained with RLHF while being much easier to train. Try SteerLM at this https URL"}, {"title": "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Personalized Generation and Rewarding/Prompting-based/Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies.pdf", "summary": {"paper_id": "survey-1_paper-31_Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies", "core_task": {"text": "simulating human behavior in experimental studies using language models", "query_variants": ["simulating human behavior in experimental studies using language models", "language models for modeling human responses in experimental research", "using LLMs to replicate human participant behavior in experiments"]}, "contributions": [{"name": "Limited diagnostic value of Turing's Imitation Game for evaluating AI simulation capabilities", "description": "The Turing Imitation Game only requires simulating a single arbitrary individual and provides little insight into which specific aspects of human behavior an AI system can accurately simulate or which populations it can represent.", "author_claim_text": "the IG is of limited diagnostic value as it says little about which humans and behaviors an LM can faithfully simulate", "source_hint": "Introduction paragraph 2"}, {"name": "Need to inform downstream applications requiring accurate human models", "description": "Understanding which aspects of human behavior language models can faithfully simulate and which they systematically distort is important for applications in education and the arts that depend on accurate human behavioral models.", "author_claim_text": "This understanding can inform downstream applications that require language models to have accurate models of humans, including various applications in education and the arts", "source_hint": "Introduction paragraph 1"}, {"name": "Turing Experiment (TE) evaluation framework", "description": "A TE evaluates AI systems by simulating human behavior in specific experiments like human subject studies, requiring simulation of representative samples of participants rather than single individuals, using zero-shot procedures without prior experiment-specific data.", "author_claim_text": "We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model can simulate different aspects of human behavior", "source_hint": "Abstract and Introduction paragraph 3"}, {"name": "Prompt-based methodology for running TEs with language models", "description": "The methodology involves creating zero-shot prompts fed into language models, using participant details like names and experimental conditions as inputs, then reconstructing text-based records of simulated experiments from the generated completions, with validation steps to maximize validity rates.", "author_claim_text": "we demonstrate their feasibility by presenting a methodology for running TEs using an LM that takes a text prompt and generates a randomized completion", "source_hint": "Introduction paragraph 5 and Section 2"}]}, "abs": "We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a \"hyper-accuracy distortion\" present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts."}, {"title": "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Personalized Generation and Rewarding/Prompting-based/UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems.pdf", "summary": {"paper_id": "survey-1_paper-32_UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems", "core_task": {"text": "Unified multi-source retrieval-augmented generation for personalized dialogue systems", "query_variants": ["Unified multi-source retrieval-augmented generation for personalized dialogue systems", "Integrated retrieval-augmented generation from multiple sources for personalized conversational agents", "Multi-source retrieval augmentation in unified generation frameworks for personalized dialogue"]}, "contributions": [{"name": "Personalization and multi-source knowledge integration in dialogue systems", "description": "The paper addresses the challenge of building personalized dialogue systems that can effectively plan and incorporate multiple sources of knowledge (e.g., user persona, system persona, documents) to generate contextually appropriate responses, rather than focusing on single sources or indiscriminately using all sources.", "author_claim_text": "the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system", "source_hint": "Abstract"}, {"name": "Limitations of existing knowledge-grounded dialogue systems", "description": "The authors are motivated by the observation that prior methods either isolate individual knowledge sources or fail to consider inter-dependencies among multiple sources, and they overlook cases where no external knowledge is needed. This leads to suboptimal performance in real-world scenarios requiring dynamic source selection.", "author_claim_text": "most of the existing knowledge-grounded dialogue systems either focus on a single source of knowledge or indiscriminately incorporate all sources of knowledge", "source_hint": "Introduction"}, {"name": "Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG)", "description": "UniMS-RAG unifies three sub-tasks (knowledge source selection, knowledge retrieval, and response generation) into a single sequence-to-sequence framework using large language models. It introduces acting tokens to decide which knowledge source to use and evaluation tokens to assess relevance scores between dialogue context and retrieved evidence.", "author_claim_text": "We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training", "source_hint": "Abstract"}, {"name": "Self-refinement mechanism for response generation", "description": "A self-refinement process is introduced during inference that reassesses generated responses by leveraging feedback from evaluation tokens and ensuring consistency between provided evidence and responses, iteratively updating evidence to improve response quality.", "author_claim_text": "we carefully design a self-refinement mechanism to iteratively refine the generated response considering 1) the consistency scores between the generated response and retrieved evidence; and 2) the relevance scores", "source_hint": "Abstract"}]}, "abs": "Large Language Models (LLMs) has shown exceptional capabilities in many natual language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system. To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation. We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens. Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their behavior to diverse task requirements. Meanwhile, evaluation tokens gauge the relevance score between the dialogue context and the retrieved evidence. In addition, we carefully design a self-refinement mechanism to iteratively refine the generated response considering 1) the consistency scores between the generated response and retrieved evidence; and 2) the relevance scores. Experiments on two personalized datasets (DuLeMon and KBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge source selection and response generation task with itself as a retriever in a unified manner. Extensive analyses and discussions are provided for shedding some new perspectives for personalized dialogue systems."}, {"title": "Beyond Goldfish Memory: Long-Term Open-Domain Conversation", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Personalized Generation and Rewarding/Prompting-based/Beyond Goldfish Memory: Long-Term Open-Domain Conversation.pdf", "summary": {"paper_id": "survey-1_paper-33_Beyond Goldfish Memory: Long-Term Open-Domain Conversation", "core_task": {"text": "long-term open-domain conversation modeling", "query_variants": ["long-term open-domain conversation modeling", "multi-turn open-ended dialogue systems", "extended horizon conversational agents in unrestricted domains"]}, "contributions": [{"name": "Short-context limitation in open-domain dialogue models", "description": "Current state-of-the-art dialogue models like Meena and BlenderBot are trained on short conversations (2-15 turns) with token truncation lengths of only 128 tokens, making them incapable of incorporating long-term conversational context across multiple sessions.", "author_claim_text": "state of the art models are trained and evaluated on short conversations with little context. In contrast, the long-term conversation setting has hardly been studied.", "source_hint": "Abstract"}, {"name": "Need for long-term conversational engagement", "description": "Long-term open-domain communication allows conversations to develop and improve over time as the model gains more context and understanding of specific user interests, which is essential for successful deployment and user reengagement.", "author_claim_text": "a successfully deployed bot will engage in many conversations over a length of time, as capturing organic user interest will garner continual reengagement from returning users.", "source_hint": "Introduction §1"}, {"name": "Multi-Session Chat dataset collection", "description": "A new dataset where crowdworkers engage in multi-session conversations spanning hours or days, with each session consisting of up to 14 utterances. Previous sessions are annotated with summaries of important personal points for use in subsequent conversations.", "author_claim_text": "we collect and release a new English dataset, entitled Multi-Session Chat(MSC). The dataset consists of human-human crowdworker chats over 5 sessions", "source_hint": "Introduction §1"}, {"name": "Retrieval-augmented generative models", "description": "Models that use retrieval systems (RAG, FiD, FiD-RAG) to select and encode relevant parts of conversation history from memory, allowing the decoder to attend to these encodings when generating responses.", "author_claim_text": "We study the performance of two long-context conversational architectures on this task: (i) retrieval-augmented generative models", "source_hint": "Introduction §1, Section 4.2"}]}, "abs": "Despite recent improvements in open-domain dialogue models, state of the art models are trained and evaluated on short conversations with little context. In contrast, the long-term conversation setting has hardly been studied. In this work we collect and release a human-human dataset consisting of multiple chat sessions whereby the speaking partners learn about each other's interests and discuss the things they have learnt from past sessions. We show how existing models trained on existing datasets perform poorly in this long-term conversation setting in both automatic and human evaluations, and we study long-context models that can perform much better. In particular, we find retrieval-augmented methods and methods with an ability to summarize and recall previous conversations outperform the standard encoder-decoder architectures currently considered state of the art."}, {"title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Personalized Generation and Rewarding/Prompting-based/From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment.pdf", "summary": {"paper_id": "survey-1_paper-34_From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment", "core_task": {"text": "personalized alignment of large language models", "query_variants": ["personalized alignment of large language models", "customized tuning of LLMs to individual user preferences", "tailoring large language model behavior to specific user values"]}, "contributions": [{"name": "Lack of personalized alignment in LLMs", "description": "Existing LLM alignment methods assume uniform human preferences and overlook the diversity in user values, cultural backgrounds, and needs, leading to systematic exclusion of minority groups and reduced user satisfaction due to lack of personalization.", "author_claim_text": "Current alignment techniques are predominately approached as a one-size-fits-all process, by assuming that all humans share the same set of values prescribed by LLM developers, fundamentally fails to account for the significant diversity inherent in human populations", "source_hint": "Introduction paragraph 1"}, {"name": "Serving diverse user populations at scale", "description": "With LLMs serving hundreds of millions of users globally, the authors are motivated by critical issues including systematic exclusion of minority groups and reduced user satisfaction, highlighting the need for personalized alignment approaches that respect individual differences.", "author_claim_text": "At a time when LLMs serve hundreds of millions of users worldwide, several critical issues have emerged: the systematic exclusion or under-representation of minority groups, reduced user satisfaction and engagement due to the lack of personalization", "source_hint": "Introduction paragraph 1"}, {"name": "Comprehensive preference space and persona framework", "description": "The authors construct a 90-dimensional preference space synthesized from psychological models, alignment research, and content platforms, combined with behavioral and descriptive persona representations to enable scalable preference inference while maintaining practical traceability.", "author_claim_text": "We establish a systematic preference space characterizing psychological and behavioral dimensions, alongside diverse persona representations for robust preference inference in real-world scenarios", "source_hint": "Abstract and Introduction paragraph 2"}, {"name": "ALIGNX dataset construction", "description": "The authors develop a systematic pipeline that transforms large-scale forum interactions into structured training data, yielding ALIGNX with over 1.3 million examples capturing persona representations, preference directions, and high-quality preference pairs.", "author_claim_text": "We introduce ALIGNX, a large-scale dataset of over 1.3 million personalized preference examples", "source_hint": "Abstract and Introduction paragraph 3"}]}, "abs": "Large language models (LLMs) have traditionally been aligned through one-size-fits-all approaches that assume uniform human preferences, fundamentally overlooking the diversity in user values and needs. This paper introduces a comprehensive framework for scalable personalized alignment of LLMs. We establish a systematic preference space characterizing psychological and behavioral dimensions, alongside diverse persona representations for robust preference inference in real-world scenarios. Building upon this foundation, we introduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million personalized preference examples, and develop two complementary alignment approaches: \\textit{in-context alignment} directly conditioning on persona representations and \\textit{preference-bridged alignment} modeling intermediate preference distributions. Extensive experiments demonstrate substantial improvements over existing methods, with an average 17.06\\% accuracy gain across four benchmarks while exhibiting a strong adaptation capability to novel preferences, robustness to limited user data, and precise preference controllability. These results validate our approach toward user-adaptive AI systems."}, {"title": "Is ChatGPT a Good Recommender? A Preliminary Study", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Personalized Generation and Rewarding/Prompting-based/Is ChatGPT a Good Recommender? A Preliminary Study.pdf", "summary": {"paper_id": "survey-1_paper-35_Is ChatGPT a Good Recommender? A Preliminary Study", "core_task": {"text": "evaluating ChatGPT for recommendation system tasks", "query_variants": ["evaluating ChatGPT for recommendation system tasks", "assessing ChatGPT performance on recommender system problems", "testing ChatGPT capabilities in recommendation tasks"]}, "contributions": [{"name": "Lack of generalization in traditional recommendation systems", "description": "Traditional recommendation systems require specific data and models for different tasks or scenarios, limiting their ability to generalize across diverse recommendation contexts without retraining.", "author_claim_text": "most traditional recommendation methods are task-specific and therefore lack efficient generalization ability", "source_hint": "Introduction paragraph 1"}, {"name": "Exploring ChatGPT's potential for recommendation via knowledge transfer", "description": "The authors aim to investigate whether ChatGPT's vast linguistic and world knowledge from large-scale pretraining can be effectively applied to recommendation tasks without task-specific fine-tuning.", "author_claim_text": "explore its potential for transferring extensive linguistic and world knowledge acquired from large-scale corpora to recommendation scenarios", "source_hint": "Abstract"}, {"name": "Task-specific prompt design for five recommendation scenarios", "description": "The authors construct specialized prompts tailored to convert five distinct recommendation tasks into natural language tasks, enabling ChatGPT to perform recommendations without fine-tuning.", "author_claim_text": "we design a set of prompts and evaluate ChatGPT's performance on five recommendation scenarios, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization", "source_hint": "Abstract"}, {"name": "Few-shot prompting with user interaction injection", "description": "The authors employ few-shot prompting techniques that incorporate user-item interaction histories into prompts, aiming to help ChatGPT capture user preferences more effectively.", "author_claim_text": "we explore the use of few-shot prompting to inject interaction information that contains user potential interest to help ChatGPT better understand user needs and interests", "source_hint": "Abstract"}]}, "abs": "Recommendation systems have witnessed significant advancements and have been widely used over the past decades. However, most traditional recommendation methods are task-specific and therefore lack efficient generalization ability. Recently, the emergence of ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. Nonetheless, the application of ChatGPT in the recommendation domain has not been thoroughly investigated. In this paper, we employ ChatGPT as a general-purpose recommendation model to explore its potential for transferring extensive linguistic and world knowledge acquired from large-scale corpora to recommendation scenarios. Specifically, we design a set of prompts and evaluate ChatGPT's performance on five recommendation scenarios. Unlike traditional recommendation methods, we do not fine-tune ChatGPT during the entire evaluation process, relying only on the prompts themselves to convert recommendation tasks into natural language tasks. Further, we explore the use of few-shot prompting to inject interaction information that contains user potential interest to help ChatGPT better understand user needs and interests. Comprehensive experimental results on Amazon Beauty dataset show that ChatGPT has achieved promising results in certain tasks and is capable of reaching the baseline level in others. We conduct human evaluations on two explainability-oriented tasks to more accurately evaluate the quality of contents generated by different models. And the human evaluations show ChatGPT can truly understand the provided information and generate clearer and more reasonable results. We hope that our study can inspire researchers to further explore the potential of language models like ChatGPT to improve recommendation performance and contribute to the advancement of the recommendation systems field."}, {"title": "Learning to Rewrite Prompts for Personalized Text Generation", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Personalized Generation and Rewarding/Prompting-based/Learning to Rewrite Prompts for Personalized Text Generation.pdf", "summary": {"paper_id": "survey-1_paper-36_Learning to Rewrite Prompts for Personalized Text Generation", "core_task": {"text": "prompt rewriting for personalized text generation", "query_variants": ["prompt rewriting for personalized text generation", "prompt reformulation for customized language model output", "query modification for individualized text synthesis"]}, "contributions": [{"name": "Optimizing prompts for frozen LLMs in personalized text generation", "description": "The paper addresses the challenge of improving personalized text generation when the underlying large language model cannot be fine-tuned and is only accessible via APIs, limiting intervention to modifying input prompts rather than model parameters.", "author_claim_text": "Most existing studies focus on designing specialized models for a particular domain, or they require fine-tuning the LLMs to generate personalized text. We consider a typical scenario in which the large language model, which generates personalized output, is frozen and can only be accessed through APIs.", "source_hint": "Abstract"}, {"name": "Practical constraints of frozen LLMs and API-only access", "description": "The authors are motivated by the practical reality that most users cannot fine-tune large language models due to lack of access to model internals or prohibitive computational costs, making prompt optimization the only viable approach for personalization.", "author_claim_text": "In most real-world cases, however, fine-tuning the LLM, or the content generator, is not feasible. On one hand, the LLM is often a black box with frozen parameters and one has no control over its internal mechanism. On the other hand, even if one has access to the model parameters, fine-tuning a LLM typically requires an inhibiting amount of computational resources for individual users.", "source_hint": "Introduction §1"}, {"name": "Automatic prompt rewriter using chained SL and RL", "description": "The authors propose a prompt rewriting method that first uses supervised learning to adapt a sequence-to-sequence model to the rewriting task, then applies reinforcement learning with generation performance as reward to optimize prompts end-to-end, addressing the large search space challenge.", "author_claim_text": "In this paper, we propose a novel method to automatically revise prompts for personalized text generation. The prompt rewriter employs a training paradigm that chains together supervised learning (SL) and reinforcement learning (RL), where SL reduces the search space of RL and RL facilitates end-to-end training of the rewriter.", "source_hint": "Abstract"}, {"name": "Writing style synthesis as additional personalization signal", "description": "The method introduces writing style synthesis as a new component in prompts, using the LLM itself to extract consistent stylistic patterns from a user's historical documents to better guide personalized generation.", "author_claim_text": "We introduce a new type of synthesis based on writing style. A user's writing style is likely to remain consistent across different topics, e.g., their choice of words, preferred sentence structure, or use of grammar. Therefore we synthesize a user's writing style to provide additional guidance to the document generator.", "source_hint": "Section 5.1"}]}, "abs": "Facilitated by large language models (LLMs), personalized text generation has become a rapidly growing research direction. Most existing studies focus on designing specialized models for a particular domain, or they require fine-tuning the LLMs to generate personalized text. We consider a typical scenario in which the large language model, which generates personalized output, is frozen and can only be accessed through APIs. Under this constraint, all one can do is to improve the input text (i.e., text prompts) sent to the LLM, a procedure that is usually done manually. In this paper, we propose a novel method to automatically revise prompts for personalized text generation. The proposed method takes the initial prompts generated by a state-of-the-art, multistage framework for personalized generation and rewrites a few critical components that summarize and synthesize the personal context. The prompt rewriter employs a training paradigm that chains together supervised learning (SL) and reinforcement learning (RL), where SL reduces the search space of RL and RL facilitates end-to-end training of the rewriter. Using datasets from three representative domains, we demonstrate that the rewritten prompts outperform both the original prompts and the prompts optimized via supervised learning or reinforcement learning alone. In-depth analysis of the rewritten prompts shows that they are not only human readable, but also able to guide manual revision of prompts when there is limited resource to employ reinforcement learning to train the prompt rewriter, or when it is costly to deploy an automatic prompt rewriter for inference."}, {"title": "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment", "path": "/mnt/data/zyh_survey_data/A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications/Personalized Alignment Techniques/Personalized Generation and Rewarding/Prompting-based/Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment.pdf", "summary": {"paper_id": "survey-1_paper-37_Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment", "core_task": {"text": "multi-objective alignment of foundation models with human preferences", "query_variants": ["multi-objective alignment of foundation models with human preferences", "aligning foundation models to human preferences across multiple objectives", "human preference alignment for foundation models with multiple objectives"]}, "contributions": [{"name": "Multi-objective alignment of foundation models with heterogeneous and conflicting human preferences", "description": "The paper addresses the challenge of aligning foundation models with multiple, diverse human preferences that are often heterogeneous and conflicting (e.g., harmlessness vs helpfulness). Existing RLHF methods are computationally expensive and unstable, especially when dealing with multiple objectives simultaneously.", "author_claim_text": "it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process", "source_hint": "Abstract"}, {"name": "Need for scalable and adaptive multi-objective alignment without expensive RL training", "description": "The authors are motivated by the inadequacy of single-reward alignment and the prohibitive computational cost of existing multi-objective RLHF approaches. They seek a solution that can accommodate diverse preferences efficiently while maintaining alignment quality across multiple objectives.", "author_claim_text": "fine-tuning large language models with a single reward model may not adequately align with diverse human preferences. This highlights the necessity for further exploration into Multi-Objective RLHF (MORLHF)", "source_hint": "Introduction §1"}, {"name": "Rewards-in-Context (RiC) with multi-reward conditional supervised fine-tuning", "description": "RiC restructures multi-objective alignment into three stages: offline training using multi-reward conditional SFT where prompts are augmented with reward values, online training that augments data near the Pareto front using multi-objective rejection sampling, and an inference stage with dynamic preference-to-reward mapping.", "author_claim_text": "we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment", "source_hint": "Abstract and Introduction §1"}, {"name": "Analytical preference-to-reward mapping based on convex optimization", "description": "The authors derive a family of preference-to-reward mappings by formulating and solving a constrained convex optimization problem. This provides closed-form solutions that map user preferences to desired reward values in prompts, enabling dynamic inference-time adaptation toward Pareto-optimal outcomes.", "author_claim_text": "Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method approaches the Pareto-optimal solution for multiple objectives", "source_hint": "Abstract and Section 3.4"}]}, "abs": "We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method approaches the Pareto-optimal solution for multiple objectives. Empirical evidence demonstrates the efficacy of our method in aligning both Large Language Models (LLMs) and diffusion models to accommodate diverse rewards with only around 10% GPU hours compared with multi-objective RL baseline."}], "survey_topic_path": "A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications", "survey_topic": "A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications", "gt_paper_count": 38, "gt": {"name": "A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications", "subtopics": [{"name": "Personalized Alignment Techniques", "subtopics": [{"name": "Alignment through Feedback", "subtopics": [{"name": "Inference-time Alignment", "papers": ["Decoding-Time Language Model Alignment with Multiple Objectives", "PAD: Personalized Alignment of LLMs at Decoding-Time", "MetaAligner: Towards Generalizable Multi-Objective Alignment of Language Models"]}, {"name": "Multi-objective Optimization", "papers": ["Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment", "Topic-Level Bayesian Surprise and Serendipity for Recommender Systems", "Eliciting User Preferences for Personalized Multi-Objective Decision Making through Comparative Feedback", "Differentiation of Multi-objective Data-driven Decision Pipeline", "Conditional Language Policy: A General Framework for Steerable Multi-Objective Finetuning"]}, {"name": "Training-time Alignment", "papers": ["Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging", "AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback", "FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning", "FedRLHF: A Convergence-Guaranteed Federated Framework for Privacy-Preserving and Personalized RLHF", "Collective Constitutional AI: Aligning a Language Model with Public Input", "PERSOMA: PERsonalized SOft ProMpt Adapter Architecture for Personalized Language Prompting", "FDLoRA: Personalized Federated Learning of Large Language Model via Dual LoRA Tuning", "Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning", "RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation"]}]}, {"name": "Personalized Generation and Rewarding", "subtopics": [{"name": "Agent-based", "papers": ["MACRec: a Multi-Agent Collaboration Framework for Recommendation", "JumpStarter: Human-AI Planning with Task-Structured Context Curation", "TravelPlanner: A Benchmark for Real-World Planning with Language Agents", "Prospect Personalized Recommendation on Large Language Model-based Agent Platform"]}, {"name": "Encoding-based", "papers": ["User-LLM: Efficient LLM Contextualization with User Embeddings", "Personalized Language Modeling from Personalized Human Feedback", "Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF", "Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning"]}, {"name": "Parameter-based", "papers": ["Selective Prompting Tuning for Personalized Conversations with LLMs", "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction", "P-React: Synthesizing Topic-Adaptive Reactions of Personality Traits via Mixture of Specialized LoRA Experts", "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"]}, {"name": "Prompting-based", "papers": ["Aligning to Thousands of Preferences via System Message Generalization", "SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF", "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies", "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems", "Beyond Goldfish Memory: Long-Term Open-Domain Conversation", "From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment", "Is ChatGPT a Good Recommender? A Preliminary Study", "Learning to Rewrite Prompts for Personalized Text Generation", "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment"]}]}]}]}, "input_paper_count": 38, "input_content": "SYSTEM PROMPT:\nYou are a senior researcher and survey-author with deep experience in structuring\nhigh-quality academic survey papers.\n\nYour task is to organize a set of research papers into a **hierarchical topic tree**\n(bottom-up), given:\n- a survey topic,\n- for each paper: title, abstract, and an structured summary\n  (which may include research problem, motivation, methodology, and findings).\n\nYour goal is NOT just to cluster by surface similarity, but to produce a taxonomy\nthat would be considered **reasonable, informative, and defensible** in a top-tier survey paper.\n\n### Hard Constraints\n1. Output must be **strictly valid JSON**.\n2. Only leaf nodes may contain `\"papers\"`; all internal nodes must contain `\"subtopics\"`.\n3. **Every paper must appear exactly once** in the entire tree.\n4. NO duplicate papers anywhere.\n5. The tree must eventually merge into **one single root node**.\n\n### CLASSIFICATION RULES\n- Group papers by semantic similarity using both title + abstract.\n- Create meaningful names for leaf-level themes.\n\n### ANTI-DUPLICATION PROCEDURE (MANDATORY)\nBefore constructing the tree:\n1. Produce an internal list of all given paper titles.\n2. Assign each paper to exactly one leaf node.\n3. After assignment, verify that:\n   - the number of assigned papers equals the number of input papers,\n   - no paper appears in more than one group.\n\n### Output Format\nUse a JSON structure like this (replace placeholders with actual paper titles) and\nthe output you produce MUST be wrapped inside a fenced code block:\n\n```json\n{\n  \"name\": \"AI Research\",\n  \"subtopics\": [\n    {\n      \"name\": \"NLP\",\n      \"subtopics\": [\n        {\n          \"name\": \"Text Summarization\",\n          \"papers\": [\"<actual paper titles>\"]\n        },\n        {\n          \"name\": \"Machine Translation\",\n          \"papers\": [\"<actual paper titles>\"]\n        }\n      ]\n    }\n  ]\n}```\n\nUSER PROMPT:\nPerform a bottom-up hierarchical clustering of the following 38 papers and produce a JSON research topic tree.\n\nSurvey Topic: A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications\n\nPaper List:\nPaper 1:\n  Title: Decoding-Time Language Model Alignment with Multiple Objectives\n  Abstract: Aligning language models (LMs) to human preferences has emerged as a critical pursuit, enabling these models to better serve diverse user needs. Existing methods primarily focus on optimizing LMs for a single reward function, limiting their adaptability to varied objectives. Here, we propose $\\textbf{multi-objective decoding (MOD)}$, a decoding-time algorithm that outputs the next token from a linear combination of predictions of all base models, for any given weightings over different objectives. We exploit a common form among a family of $f$-divergence regularized alignment approaches (such as PPO, DPO, and their variants) to identify a closed-form solution by Legendre transform, and derive an efficient decoding strategy. Theoretically, we show why existing approaches can be sub-optimal even in natural settings and obtain optimality guarantees for our method. Empirical results demonstrate the effectiveness of the algorithm. For example, compared to a parameter-merging baseline, MOD achieves 12.8% overall reward improvement when equally optimizing towards $3$ objectives. Moreover, we experiment with MOD on combining three fully-finetuned LLMs of different model sizes, each aimed at different objectives such as safety, coding, and general user preference. Unlike traditional methods that require careful curation of a mixture of datasets to achieve comprehensive improvement, we can quickly experiment with preference weightings using MOD to find the best combination of models. Our best combination reduces toxicity on Toxigen to nearly 0% and achieves 7.9--33.3% improvement across other three metrics ($\\textit{i.e.}$, Codex@1, GSM-COT, BBH-COT).\n  Structured Summary:\n    - Limited adaptability of language models to multiple objectives: Current alignment approaches like PPO and DPO optimize language models for only one reward function at a time, making them unable to flexibly adapt to different combinations of objectives that users may prefer in practice.\n    - Need for on-the-fly adaptation to diverse user preferences: Different applications and users require different trade-offs among objectives (e.g., helpfulness vs. harmlessness in dialogue agents), necessitating methods that can dynamically adjust to various preference weightings without expensive retraining for each combination.\n    - Multi-objective decoding (MOD) algorithm: MOD is a training-free decoding algorithm that combines predictions from multiple base models (each trained for a single objective) at inference time. It uses Legendre transform to derive a closed-form solution for token-level decoding that optimally balances multiple objectives according to user-specified preference weightings.\n    - Closed-form solution via Legendre transform: The method leverages the mathematical structure of f-divergence regularized alignment objectives and applies Legendre transform from convex optimization to obtain an efficient closed-form decoding rule that combines base model predictions without requiring normalization factors.\n\nPaper 2:\n  Title: PAD: Personalized Alignment of LLMs at Decoding-Time\n  Abstract: Aligning with personalized preferences, which vary significantly across cultural, educational, and political differences, poses a significant challenge due to the computational costs and data demands of traditional alignment methods. In response, this paper presents Personalized Alignment at Decoding-time (PAD), a novel framework designed to align LLM outputs with diverse personalized preferences during the inference phase, eliminating the need for additional training. By introducing a unique personalized reward modeling strategy, this framework decouples the text generation process from personalized preferences, facilitating the generation of generalizable token-level personalized rewards. The PAD algorithm leverages these rewards to guide the decoding process, dynamically tailoring the base model's predictions to personalized preferences. Extensive experimental results demonstrate that PAD not only outperforms existing training-based alignment methods in terms of aligning with diverse preferences but also shows significant generalizability to preferences unseen during training and scalability across different base models. This work advances the capability of LLMs to meet user needs in real-time applications, presenting a substantial step forward in personalized LLM alignment.\n  Structured Summary:\n    - High computational cost and data demands of aligning LLMs with diverse personalized preferences: The paper addresses the challenge that existing alignment methods struggle to accommodate diverse personalized preferences across users due to the high computational costs and extensive data requirements needed for training separate policy models for each preference configuration.\n    - Need for real-time personalized alignment without retraining: The authors are motivated by the observation that user preferences vary substantially across cultural, educational, and political dimensions, and even for the same user across different scenarios. Current frameworks cannot efficiently handle this diversity without expensive retraining and data collection.\n    - Personalized reward modeling strategy decoupling preferences from MDP dynamics: The authors introduce a reward modeling approach that separates the dynamics of text generation from personalized preferences by representing rewards as the inner product of state-action features and preference weights. This enables learning generalizable token-level rewards with a single model.\n    - PAD algorithm for guided decoding with personalized rewards: The PAD algorithm performs inference-time alignment by using a personalized reward model to score candidate tokens at each generation step. These scores are combined with the base model's likelihood to guide token selection, dynamically tailoring outputs to personalized preferences without additional training.\n\nPaper 3:\n  Title: MetaAligner: Towards Generalizable Multi-Objective Alignment of Language Models\n  Abstract: Recent advancements in large language models (LLMs) focus on aligning to heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are dependent on the policy model parameters, which require high-cost repetition of their alignment algorithms for each new policy model, and they cannot expand to unseen objectives due to their static alignment objectives. In this work, we propose Meta-Objective Aligner (MetaAligner), the first policy-agnostic and generalizable method for multi-objective preference alignment. MetaAligner models multi-objective alignment into three stages: (1) dynamic objectives reformulation algorithm reorganizes traditional alignment datasets to supervise the model on performing flexible alignment across different objectives; (2) conditional weak-to-strong correction paradigm aligns the weak outputs of fixed policy models to approach strong outputs with higher preferences in the corresponding alignment objectives, enabling plug-and-play inferences on any policy models, which significantly reduces training costs and facilitates alignment on close-source policy models; (3) generalizable inference method flexibly adjusts target objectives by updating their text descriptions in the prompts, facilitating generalizable alignment to unseen objectives. Experimental results show that MetaAligner achieves significant and balanced improvements in multi-objective alignments on 10 state-of-the-art policy models, and saves up to 93.63% of GPU training hours compared to previous alignment methods. The model also effectively aligns unseen objectives, marking the first step towards generalizable multi-objective preference alignment.\n  Structured Summary:\n    - Policy-dependent and non-generalizable multi-objective alignment: Current multi-objective alignment methods require retraining for each new policy model and cannot generalize to objectives not seen during training, limiting their practical applicability and scalability.\n    - Reducing training costs and enabling generalizable alignment: The authors are motivated by the need to reduce computational costs associated with aligning each new policy model and to enable flexible alignment to new objectives without retraining, addressing scalability challenges in modern foundation models.\n    - Dynamic objectives reformulation algorithm: An algorithm that restructures alignment datasets by incorporating text descriptions of various objectives in a prompt-based manner, enabling the model to learn flexible alignment across different objective combinations during training.\n    - Conditional weak-to-strong correction paradigm: A training paradigm that stacks MetaAligner on top of frozen policy models to correct their outputs toward higher-preference responses, using a three-step process (warming up, equal-preference alignment, contrastive-preference alignment) that enables policy-agnostic alignment without accessing model parameters.\n\nPaper 4:\n  Title: Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment\n  Abstract: Alignment in artificial intelligence pursues the consistency between model responses and human preferences as well as values. In practice, the multifaceted nature of human preferences inadvertently introduces what is known as the \"alignment tax\" -a compromise where enhancements in alignment within one objective (e.g.,harmlessness) can diminish performance in others (e.g.,helpfulness). However, existing alignment techniques are mostly unidirectional, leading to suboptimal trade-offs and poor flexibility over various objectives. To navigate this challenge, we argue the prominence of grounding LLMs with evident preferences. We introduce controllable preference optimization (CPO), which explicitly specifies preference scores for different objectives, thereby guiding the model to generate responses that meet the requirements. Our experimental analysis reveals that the aligned models can provide responses that match various preferences among the \"3H\" (helpfulness, honesty, harmlessness) desiderata. Furthermore, by introducing diverse data and alignment goals, we surpass baseline methods in aligning with single objectives, hence mitigating the impact of the alignment tax and achieving improvements in multi-objective alignment.\n  Structured Summary:\n    - Alignment tax in multi-objective LLM alignment: The paper addresses the challenge that optimizing large language models for multiple alignment objectives (helpfulness, honesty, harmlessness) simultaneously leads to performance trade-offs, where improving one objective degrades others. Existing alignment techniques are unidirectional and fail to balance these conflicting goals effectively.\n    - Need for controllable multi-objective alignment: The authors are motivated by the observation that different contexts require different priority balances among alignment objectives. Rather than maximizing all objectives simultaneously, they propose that explicit user-specified preference conditions can guide models to generate contextually appropriate responses while mitigating inherent conflicts.\n    - Controllable Preference Optimization (CPO) algorithm: CPO is a two-stage alignment method. CPSFT trains the model to generate responses conditioned on explicit preference tokens (e.g., <Helpfulness:5>). CDPO then optimizes preferences by comparing responses under value conditions using a conditional multi-preference reward, increasing probability of better responses while decreasing others.\n    - Conditional multi-objective optimization formulation: The authors reformulate the alignment problem by introducing preference conditions into the optimization objective. For controlled objectives, the goal becomes minimizing the distance between model output and the specified condition; for uncontrolled objectives, the goal remains maximizing preference scores.\n\nPaper 5:\n  Title: Topic-Level Bayesian Surprise and Serendipity for Recommender Systems\n  Abstract: A recommender system that optimizes its recommendations solely to fit a user's history of ratings for consumed items can create a filter bubble, wherein the user does not get to experience items from novel, unseen categories. One approach to mitigate this undesired behavior is to recommend items with high potential for serendipity, namely surprising items that are likely to be highly rated. In this paper, we propose a content-based formulation of serendipity that is rooted in Bayesian surprise and use it to measure the serendipity of items after they are consumed and rated by the user. When coupled with a collaborative-filtering component that identifies similar users, this enables recommending items with high potential for serendipity. To facilitate the evaluation of topic-level models for surprise and serendipity, we introduce a dataset of book reading histories extracted from Goodreads, containing over 26 thousand users and close to 1.3 million books, where we manually annotate 449 books read by 4 users in terms of their time-dependent, topic-level surprise. Experimental evaluations show that models that use Bayesian surprise correlate much better with the manual annotations of topic-level surprise than distance-based heuristics, and also obtain better serendipitous item recommendation performance.\n  Structured Summary:\n    - Filter bubble creation by recommender systems optimizing solely for user rating history: The paper addresses the problem that recommender systems focusing exclusively on fitting user rating histories create filter bubbles, preventing users from experiencing items from novel or unseen categories. This is exacerbated by feedback loops between ranking models and user reactions.\n    - Mitigating filter bubbles through serendipitous recommendations: The authors are motivated by the need to break filter bubbles by recommending serendipitous items—those that are both surprising and positively rated—rather than optimizing solely for diversity or relevance. This addresses the limitation that users become insulated from different topics or viewpoints.\n    - Content-based serendipity formulation using Bayesian surprise: The authors propose a content-based measure of serendipity defined as the product of an item's rating and its Bayesian surprise, computed as the KL divergence between posterior and prior user preference distributions. This post-factum measure captures both the surprising and valuable aspects of serendipity.\n    - Collaborative filtering integration for serendipity recommendation: The method integrates the Bayesian surprise-based serendipity measure with a collaborative filtering component. This component identifies similar users who consumed serendipitous items in the past, enabling proactive recommendation of items with high serendipity potential for the target user.\n\nPaper 6:\n  Title: Eliciting User Preferences for Personalized Multi-Objective Decision Making through Comparative Feedback\n  Abstract: In classic reinforcement learning (RL) and decision making problems, policies are evaluated with respect to a scalar reward function, and all optimal policies are the same with regards to their expected return. However, many real-world problems involve balancing multiple, sometimes conflicting, objectives whose relative priority will vary according to the preferences of each user. Consequently, a policy that is optimal for one user might be sub-optimal for another. In this work, we propose a multi-objective decision making framework that accommodates different user preferences over objectives, where preferences are learned via policy comparisons. Our model consists of a Markov decision process with a vector-valued reward function, with each user having an unknown preference vector that expresses the relative importance of each objective. The goal is to efficiently compute a near-optimal policy for a given user. We consider two user feedback models. We first address the case where a user is provided with two policies and returns their preferred policy as feedback. We then move to a different user feedback model, where a user is instead provided with two small weighted sets of representative trajectories and selects the preferred one. In both cases, we suggest an algorithm that finds a nearly optimal policy for the user using a small number of comparison queries.\n  Structured Summary:\n    - Personalized multi-objective decision making with unknown user preferences: The paper addresses the challenge of finding optimal personalized policies in multi-objective decision making when user preferences over objectives are unknown and must be learned through interaction. Unlike prior work that assumes known preferences or focuses on Pareto-optimal sets, this work tackles the problem of eliciting unknown preferences efficiently.\n    - Practical need for intuitive preference elicitation in personalized systems: The authors are motivated by the practical difficulty users face in explicitly defining fine-grained preferences over multiple objectives. They argue that comparative feedback (comparing two policies) is more natural and intuitive than providing numerical scores or stochastic feedback on single policies, making it more suitable for real-world personalized systems like navigation or recommendation.\n    - Preference learning via policy comparison queries with basis policies: The method identifies a set of basis policies with linearly independent values, estimates ratios between personalized values using binary search with comparison queries, and solves a linear program to approximate the user's preference vector. This enables finding a near-optimal personalized policy with query complexity logarithmic in precision.\n    - Weighted trajectory set representation for interpretable policies: The authors propose representing policies as small weighted sets of representative trajectories (at most k+1 trajectories) rather than explicit state-action mappings. This representation preserves multi-objective values while being more interpretable for users, and can be computed efficiently using expanding-compressing or flow decomposition approaches.\n\nPaper 7:\n  Title: Differentiation of Multi-objective Data-driven Decision Pipeline\n  Abstract: Real-world scenarios frequently involve multi-objective data-driven optimization problems, characterized by unknown problem coefficients and multiple conflicting objectives. Traditional two-stage methods independently apply a machine learning model to estimate problem coefficients, followed by invoking a solver to tackle the predicted optimization problem. The independent use of optimization solvers and prediction models may lead to suboptimal performance due to mismatches between their objectives. Recent efforts have focused on end-to-end training of predictive models that use decision loss derived from the downstream optimization problem. However, these methods have primarily focused on single-objective optimization problems, thus limiting their applicability. We aim to propose a multi-objective decision-focused approach to address this gap. In order to better align with the inherent properties of multi-objective optimization problems, we propose a set of novel loss functions. These loss functions are designed to capture the discrepancies between predicted and true decision problems, considering solution space, objective space, and decision quality, named landscape loss, Pareto set loss, and decision loss, respectively. Our experimental results demonstrate that our proposed method significantly outperforms traditional two-stage methods and most current decision-focused methods.\n  Structured Summary:\n    - Misalignment between prediction and multi-objective optimization in data-driven decision problems: The paper addresses the challenge that traditional two-stage approaches, which separate prediction and optimization phases, lead to suboptimal outcomes in multi-objective data-driven problems because prediction models prioritize accuracy without considering the structure of downstream optimization problems, and the impact of predicted coefficients on decisions is non-uniform.\n    - Extending decision-focused learning to multi-objective optimization problems: The authors are motivated by the fact that existing decision-focused learning methods concentrate on single-objective problems, while numerous real-world scenarios involve multi-objective data-driven optimization with conflicting objectives and unknown coefficients, necessitating specialized approaches that account for Pareto fronts and gradient conflicts.\n    - Multi-Objective Decision-Focused Learning (MoDFL) framework: The authors propose MoDFL, an end-to-end framework that integrates predictive models with multi-objective optimization problems for unified training. The framework employs differentiation via reparameterization and transforms multi-objective problems into single-objective ones using the weighted sum method to enable gradient-based training.\n    - Three novel loss functions for multi-objective decision-focused learning: The authors introduce three complementary loss functions: landscape loss (using sample rank maximum mean discrepancy to measure objective space distance), Pareto set loss (directly measuring distance between Pareto sets in solution space), and decision loss (evaluating decision quality of representative solutions obtained via weighted sum method).\n\nPaper 8:\n  Title: Conditional Language Policy: A General Framework for Steerable Multi-Objective Finetuning\n  Abstract: Reward-based finetuning is crucial for aligning language policies with intended behaviors (e.g., creativity and safety). A key challenge is to develop steerable language models that trade-off multiple (conflicting) objectives in a flexible and efficient manner. This paper presents Conditional Language Policy (CLP), a general framework for finetuning language models on multiple objectives. Building on techniques from multi-task training and parameter-efficient finetuning, CLP learn steerable models that effectively trade-off conflicting objectives at inference time. Notably, this does not require training or maintaining multiple models to achieve different trade-offs between the objectives. Through extensive experiments and ablations on two summarization datasets, we show that CLP learns steerable language models that outperform and Pareto-dominate the existing approaches for multi-objective finetuning.\n  Structured Summary:\n    - Steerable multi-objective finetuning of language models: The paper addresses the difficulty of training language models that can flexibly balance multiple, often conflicting objectives (such as creativity versus safety, or succinctness versus completeness) without requiring separate models for each trade-off configuration.\n    - Pluralistic alignment and decision support: The authors are motivated by the need for pluralistic alignment that respects diverse human preferences and enables decision support systems that can generate multiple diverse outputs covering a wide space of interests, allowing users to personalize models to their preferences.\n    - Conditional Language Policy (CLP) framework: CLP is a general framework that learns a set of parameters which can be conditioned on reward and KL weightings via parameter-averaging mechanisms. It employs multi-task training across a continuum of weightings to maximize the multi-objective finetuning objective, enabling flexible trade-offs at inference time without maintaining multiple models.\n    - Parameter-based conditioning mechanism: The conditioning mechanism maintains multiple sets of parameters for different rewards and linearly combines them based on reward weightings, then mixes with reference parameters based on KL weightings. This produces a single conditioned language model for any given weighting without requiring multiple forward passes.\n\nPaper 9:\n  Title: Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging\n  Abstract: While Reinforcement Learning from Human Feedback (RLHF) aligns Large Language Models (LLMs) with general, aggregate human preferences, it is suboptimal for learning diverse, individual perspectives. In this work, we study Reinforcement Learning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are aligned to multiple (sometimes conflicting) preferences by modeling alignment as a Multi-Objective Reinforcement Learning (MORL) problem. Compared to strong single-objective baselines, we show that we can achieve personalized alignment by decomposing preferences into multiple dimensions. These dimensions are defined based on personalizations that are declared as desirable by the user. In this work, we show that they can be efficiently trained independently in a distributed manner and combined effectively post-hoc through parameter merging. The code is available at this https URL.\n  Structured Summary:\n    - Suboptimal alignment of LLMs to individual preferences: The paper addresses the challenge of aligning large language models (LLMs) to diverse and potentially conflicting individual human preferences, which is not effectively handled by existing RLHF methods.\n    - Need for personalized alignment in LLMs: The authors are motivated by the need to provide users with control over the alignment of LLMs to their specific preferences, enabling more personalized and adaptable interactions.\n    - Reinforcement Learning from Personalized Human Feedback (RLPHF): The authors propose RLPHF, a multi-objective reinforcement learning framework that aligns LLMs with diverse and potentially conflicting human preferences by treating alignment as a multi-objective optimization problem.\n    - PERSONALIZED SOUPS: The authors introduce PERSONALIZED SOUPS, a method that trains separate policy models for individual preferences and combines their parameters dynamically during inference, enabling efficient and scalable alignment.\n\nPaper 10:\n  Title: AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback\n  Abstract: The notable success of large language models (LLMs) has sparked an upsurge in building language agents to complete various complex tasks. We present AMOR, an agent framework based on open-source LLMs, which reasons with external knowledge bases and adapts to specific domains through human supervision to the reasoning process. AMOR builds reasoning logic over a finite state machine (FSM) that solves problems through autonomous executions and transitions over disentangled modules. This allows humans to provide direct feedback to the individual modules, and thus naturally forms process supervision. Based on this reasoning and feedback framework, we develop AMOR through two-stage fine-tuning: warm-up and adaptation. The former fine-tunes the LLM with examples automatically constructed from various public datasets, enabling AMOR to generalize across different knowledge environments, while the latter tailors AMOR to specific domains using process feedback. Extensive experiments across multiple domains demonstrate the advantage of AMOR to strong baselines, thanks to its FSM-based reasoning and process feedback mechanism. The code and data are publicly available at \\url{this https URL}.\n  Structured Summary:\n    - Building adaptable knowledge agents with controllable reasoning and human supervision: Existing language agents struggle to simultaneously provide robust reasoning logic, adaptive mechanisms for specific environments, and amenable processes for human intervention through direct feedback, especially when built on less powerful open-source LLMs.\n    - Need for agents with structured reasoning, adaptability, and human-steerable processes: The authors identify three critical desiderata for knowledge agents: robust reasoning logic for precise problem-solving pathways, adaptive mechanisms for environment-specific adjustment, and reasoning processes that allow human steering through direct process feedback rather than only outcome feedback.\n    - FSM-based reasoning logic with modular architecture: AMOR employs a finite state machine framework that decomposes complex tasks into disentangled modules (LLM modules and tool modules) with explicit inter-step dependencies, enabling structured thinking, skill disentanglement, and intervenable workflows for knowledge-intensive tasks.\n    - Two-stage fine-tuning strategy with warm-up and adaptation: The training approach consists of a warm-up stage that fine-tunes open-source LLMs on 50k examples from multiple public datasets to enable generalization, followed by an adaptation stage that tailors the agent to target domains through exploration, process feedback collection, and exploitation via further fine-tuning.\n\nPaper 11:\n  Title: FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning\n  Abstract: LLMs have demonstrated great capabilities in various NLP tasks. Different entities can further improve the performance of those LLMs on their specific downstream tasks by fine-tuning LLMs. When several entities have similar interested tasks, but their data cannot be shared because of privacy concerns regulations, federated learning (FL) is a mainstream solution to leverage the data of different entities. However, fine-tuning LLMs in federated learning settings still lacks adequate support from existing FL frameworks because it has to deal with optimizing the consumption of significant communication and computational resources, data preparation for different tasks, and distinct information protection demands. This paper first discusses these challenges of federated fine-tuning LLMs, and introduces our package FS-LLM as a main contribution, which consists of the following components: (1) we build an end-to-end benchmarking pipeline, automizing the processes of dataset preprocessing, federated fine-tuning execution, and performance evaluation on federated LLM fine-tuning; (2) we provide comprehensive federated parameter-efficient fine-tuning algorithm implementations and versatile programming interfaces for future extension in FL scenarios with low communication and computation costs, even without accessing the full model; (3) we adopt several accelerating and resource-efficient operators for fine-tuning LLMs with limited resources and the flexible pluggable sub-routines for interdisciplinary study. We conduct extensive experiments to validate the effectiveness of FS-LLM and benchmark advanced LLMs with state-of-the-art parameter-efficient fine-tuning algorithms in FL settings, which also yields valuable insights into federated fine-tuning LLMs for the research community. To facilitate further research and adoption, we release FS-LLM at this https URL.\n  Structured Summary:\n    - Inadequate support for fine-tuning LLMs in federated learning settings: Existing federated learning frameworks do not adequately support fine-tuning large language models due to challenges in managing high communication and computational costs, diverse data preparation requirements across tasks, and varying information protection needs. This gap prevents effective collaborative training of LLMs across multiple entities with privacy constraints.\n    - Enabling collaborative LLM fine-tuning without direct data sharing: The authors are motivated by the need to allow multiple entities to improve LLM performance on domain-specific tasks using their private data without direct sharing. Federated learning provides a solution that respects privacy regulations while enabling collaborative model improvement and can achieve rigorous data privacy protection and model customization.\n    - Complete end-to-end benchmarking pipeline for federated LLM fine-tuning: The authors develop an automated pipeline that handles dataset preprocessing, execution or simulation of federated fine-tuning, and performance evaluation. This pipeline includes diverse federated datasets from various domains with tunable heterogeneity levels and corresponding evaluation tasks to benchmark algorithms under different FL scenarios.\n    - Comprehensive federated parameter-efficient fine-tuning algorithm implementations: The package provides ready-to-use implementations of PEFT algorithms (LoRA, prefix-tuning, P-tuning, prompt tuning) adapted for federated settings, along with flexible programming interfaces. These enable fine-tuning LLMs with low communication and computation costs, even supporting scenarios where clients cannot access the full model such as closed-source LLMs.\n\nPaper 12:\n  Title: FedRLHF: A Convergence-Guaranteed Federated Framework for Privacy-Preserving and Personalized RLHF\n  Abstract: In the era of increasing privacy concerns and demand for personalized experiences, traditional Reinforcement Learning with Human Feedback (RLHF) frameworks face significant challenges due to their reliance on centralized data. We introduce Federated Reinforcement Learning with Human Feedback (FedRLHF), a novel framework that decentralizes the RLHF process. FedRLHF enables collaborative policy learning across multiple clients without necessitating the sharing of raw data or human feedback, thereby ensuring robust privacy preservation. Leveraging federated reinforcement learning, each client integrates human feedback locally into their reward functions and updates their policies through personalized RLHF processes. We establish rigorous theoretical foundations for FedRLHF, providing convergence guarantees, and deriving sample complexity bounds that scale efficiently with the number of clients. Empirical evaluations on the MovieLens and IMDb datasets demonstrate that FedRLHF not only preserves user privacy but also achieves performance on par with centralized RLHF, while enhancing personalization across diverse client environments.\n  Structured Summary:\n    - Privacy and personalization challenges in centralized RLHF: Centralized RLHF systems aggregate user data and feedback, creating privacy risks (e.g., violating HIPAA, GDPR) and limiting personalization, as a single global policy cannot effectively cater to diverse user preferences.\n    - Need for privacy-preserving and personalized RLHF: The authors are motivated by growing privacy regulations and the need for personalized AI systems that respect individual user preferences without centralizing sensitive data or feedback.\n    - FedRLHF framework: FedRLHF decentralizes RLHF by enabling clients to perform local policy updates using private human feedback and intrinsic rewards, sharing only model updates (not raw data) with a central server for aggregation.\n    - Client-specific reward shaping with human feedback: Each client shapes its reward function as Rk = R0_k + λHk, where R0_k is intrinsic reward, Hk is local human feedback, and λ controls personalization, allowing diverse client preferences without sharing feedback.\n\nPaper 13:\n  Title: Collective Constitutional AI: Aligning a Language Model with Public Input\n  Abstract: There is growing consensus that language model (LM) developers should not be the sole deciders of LM behavior, creating a need for methods that enable the broader public to collectively shape the behavior of LM systems that affect them. To address this need, we present Collective Constitutional AI (CCAI): a multi-stage process for sourcing and integrating public input into LMs-from identifying a target population to sourcing principles to training and evaluating a model. We demonstrate the real-world practicality of this approach by creating what is, to our knowledge, the first LM fine-tuned with collectively sourced public input and evaluating this model against a baseline model trained with established principles from a LM developer. Our quantitative evaluations demonstrate several benefits of our approach: the CCAI-trained model shows lower bias across nine social dimensions compared to the baseline model, while maintaining equivalent performance on language, math, and helpful-harmless evaluations. Qualitative comparisons of the models suggest that the models differ on the basis of their respective constitutions, e.g., when prompted with contentious topics, the CCAI-trained model tends to generate responses that reframe the matter positively instead of a refusal. These results demonstrate a promising, tractable pathway toward publicly informed development of language models.\n  Structured Summary:\n    - Lack of public input in language model alignment: The paper addresses the challenge that current language model development lacks systematic methods for incorporating public preferences and values into model behavior, with developers making normative decisions that affect diverse populations without their meaningful input.\n    - Democratic legitimacy and harm mitigation in AI systems: The authors are motivated by the need to ensure AI systems reflect diverse societal values and to mitigate harms through collaboration with affected communities, particularly as language models become more widely deployed and impact varied populations.\n    - Collective Constitutional AI (CCAI) framework: The authors propose CCAI, a multi-stage framework that combines the Polis deliberation platform to collect and aggregate public input into constitutional principles, then uses Constitutional AI training methods to fine-tune language models according to these collectively-sourced principles.\n    - Group-aware consensus (GAC) for principle selection: The authors use a group-aware consensus metric to select constitutional principles that have broad support across different opinion groups, protecting against tyranny of the majority by favoring statements with cross-group agreement rather than those with strong support from only one group.\n\nPaper 14:\n  Title: PERSOMA: PERsonalized SOft ProMpt Adapter Architecture for Personalized Language Prompting\n  Abstract: Understanding the nuances of a user's extensive interaction history is key to building accurate and personalized natural language systems that can adapt to evolving user preferences. To address this, we introduce PERSOMA, Personalized Soft Prompt Adapter architecture. Unlike previous personalized prompting methods for large language models, PERSOMA offers a novel approach to efficiently capture user history. It achieves this by resampling and compressing interactions as free form text into expressive soft prompt embeddings, building upon recent research utilizing embedding representations as input for LLMs. We rigorously validate our approach by evaluating various adapter architectures, first-stage sampling strategies, parameter-efficient tuning techniques like LoRA, and other personalization methods. Our results demonstrate PERSOMA's superior ability to handle large and complex user histories compared to existing embedding-based and text-prompt-based techniques.\n  Structured Summary:\n    - Ineffective personalization in LLMs with extensive user histories: Existing text-based prompting methods for personalizing large language models struggle with long user histories due to context length limitations, information loss in lengthy prompts, and distraction from irrelevant context, raising questions about the effectiveness of natural language prompting for personalization at scale.\n    - Need for efficient personalization with large-scale user histories: The authors are motivated by the need to build personalized natural language systems that can effectively leverage extensive user interaction histories to adapt to evolving preferences, addressing limitations in current LLM personalization approaches that rely on lengthy text prompts.\n    - PERSOMA soft prompt adapter architecture: PERSOMA is a novel architecture that uses a soft prompt adapter network to compress and resample user historical interactions into compact expressive prompt embeddings. These embeddings are mapped to the LLM vocabulary space and concatenated with task prompts to steer a frozen LLM toward personalized outputs without sacrificing in-context learning capabilities.\n    - History encoder with SentenceT5 and adapter networks: The method employs SentenceT5 to encode natural language user interactions into embeddings, which are then processed by soft prompt adapter networks (including Perceiver, Transformer, or MLP variants) to generate personalized soft prompt tokens that jointly compress and resample the user history.\n\nPaper 15:\n  Title: FDLoRA: Personalized Federated Learning of Large Language Model via Dual LoRA Tuning\n  Abstract: Large language models (LLMs) have emerged as important components across various fields, yet their training requires substantial computation resources and abundant labeled data. It poses a challenge to robustly training LLMs for individual users (clients). To tackle this challenge, the intuitive idea is to introduce federated learning (FL), which can collaboratively train models on distributed private data. However, existing methods suffer from the challenges of data heterogeneity, system heterogeneity, and model size, resulting in suboptimal performance and high costs. In this work, we proposed a variant of personalized federated learning (PFL) framework, namely FDLoRA, which allows the client to be a single device or a cluster and adopts low-rank adaptation (LoRA) tuning. FDLoRA sets dual LoRA modules on each client to capture personalized and global knowledge, respectively, and only the global LoRA module uploads parameters to the central server to aggregate cross-client knowledge. Finally, an adaptive fusion approach is employed to combine the parameters of the dual LoRAs. This enables FDLoRA to make effective use of private data distributed across different clients, thereby improving performance on the client without incurring high communication and computing costs. We conducted extensive experiments in two practice scenarios. The results demonstrate that FDLoRA outperforms six baselines in terms of performance, stability, robustness, computation cost, and communication cost.\n  Structured Summary:\n    - Training LLMs for individual users with limited labeled data and high costs: The paper addresses the challenge of training large language models for individual users or clients when computational resources are limited and labeled data is scarce, making traditional training approaches prohibitively expensive and impractical.\n    - Leveraging distributed private data while maintaining privacy and reducing costs: The authors are motivated by the need to collaboratively train models on distributed private data across clients while maintaining data privacy, improving individual client performance, and avoiding the high communication and computational costs associated with existing federated learning methods for LLMs.\n    - Dual LoRA modules for personalized and global knowledge: The framework initializes two separate LoRA modules on each client: a personalized LoRA module that captures client-specific knowledge from local data without sharing parameters, and a global LoRA module that uploads parameters to the server for aggregation to integrate knowledge across different clients.\n    - Adaptive fusion approach for combining dual LoRAs: The method employs a gradient-free optimization technique called AdaFusion to adaptively combine the parameters of both the personalized and global LoRA modules, dynamically adjusting fusion weights to enhance performance on each client's local task.\n\nPaper 16:\n  Title: Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning\n  Abstract: Personalization in large language models (LLMs) is increasingly important, aiming to align the LLMs' interactions, content, and recommendations with individual user preferences. Recent advances have highlighted effective prompt design by enriching user queries with non-parametric knowledge through behavior history retrieval and textual profiles. However, these methods faced limitations due to a lack of model ownership, resulting in constrained customization and privacy issues, and often failed to capture complex, dynamic user behavior patterns. To address these shortcomings, we introduce One PEFT Per User (OPPU), employing personalized parameter-efficient fine-tuning (PEFT) modules to store user-specific behavior patterns and preferences. By plugging in personal PEFT parameters, users can own and use their LLMs individually. OPPU integrates parametric user knowledge in the personal PEFT parameters with non-parametric knowledge from retrieval and profiles, adapting LLMs to user behavior shifts. Experimental results demonstrate that OPPU significantly outperforms existing prompt-based methods across seven diverse tasks in the LaMP benchmark. Further studies reveal OPPU's enhanced capabilities in handling user behavior shifts, modeling users at different activity levels, maintaining robustness across various user history formats, and displaying versatility with different PEFT methods.\n  Structured Summary:\n    - Lack of model ownership and poor generalization to user behavior shifts in LLM personalization: Current LLM personalization methods rely on centralized models with prompt-based approaches, which limit users' ability to own and customize their models while raising privacy concerns. Additionally, these methods struggle to generalize when user behavior patterns shift or when retrieved history is not highly relevant to current queries.\n    - Enabling democratized, privacy-preserving personalized LLMs with better adaptation to behavior changes: The authors are motivated to address the ownership and behavior shift challenges in LLM personalization. They aim to enable users to possess their own personalized models through lightweight PEFT modules, enhancing both privacy and the model's ability to adapt to evolving user preferences without heavy reliance on highly relevant retrieved examples.\n    - One PEFT Per User (OPPU) framework: OPPU equips each user with a personal PEFT module (e.g., LoRA) that is fine-tuned on the user's behavior history. This module encapsulates parametric user knowledge and can be plugged into base LLMs, allowing users to own their personalized models. The framework supports integration with non-parametric knowledge via retrieval and profile augmentation.\n    - Integration of parametric and non-parametric user knowledge: The method combines user-specific patterns stored in PEFT parameters (parametric knowledge) with dynamically retrieved behavior history and textual user profiles (non-parametric knowledge). This hybrid approach enhances personalization by leveraging both learned representations and contextual information.\n\nPaper 17:\n  Title: RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation\n  Abstract: Reinforcement learning from human feedback (RLHF) has been an effective technique for aligning AI systems with human values, with remarkable successes in fine-tuning large-language models recently. Most existing RLHF paradigms make the underlying assumption that human preferences are relatively homogeneous, and can be encoded by a single reward model. In this paper, we focus on addressing the issues due to the inherent heterogeneity in human preferences, as well as their potential strategic behavior in providing feedback. Specifically, we propose two frameworks to address heterogeneous human feedback in principled ways: personalization-based one and aggregation-based one. For the former, we propose two approaches based on representation learning and clustering, respectively, for learning multiple reward models that trades off the bias (due to preference heterogeneity) and variance (due to the use of fewer data for learning each model by personalization). We then establish sample complexity guarantees for both approaches. For the latter, we aim to adhere to the single-model framework, as already deployed in the current RLHF paradigm, by carefully aggregating diverse and truthful preferences from humans. We propose two approaches based on reward and preference aggregation, respectively: the former utilizes both utilitarianism and Leximin approaches to aggregate individual reward models, with sample complexity guarantees; the latter directly aggregates the human feedback in the form of probabilistic opinions. Under the probabilistic-opinion-feedback model, we also develop an approach to handle strategic human labelers who may bias and manipulate the aggregated preferences with untruthful feedback. Based on the ideas in mechanism design, our approach ensures truthful preference reporting, with the induced aggregation rule maximizing social welfare functions.\n  Structured Summary:\n    - Heterogeneity in human preferences in RLHF: Existing RLHF methods assume homogeneous human preferences that can be captured by a single reward model, but this assumption fails when users have diverse preferences. The paper addresses the challenge of learning reward models from heterogeneous user preference data, where individual feedback may be insufficient and users may behave strategically.\n    - Aligning AI systems with diverse human values: The authors are motivated by the need to align increasingly powerful AI models with human preferences, particularly when the target user group is heterogeneous. Accurately approximating diverse user preferences is critical but becomes significantly more challenging with heterogeneous users.\n    - Personalization via representation learning: The authors propose learning personalized reward models for each user by assuming individual reward functions share a common representation structure. They model each reward as the inner product of a shared representation and an individual parameter vector, leveraging data from all users to learn the representation while estimating individual parameters.\n    - Personalization via clustering: The authors develop a clustering-based approach that groups users into clusters and learns a reward model for each cluster. This method addresses the bias-variance tradeoff by reducing variance through pooling data within clusters while managing bias from preference heterogeneity across clusters.\n\nPaper 18:\n  Title: MACRec: a Multi-Agent Collaboration Framework for Recommendation\n  Abstract: LLM-based agents have gained considerable attention for their decision-making skills and ability to handle complex tasks. Recognizing the current gap in leveraging agent capabilities for multi-agent collaboration in recommendation systems, we introduce MACRec, a novel framework designed to enhance recommendation systems through multi-agent collaboration. Unlike existing work on using agents for user/item simulation, we aim to deploy multi-agents to tackle recommendation tasks directly. In our framework, recommendation tasks are addressed through the collaborative efforts of various specialized agents, including Manager, User/Item Analyst, Reflector, Searcher, and Task Interpreter, with different working flows. Furthermore, we provide application examples of how developers can easily use MACRec on various recommendation tasks, including rating prediction, sequential recommendation, conversational recommendation, and explanation generation of recommendation results. The framework and demonstration video are publicly available at this https URL.\n  Structured Summary:\n    - Gap in leveraging multi-agent collaboration for recommendation tasks: Existing work primarily uses agents for simulating user or item behaviors rather than directly tackling recommendation tasks. Single-agent approaches fail to handle complex recommendation scenarios effectively, and prior multi-agent frameworks have limited agent types and fixed collaboration modes.\n    - Unleashing multi-agent collaboration potential for complex recommendation tasks: Multi-agent collaboration, mirroring human workflows, is believed to accomplish complex recommendation tasks better through collective intelligence. The authors aim to harness diverse agent capabilities to build recommenders directly rather than just simulating behaviors.\n    - MACRec multi-agent collaboration framework with specialized agents: MACRec provides customizable agents with different roles powered by LLMs and tools, including Manager (plans and manages task execution), Reflector (reflects on errors), User/Item Analysts (analyze characteristics), Searcher (searches information), and Task Interpreter (translates dialogs into tasks). These agents work collaboratively to tackle specific recommendation tasks.\n    - Application examples on diverse recommendation scenarios: The framework demonstrates how to select and customize agents for different recommendation tasks. Each scenario requires different agent combinations: rating prediction uses User and Item Analysts, sequential recommendation adds Reflector, explanation generation includes Searcher, and conversational recommendation employs Task Interpreter.\n\nPaper 19:\n  Title: JumpStarter: Human-AI Planning with Task-Structured Context Curation\n  Abstract: Human-AI planning for complex goals remains challenging with current large language models (LLMs), which rely on linear chat histories and simplistic memory mechanisms. Despite advances in long-context prompting, users still manually manage information, leading to a high cognitive burden. Hence, we propose JumpStarter, a system that enables LLMs to collaborate with humans on complex goals by dynamically decomposing tasks to help users manage context. We specifically introduce task-structured context curation, a novel framework that breaks down a user's goal into a hierarchy of actionable subtasks, and scopes context to localized decision points, enabling finer-grained personalization and reuse. The framework is realized through three core mechanisms: context elicitation, selection, and reuse. We demonstrate that task-structured context curation significantly improves plan quality by 16% over ablations. Our user study shows that JumpStarter helped users generate plans with 79% higher quality compared to ChatGPT.\n  Structured Summary:\n    - Challenges in human-AI planning with current LLMs: Current LLMs struggle to maintain context over long interactions for complex goal planning. Users must actively manage information through intensive prompt engineering, manually curating chat histories and resupplying information, resulting in high cognitive burden and generic outputs misaligned with prior interactions.\n    - Need for structured, transparent, and efficient human-AI planning: The authors are motivated by the need to reduce user burden in managing context while enabling personalized, goal-oriented planning. They aim to create systems where tasks and context are treated as dynamic, composable units rather than requiring manual curation through opaque memory mechanisms.\n    - Task-structured context curation framework: A framework that decomposes user goals into hierarchical actionable subtasks and scopes context to localized decision points. It enables finer-grained personalization and reuse through three core mechanisms: context elicitation (prompting for missing information), context selection (surfacing relevant prior inputs), and context reuse (incorporating approved drafts into future subtasks).\n    - JumpStarter system implementation: An interactive system that operationalizes task-structured context curation. It takes user goals, elicits context, decomposes goals into subtasks, helps users select relevant context, generates answer drafts, and supports iterative refinement. The system uses hierarchical task trees and enables task forking for parallel subtasks.\n\nPaper 20:\n  Title: TravelPlanner: A Benchmark for Real-World Planning with Language Agents\n  Abstract: Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of 0.6%. Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints. However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress. TravelPlanner provides a challenging yet meaningful testbed for future language agents.\n  Structured Summary:\n    - Limitations of language agents in complex real-world planning: The paper addresses whether current language agents powered by large language models can handle complex, multi-constraint planning tasks in realistic settings, which have been beyond the capabilities of earlier AI agents that operated in constrained environments.\n    - Need for realistic evaluation of language agent planning capabilities: Existing evaluation methods for language agents focus on single objectives with fixed ground truths, failing to capture the complexity of real-world planning scenarios that involve multiple interdependent constraints and require strong agency for information gathering and decision-making.\n    - TravelPlanner benchmark with sandbox environment and diverse tools: The authors introduce TravelPlanner, a benchmark centered on travel planning that includes a static sandbox environment with approximately four million data entries accessible through six tools, along with 1,225 diverse queries and human-annotated reference plans to evaluate language agents.\n    - Multi-constraint evaluation framework with three constraint types: The benchmark evaluates agents across three constraint categories: environment constraints testing adaptability to dynamic conditions, commonsense constraints assessing implicit reasoning, and hard constraints measuring satisfaction of explicit user needs like budget and preferences.\n\nPaper 21:\n  Title: Prospect Personalized Recommendation on Large Language Model-based Agent Platform\n  Abstract: The new kind of Agent-oriented information system, exemplified by GPTs, urges us to inspect the information system infrastructure to support Agent-level information processing and to adapt to the characteristics of Large Language Model (LLM)-based Agents, such as interactivity. In this work, we envisage the prospect of the recommender system on LLM-based Agent platforms and introduce a novel recommendation paradigm called Rec4Agentverse, comprised of Agent Items and Agent Recommender. Rec4Agentverse emphasizes the collaboration between Agent Items and Agent Recommender, thereby promoting personalized information services and enhancing the exchange of information beyond the traditional user-recommender feedback loop. Additionally, we prospect the evolution of Rec4Agentverse and conceptualize it into three stages based on the enhancement of the interaction and information exchange among Agent Items, Agent Recommender, and the user. A preliminary study involving several cases of Rec4Agentverse validates its significant potential for application. Lastly, we discuss potential issues and promising directions for future research.\n  Structured Summary:\n    - Adapting recommender systems to LLM-based Agent platforms: The paper addresses the challenge of redesigning recommender system infrastructure to support Agent-level information processing on LLM-based Agent platforms, which exhibit novel characteristics like interactivity, intelligence, and proactiveness that differ fundamentally from traditional passive items.\n    - Leveraging Agent properties for personalized information services: The authors are motivated by the opportunity to exploit the unique properties of LLM-based Agents—such as interactivity and proactiveness—to enable deeper collaboration between recommended items and recommender systems, thereby enhancing personalized information services beyond traditional user-recommender feedback loops.\n    - Rec4Agentverse paradigm with Agent Items and Agent Recommender: The authors propose Rec4Agentverse, a new recommendation paradigm where LLM-based Agents serve as recommended items (Agent Items) and an Agent Recommender provides personalized agent recommendations. This paradigm emphasizes collaboration and information exchange between Agent Items and Agent Recommender.\n    - Three-stage evolution framework for Rec4Agentverse: The authors conceptualize the development of Rec4Agentverse into three progressive stages: User-Agent Interaction (users interact with Agent Items), Agent-Recommender Collaboration (Agent Items and Agent Recommender exchange information), and Agents Collaboration (multiple Agent Items collaborate to serve users).\n\nPaper 22:\n  Title: User-LLM: Efficient LLM Contextualization with User Embeddings\n  Abstract: Large language models (LLMs) have achieved remarkable success across various domains, but effectively incorporating complex and potentially noisy user timeline data into LLMs remains a challenge. Current approaches often involve translating user timelines into text descriptions before feeding them to LLMs, which can be inefficient and may not fully capture the nuances of user behavior. Inspired by how LLMs are effectively integrated with images through direct embeddings, we propose User-LLM, a novel framework that leverages user embeddings to directly contextualize LLMs with user history interactions. These embeddings, generated by a user encoder pretrained using self-supervised learning on diverse user interactions, capture latent user behaviors and interests as well as their evolution over time. We integrate these user embeddings with LLMs through cross-attention, enabling LLMs to dynamically adapt their responses based on the context of a user's past actions and preferences.\nOur approach achieves significant efficiency gains by representing user timelines directly as embeddings, leading to substantial inference speedups of up to 78.1X. Comprehensive experiments on MovieLens, Amazon Review, and Google Local Review datasets demonstrate that User-LLM outperforms text-prompt-based contextualization on tasks requiring deep user understanding, with improvements of up to 16.33%, particularly excelling on long sequences that capture subtle shifts in user behavior. Furthermore, the incorporation of Perceiver layers streamlines the integration between user encoders and LLMs, yielding additional computational savings.\n  Structured Summary:\n    - Inefficient and incomplete incorporation of user timeline data into LLMs: Existing methods for integrating user interaction histories into LLMs rely on converting timelines into lengthy text descriptions, which creates computational overhead due to long context windows and fails to capture subtle behavioral patterns essential for personalization.\n    - Enabling personalized AI agents through efficient user understanding: The authors aim to leverage LLMs' capabilities to build personalized AI systems that can understand and respond to individual user needs by effectively incorporating rich user behavioral data from diverse interaction timelines.\n    - USER-LLM framework with user embeddings and cross-attention integration: USER-LLM treats user timelines as a distinct modality, using a pretrained Transformer-based user encoder to generate compact embeddings from ID-based interaction sequences. These embeddings are integrated with LLMs via cross-attention layers, enabling dynamic adaptation to user contexts without lengthy text prompts.\n    - Two-stage training framework with encoder pretraining and co-training strategies: The framework first pretrains the user encoder using self-supervised learning on user interaction sequences, then finetunes the integrated system with four flexible strategies (Full, Enc, LoRA, Proj) to align user embeddings with LLM text representations while preserving pretrained knowledge.\n\nPaper 23:\n  Title: Personalized Language Modeling from Personalized Human Feedback\n  Abstract: Personalized large language models (LLMs) are designed to tailor responses to individual user preferences. While Reinforcement Learning from Human Feedback (RLHF) is a commonly used framework for aligning LLMs with human preferences, vanilla RLHF assumes that all human preferences share the same distribution, preventing fine-tuned LLMs from generating personalized content when user preferences are diverse. In this work, we propose Personalized-RLHF (P-RLHF), an efficient framework that utilizes a lightweight user model to capture individual user preferences and jointly learns the user model and the personalized LLM from human feedback. P-RLHF exhibits the following three characteristics: (1) It enables an LLM to generate personalized content and scale efficiently with growing number of users. (2) It handles both explicit user preferences described as textual input and implicit user preferences encoded in the feedback data. (3) It eliminates the need for users to fully articulate their preferences, which are normally needed for prompting LLMs to generate personalized content yet are often impractical to obtain in real-world scenarios. Our experimental results show that personalized LLMs trained using P-RLHF generate responses that are more closely aligned with individual user preferences, outperforming vanilla, non-personalized RLHF and prompting-based personalization approaches across different tasks. We opensource our code at this https URL.\n  Structured Summary:\n    - Vanilla RLHF assumes uniform user preferences, limiting personalization: Standard RLHF methods implicitly assume all users have identical preferences, which causes fine-tuned LLMs to fail at generating personalized responses when users exhibit diverse or conflicting preferences. This uniform preference assumption leads to majority-voting behavior that can misalign with minority user preferences.\n    - Need for efficient personalized LLMs that scale with diverse user bases: The authors are motivated by the need to personalize LLMs for diverse user preferences without the computational burden of training separate models per user or preference dimension. Existing personalization methods are inefficient and inflexible, especially when user preferences cannot be fully articulated or enumerated.\n    - Personalized-RLHF (P-RLHF) framework with lightweight user model: P-RLHF introduces a general framework consisting of a lightweight user model that extracts user embeddings from user information and a base LLM. The user model captures both explicit preferences (from textual user information) and implicit preferences (from feedback data), enabling efficient personalization without training separate models per user.\n    - User model designs encoding preference assumptions: The authors propose multiple user model architectures (uniform, individualized, cluster-based) that encode different structural assumptions about user preference relationships. These designs allow flexible modeling of implicit preferences learned from feedback data, with cluster-based models providing efficient low-rank approximations for large user bases.\n\nPaper 24:\n  Title: Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF\n  Abstract: In practice, preference learning from human feedback depends on incomplete data with hidden context. Hidden context refers to data that affects the feedback received, but which is not represented in the data used to train a preference model. This captures common issues of data collection, such as having human annotators with varied preferences, cognitive processes that result in seemingly irrational behavior, and combining data labeled according to different criteria. We prove that standard applications of preference learning, including reinforcement learning from human feedback (RLHF), implicitly aggregate over hidden contexts according to a well-known voting rule called Borda count. We show this can produce counter-intuitive results that are very different from other methods which implicitly aggregate via expected utility. Furthermore, our analysis formalizes the way that preference learning from users with diverse values tacitly implements a social choice function. A key implication of this result is that annotators have an incentive to misreport their preferences in order to influence the learned model, leading to vulnerabilities in the deployment of RLHF. As a step towards mitigating these problems, we introduce a class of methods called distributional preference learning (DPL). DPL methods estimate a distribution of possible score values for each alternative in order to better account for hidden context. Experimental results indicate that applying DPL to RLHF for LLM chatbots identifies hidden context in the data and significantly reduces subsequent jailbreak vulnerability. Our code and data are available at this https URL\n  Structured Summary:\n    - Hidden context in preference learning affects RLHF outcomes: The paper addresses the challenge that preference learning methods, including RLHF, operate on data where important contextual information (such as annotator identity, cognitive states, or evaluation criteria) influences feedback but is not available to the learned model. This hidden context can lead to unexpected and undesirable outcomes.\n    - Understanding implicit aggregation and its consequences for safe AI deployment: The authors are motivated by the need to understand how standard preference learning methods implicitly aggregate over hidden contexts, as this aggregation can produce counter-intuitive results, create vulnerabilities (such as incentives for annotators to misreport preferences), and impact the safe deployment of systems trained with RLHF.\n    - Formal model of preference learning with hidden context: The authors extend the standard preference learning framework by introducing utility functions u(a,z) that depend on both observed alternatives a and hidden context z. This model encompasses diverse preferences, irrationality, partial observability, and multiple objectives as special cases of hidden context.\n    - Distributional preference learning (DPL): DPL estimates a distribution over utility values for each input rather than a single scalar, allowing the method to detect situations where hidden context influences preferences. The approach can identify hidden context through an explained variance metric and enable risk-averse decision-making to mitigate issues like jailbreaks.\n\nPaper 25:\n  Title: Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning\n  Abstract: Reinforcement Learning from Human Feedback (RLHF) is a powerful paradigm for aligning foundation models to human values and preferences. However, current RLHF techniques cannot account for the naturally occurring differences in individual human preferences across a diverse population. When these differences arise, traditional RLHF frameworks simply average over them, leading to inaccurate rewards and poor performance for individual subgroups. To address the need for pluralistic alignment, we develop a class of multimodal RLHF methods. Our proposed techniques are based on a latent variable formulation - inferring a novel user-specific latent and learning reward models and policies conditioned on this latent without additional user-specific data. While conceptually simple, we show that in practice, this reward modeling requires careful algorithmic considerations around model architecture and reward scaling. To empirically validate our proposed technique, we first show that it can provide a way to combat underspecification in simulated control problems, inferring and optimizing user-specific reward functions. Next, we conduct experiments on pluralistic language datasets representing diverse user preferences and demonstrate improved reward function accuracy. We additionally show the benefits of this probabilistic framework in terms of measuring uncertainty, and actively learning user preferences. This work enables learning from diverse populations of users with divergent preferences, an important challenge that naturally occurs in problems from robot learning to foundation model alignment.\n  Structured Summary:\n    - Inability of current RLHF to account for diverse individual preferences: Standard RLHF methods assume all users share a single utility function and use a unimodal BTL model, which fails when preferences are multi-modal due to diverse user populations. This model misspecification causes reward models to average divergent preferences, resulting in inaccurate rewards and policies that fail to satisfy any distinct user group.\n    - Need for pluralistic alignment to serve diverse populations: The authors are motivated by the need to democratize RLHF to account for diverse human values and preferences, particularly to avoid disadvantaging minority groups whose preferences would be ignored by majority-driven reward models. This is critical for applications ranging from LLMs to assistive robotics where users have fundamentally different preferences.\n    - Variational Preference Learning (VPL) with latent variable formulation: VPL formulates RLHF as a latent variable problem where a latent z represents hidden user context. The method uses variational inference to learn an encoder that infers user-specific latent distributions from preference annotations, and a latent-conditional reward model that predicts rewards based on both state and latent. This is optimized via an evidence lower bound (ELBO) objective.\n    - Scaled rewards via pairwise classification for multi-task learning: To address the problem that different latent values produce vastly different reward scales (which harms multi-user policy optimization), VPL replaces raw BTL rewards with expected preference likelihoods. This approach, termed VPL-SPO, ensures rewards are consistently scaled across different user latents by using probabilities normalized in the [0,1] range.\n\nPaper 26:\n  Title: Selective Prompting Tuning for Personalized Conversations with LLMs\n  Abstract: In conversational AI, personalizing dialogues with persona profiles and contextual understanding is essential. Despite large language models' (LLMs) improved response coherence, effective persona integration remains a challenge. In this work, we first study two common approaches for personalizing LLMs: textual prompting and direct fine-tuning. We observed that textual prompting often struggles to yield responses that are similar to the ground truths in datasets, while direct fine-tuning tends to produce repetitive or overly generic replies. To alleviate those issues, we propose \\textbf{S}elective \\textbf{P}rompt \\textbf{T}uning (SPT), which softly prompts LLMs for personalized conversations in a selective way. Concretely, SPT initializes a set of soft prompts and uses a trainable dense retriever to adaptively select suitable soft prompts for LLMs according to different input contexts, where the prompt retriever is dynamically updated through feedback from the LLMs. Additionally, we propose context-prompt contrastive learning and prompt fusion learning to encourage the SPT to enhance the diversity of personalized conversations. Experiments on the CONVAI2 dataset demonstrate that SPT significantly enhances response diversity by up to 90\\%, along with improvements in other critical performance indicators. Those results highlight the efficacy of SPT in fostering engaging and personalized dialogue generation. The SPT model code (this https URL) is publicly available for further exploration.\n  Structured Summary:\n    - Difficulty in personalizing LLMs for diverse conversational settings: The paper addresses the challenge of effectively personalizing large language models for conversations with diverse settings, including varied persona profiles and conversation lengths, where textual prompting struggles to generate responses aligned with ground truths and direct fine-tuning produces repetitive or generic replies.\n    - Overfitting to small-scale persona-based dialogue datasets: When LLMs are tuned on small datasets like PersonaChat, they risk overfitting to specific persona profiles, resulting in repetitive responses that are overly aligned with the persona at the cost of dynamic and contextually appropriate interactions.\n    - Enhancing personalization and diversity in LLM-based dialogue systems: The authors are motivated by the need to create personalized dialogue systems that adapt conversations to individual preferences and contexts, fostering deeper user connections and more intuitive interactions, while addressing the limitations of current LLM approaches in achieving effective persona integration.\n    - Selective Prompt Tuning (SPT) framework: SPT is a framework that combines a soft prompt group with a trainable dense retriever to adaptively select appropriate soft prompts based on input contexts. The retriever is dynamically updated through feedback from the LLM, enabling the model to discover intrinsic patterns in personalized dialogues without explicit annotations.\n\nPaper 27:\n  Title: Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction\n  Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities in generalizing to new tasks in a zero-shot or few-shot manner. However, the extent to which LLMs can comprehend user preferences based on their previous behavior remains an emerging and still unclear research question. Traditionally, Collaborative Filtering (CF) has been the most effective method for these tasks, predominantly relying on the extensive volume of rating data. In contrast, LLMs typically demand considerably less data while maintaining an exhaustive world knowledge about each item, such as movies or products. In this paper, we conduct a thorough examination of both CF and LLMs within the classic task of user rating prediction, which involves predicting a user's rating for a candidate item based on their past ratings. We investigate various LLMs in different sizes, ranging from 250M to 540B parameters and evaluate their performance in zero-shot, few-shot, and fine-tuning scenarios. We conduct comprehensive analysis to compare between LLMs and strong CF methods, and find that zero-shot LLMs lag behind traditional recommender models that have the access to user interaction data, indicating the importance of user interaction data. However, through fine-tuning, LLMs achieve comparable or even better performance with only a small fraction of the training data, demonstrating their potential through data efficiency.\n  Structured Summary:\n    - Understanding user preferences with LLMs for rating prediction: The paper addresses the open question of whether large language models can effectively understand and predict user preferences based on historical behavior, specifically in the context of user rating prediction tasks.\n    - Comparing LLMs with traditional collaborative filtering methods: The authors are motivated to investigate whether LLMs, which require less data and possess extensive world knowledge, can compete with traditional collaborative filtering methods that rely heavily on large volumes of rating data.\n    - Zero-shot and few-shot prompting for rating prediction: The authors design prompts that encode user rating history and item metadata as text, enabling LLMs to predict ratings in zero-shot and few-shot settings without task-specific training.\n    - Fine-tuning LLMs with classification and regression objectives: The authors propose two fine-tuning approaches for LLMs on rating prediction: treating it as multi-class classification with cross-entropy loss or as regression with MSE loss, adapting the model's output layer accordingly.\n\nPaper 28:\n  Title: P-React: Synthesizing Topic-Adaptive Reactions of Personality Traits via Mixture of Specialized LoRA Experts\n  Abstract: Personalized large language models (LLMs) have attracted great attention in many applications, such as emotional support and role-playing. However, existing works primarily focus on modeling explicit character profiles, while ignoring the underlying personality traits that truly shape behaviors and decision-making, hampering the development of more anthropomorphic and psychologically-grounded AI systems. In this paper, we explore the modeling of Big Five personality traits, which is the most widely used trait theory in psychology, and propose P-React, a mixture of experts (MoE)-based personalized LLM. Particularly, we integrate a Personality Specialization Loss (PSL) to better capture individual trait expressions, providing a more nuanced and psychologically grounded personality simulacrum. To facilitate research in this field, we curate OCEAN-Chat, a high-quality, human-verified dataset designed to train LLMs in expressing personality traits across diverse topics. Extensive experiments demonstrate the effectiveness of P-React in maintaining consistent and real personality.\n  Structured Summary:\n    - Lack of psychological grounding in personalized LLMs: Current personalized large language models focus on surface-level character attributes (name, skill, experience) rather than deeper psychological personality traits that actually drive human behavior and decision-making, limiting their authenticity and psychological realism.\n    - Need for consistent and realistic personality traits in AI applications: The authors are motivated by applications like psychological counseling and digital tutoring that demand consistent, realistic personality simulation. Additionally, the scarcity of large-scale, high-quality dialogue datasets with personality annotations has hindered progress in this area.\n    - P-React: Mixture of LoRA experts with personality-guided routing: P-React is a mixture-of-experts model where each expert is a LoRA module. A personality-guided routing mechanism assigns unique combinations of expert weights based on the Big Five personality trait being simulated, enabling trait-specific modeling.\n    - Personality Specialization Loss (PSL): PSL is a novel auxiliary loss that encourages different experts to specialize in distinct personality traits by minimizing the similarity of expert weight distributions across traits, ensuring that each expert captures unique trait-specific reactions rather than learning all traits uniformly.\n\nPaper 29:\n  Title: Fine-Grained Human Feedback Gives Better Rewards for Language Model Training\n  Abstract: Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF) - where human preference judgments on LM outputs are transformed into a learning signal - has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how learning with such reward functions leads to improved performance, supported by both automatic and human evaluation. Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models. We release all data, collected human feedback, and codes at this https URL.\n  Structured Summary:\n    - Limited information from holistic feedback in RLHF: Existing RLHF methods use holistic human preference judgments that provide only a single scalar reward for entire generated sequences. This sparse signal fails to localize errors or distinguish between different types of undesired behaviors, making training unreliable especially for long-form text generation tasks.\n    - Improving reward signals through fine-grained feedback: The authors are motivated by prior research showing that fine-grained evaluation (identifying specific error types and their locations) offers clearer insights than holistic quality comparisons. They aim to leverage this principle to create more informative training signals for language models through RLHF.\n    - FINE-GRAINED RLHF framework: The proposed framework trains multiple reward models, each targeting a specific error category (e.g., toxicity, factual incorrectness, irrelevance) and providing rewards at different densities (e.g., sub-sentence, sentence, or sequence level). These fine-grained rewards are integrated into PPO training to optimize the policy language model.\n    - QA-FEEDBACK dataset with fine-grained annotations: The authors create a new dataset for long-form question answering based on ASQA, annotated with fine-grained human feedback across three error categories at different densities: irrelevance/repetition/incoherence (sub-sentence level), incorrect/unverifiable facts (sentence level), and incomplete information (sequence level).\n\nPaper 30:\n  Title: Aligning to Thousands of Preferences via System Message Generalization\n  Abstract: Although humans inherently have diverse values, current large language model (LLM) alignment methods often assume that aligning LLMs with the general public's preferences is optimal. A major challenge in adopting a more individualized approach to LLM alignment is its lack of scalability, as it involves repeatedly acquiring preference data and training new reward models and LLMs for each individual's preferences. To address these challenges, we propose a new paradigm where users specify what they value most within the system message, steering the LLM's generation behavior to better align with the user's intentions. However, a naive application of such an approach is non-trivial since LLMs are typically trained on a uniform system message (e.g., \"You are a helpful assistant\") which limits their ability to generalize to diverse, unseen system messages. To improve this generalization, we create the Multifaceted Collection, a preference dataset with 192k combinations of values beyond generic helpfulness and harmlessness, spanning 65k user instructions. Using this dataset, we train a 7B LLM called Janus and test it on 921 prompts from 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct) by adding various unseen system messages that reflect user preferences. Janus achieves tie+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct v0.2, GPT-3.5 Turbo, and GPT-4, respectively. Unexpectedly, on three benchmarks focused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto v0.1), Janus also outperforms LLaMA 3 8B Instruct by a +4.0%, +0.1%, +3.0% margin, underscoring that training with a vast array of system messages could also enhance alignment to the general public's preference as well. Our code, dataset, benchmark, and models are available at this https URL.\n  Structured Summary:\n    - Scalability of individualized LLM alignment: The paper addresses the challenge of scaling individualized alignment of large language models (LLMs) to diverse user preferences without the need for repeated data collection and retraining.\n    - Diverse human preferences in LLM alignment: The authors are motivated by the need to align LLMs with the diverse and nuanced preferences of individuals, which are not adequately captured by current alignment methods focused on general preferences.\n    - System message generalization: The authors propose using system messages to explicitly encode user preferences, allowing LLMs to adapt their behavior to align with individual user intentions without retraining.\n    - MULTIFACETED COLLECTION dataset: The authors develop a dataset called MULTIFACETED COLLECTION, which includes a large number of system messages generated by combining hierarchical user value dimensions to train LLMs for diverse preference alignment.\n\nPaper 31:\n  Title: SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF\n  Abstract: Model alignment with human preferences is an essential step in making Large Language Models (LLMs) helpful and consistent with human values. It typically consists of supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) stages. However, RLHF faces inherent limitations stemming from a complex training setup and its tendency to align the model with implicit values that end users cannot control at run-time. Moreover, reward models in RLHF stage commonly rely on single-dimensional feedback as opposed to explicit, multifaceted signals that indicate attributes such as helpfulness, humor, and toxicity. To address these limitations, we propose SteerLM, a supervised fine-tuning method that empowers end-users to control responses during inference. SteerLM conditions responses to conform to an explicitly defined multi-dimensional set of attributes, thereby empowering a steerable AI capable of generating helpful and high-quality responses while maintaining customizability. Experiments show that SteerLM trained on open source datasets generates responses that are preferred by human and automatic evaluators to many state-of-the-art baselines trained with RLHF while being much easier to train. Try SteerLM at this https URL\n  Structured Summary:\n    - Limitations of RLHF for model alignment: The paper identifies that RLHF has a complex training configuration and aligns models to implicit values that users cannot adjust during inference. Additionally, reward models in RLHF rely on single-dimensional feedback rather than explicit multi-dimensional signals for attributes like helpfulness, humor, and toxicity.\n    - Need for user-steerable AI with explicit multi-dimensional control: The authors are motivated to create a model alignment approach that allows users to explicitly control multiple response attributes at inference time, enabling customizable and steerable AI that generates helpful, high-quality responses while maintaining flexibility for different use cases.\n    - Attribute Prediction Model: A language model is trained to predict multiple human-annotated attributes (quality, toxicity, humor, etc.) for responses, providing multi-dimensional feedback instead of a single reward signal. This model is used to annotate diverse datasets with attribute values.\n    - Attribute Conditioned Supervised Fine-Tuning: The language model is fine-tuned to generate responses conditioned on both the prompt and desired attribute values. This approach uses only supervised learning with an offline annotated dataset, avoiding the complexity of online sampling and evaluation required in RLHF.\n\nPaper 32:\n  Title: Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies\n  Abstract: We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a \"hyper-accuracy distortion\" present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts.\n  Structured Summary:\n    - Limited diagnostic value of Turing's Imitation Game for evaluating AI simulation capabilities: The Turing Imitation Game only requires simulating a single arbitrary individual and provides little insight into which specific aspects of human behavior an AI system can accurately simulate or which populations it can represent.\n    - Need to inform downstream applications requiring accurate human models: Understanding which aspects of human behavior language models can faithfully simulate and which they systematically distort is important for applications in education and the arts that depend on accurate human behavioral models.\n    - Turing Experiment (TE) evaluation framework: A TE evaluates AI systems by simulating human behavior in specific experiments like human subject studies, requiring simulation of representative samples of participants rather than single individuals, using zero-shot procedures without prior experiment-specific data.\n    - Prompt-based methodology for running TEs with language models: The methodology involves creating zero-shot prompts fed into language models, using participant details like names and experimental conditions as inputs, then reconstructing text-based records of simulated experiments from the generated completions, with validation steps to maximize validity rates.\n\nPaper 33:\n  Title: UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems\n  Abstract: Large Language Models (LLMs) has shown exceptional capabilities in many natual language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system. To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation. We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens. Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their behavior to diverse task requirements. Meanwhile, evaluation tokens gauge the relevance score between the dialogue context and the retrieved evidence. In addition, we carefully design a self-refinement mechanism to iteratively refine the generated response considering 1) the consistency scores between the generated response and retrieved evidence; and 2) the relevance scores. Experiments on two personalized datasets (DuLeMon and KBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge source selection and response generation task with itself as a retriever in a unified manner. Extensive analyses and discussions are provided for shedding some new perspectives for personalized dialogue systems.\n  Structured Summary:\n    - Personalization and multi-source knowledge integration in dialogue systems: The paper addresses the challenge of building personalized dialogue systems that can effectively plan and incorporate multiple sources of knowledge (e.g., user persona, system persona, documents) to generate contextually appropriate responses, rather than focusing on single sources or indiscriminately using all sources.\n    - Limitations of existing knowledge-grounded dialogue systems: The authors are motivated by the observation that prior methods either isolate individual knowledge sources or fail to consider inter-dependencies among multiple sources, and they overlook cases where no external knowledge is needed. This leads to suboptimal performance in real-world scenarios requiring dynamic source selection.\n    - Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG): UniMS-RAG unifies three sub-tasks (knowledge source selection, knowledge retrieval, and response generation) into a single sequence-to-sequence framework using large language models. It introduces acting tokens to decide which knowledge source to use and evaluation tokens to assess relevance scores between dialogue context and retrieved evidence.\n    - Self-refinement mechanism for response generation: A self-refinement process is introduced during inference that reassesses generated responses by leveraging feedback from evaluation tokens and ensuring consistency between provided evidence and responses, iteratively updating evidence to improve response quality.\n\nPaper 34:\n  Title: Beyond Goldfish Memory: Long-Term Open-Domain Conversation\n  Abstract: Despite recent improvements in open-domain dialogue models, state of the art models are trained and evaluated on short conversations with little context. In contrast, the long-term conversation setting has hardly been studied. In this work we collect and release a human-human dataset consisting of multiple chat sessions whereby the speaking partners learn about each other's interests and discuss the things they have learnt from past sessions. We show how existing models trained on existing datasets perform poorly in this long-term conversation setting in both automatic and human evaluations, and we study long-context models that can perform much better. In particular, we find retrieval-augmented methods and methods with an ability to summarize and recall previous conversations outperform the standard encoder-decoder architectures currently considered state of the art.\n  Structured Summary:\n    - Short-context limitation in open-domain dialogue models: Current state-of-the-art dialogue models like Meena and BlenderBot are trained on short conversations (2-15 turns) with token truncation lengths of only 128 tokens, making them incapable of incorporating long-term conversational context across multiple sessions.\n    - Need for long-term conversational engagement: Long-term open-domain communication allows conversations to develop and improve over time as the model gains more context and understanding of specific user interests, which is essential for successful deployment and user reengagement.\n    - Multi-Session Chat dataset collection: A new dataset where crowdworkers engage in multi-session conversations spanning hours or days, with each session consisting of up to 14 utterances. Previous sessions are annotated with summaries of important personal points for use in subsequent conversations.\n    - Retrieval-augmented generative models: Models that use retrieval systems (RAG, FiD, FiD-RAG) to select and encode relevant parts of conversation history from memory, allowing the decoder to attend to these encodings when generating responses.\n\nPaper 35:\n  Title: From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment\n  Abstract: Large language models (LLMs) have traditionally been aligned through one-size-fits-all approaches that assume uniform human preferences, fundamentally overlooking the diversity in user values and needs. This paper introduces a comprehensive framework for scalable personalized alignment of LLMs. We establish a systematic preference space characterizing psychological and behavioral dimensions, alongside diverse persona representations for robust preference inference in real-world scenarios. Building upon this foundation, we introduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million personalized preference examples, and develop two complementary alignment approaches: \\textit{in-context alignment} directly conditioning on persona representations and \\textit{preference-bridged alignment} modeling intermediate preference distributions. Extensive experiments demonstrate substantial improvements over existing methods, with an average 17.06\\% accuracy gain across four benchmarks while exhibiting a strong adaptation capability to novel preferences, robustness to limited user data, and precise preference controllability. These results validate our approach toward user-adaptive AI systems.\n  Structured Summary:\n    - Lack of personalized alignment in LLMs: Existing LLM alignment methods assume uniform human preferences and overlook the diversity in user values, cultural backgrounds, and needs, leading to systematic exclusion of minority groups and reduced user satisfaction due to lack of personalization.\n    - Serving diverse user populations at scale: With LLMs serving hundreds of millions of users globally, the authors are motivated by critical issues including systematic exclusion of minority groups and reduced user satisfaction, highlighting the need for personalized alignment approaches that respect individual differences.\n    - Comprehensive preference space and persona framework: The authors construct a 90-dimensional preference space synthesized from psychological models, alignment research, and content platforms, combined with behavioral and descriptive persona representations to enable scalable preference inference while maintaining practical traceability.\n    - ALIGNX dataset construction: The authors develop a systematic pipeline that transforms large-scale forum interactions into structured training data, yielding ALIGNX with over 1.3 million examples capturing persona representations, preference directions, and high-quality preference pairs.\n\nPaper 36:\n  Title: Is ChatGPT a Good Recommender? A Preliminary Study\n  Abstract: Recommendation systems have witnessed significant advancements and have been widely used over the past decades. However, most traditional recommendation methods are task-specific and therefore lack efficient generalization ability. Recently, the emergence of ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. Nonetheless, the application of ChatGPT in the recommendation domain has not been thoroughly investigated. In this paper, we employ ChatGPT as a general-purpose recommendation model to explore its potential for transferring extensive linguistic and world knowledge acquired from large-scale corpora to recommendation scenarios. Specifically, we design a set of prompts and evaluate ChatGPT's performance on five recommendation scenarios. Unlike traditional recommendation methods, we do not fine-tune ChatGPT during the entire evaluation process, relying only on the prompts themselves to convert recommendation tasks into natural language tasks. Further, we explore the use of few-shot prompting to inject interaction information that contains user potential interest to help ChatGPT better understand user needs and interests. Comprehensive experimental results on Amazon Beauty dataset show that ChatGPT has achieved promising results in certain tasks and is capable of reaching the baseline level in others. We conduct human evaluations on two explainability-oriented tasks to more accurately evaluate the quality of contents generated by different models. And the human evaluations show ChatGPT can truly understand the provided information and generate clearer and more reasonable results. We hope that our study can inspire researchers to further explore the potential of language models like ChatGPT to improve recommendation performance and contribute to the advancement of the recommendation systems field.\n  Structured Summary:\n    - Lack of generalization in traditional recommendation systems: Traditional recommendation systems require specific data and models for different tasks or scenarios, limiting their ability to generalize across diverse recommendation contexts without retraining.\n    - Exploring ChatGPT's potential for recommendation via knowledge transfer: The authors aim to investigate whether ChatGPT's vast linguistic and world knowledge from large-scale pretraining can be effectively applied to recommendation tasks without task-specific fine-tuning.\n    - Task-specific prompt design for five recommendation scenarios: The authors construct specialized prompts tailored to convert five distinct recommendation tasks into natural language tasks, enabling ChatGPT to perform recommendations without fine-tuning.\n    - Few-shot prompting with user interaction injection: The authors employ few-shot prompting techniques that incorporate user-item interaction histories into prompts, aiming to help ChatGPT capture user preferences more effectively.\n\nPaper 37:\n  Title: Learning to Rewrite Prompts for Personalized Text Generation\n  Abstract: Facilitated by large language models (LLMs), personalized text generation has become a rapidly growing research direction. Most existing studies focus on designing specialized models for a particular domain, or they require fine-tuning the LLMs to generate personalized text. We consider a typical scenario in which the large language model, which generates personalized output, is frozen and can only be accessed through APIs. Under this constraint, all one can do is to improve the input text (i.e., text prompts) sent to the LLM, a procedure that is usually done manually. In this paper, we propose a novel method to automatically revise prompts for personalized text generation. The proposed method takes the initial prompts generated by a state-of-the-art, multistage framework for personalized generation and rewrites a few critical components that summarize and synthesize the personal context. The prompt rewriter employs a training paradigm that chains together supervised learning (SL) and reinforcement learning (RL), where SL reduces the search space of RL and RL facilitates end-to-end training of the rewriter. Using datasets from three representative domains, we demonstrate that the rewritten prompts outperform both the original prompts and the prompts optimized via supervised learning or reinforcement learning alone. In-depth analysis of the rewritten prompts shows that they are not only human readable, but also able to guide manual revision of prompts when there is limited resource to employ reinforcement learning to train the prompt rewriter, or when it is costly to deploy an automatic prompt rewriter for inference.\n  Structured Summary:\n    - Optimizing prompts for frozen LLMs in personalized text generation: The paper addresses the challenge of improving personalized text generation when the underlying large language model cannot be fine-tuned and is only accessible via APIs, limiting intervention to modifying input prompts rather than model parameters.\n    - Practical constraints of frozen LLMs and API-only access: The authors are motivated by the practical reality that most users cannot fine-tune large language models due to lack of access to model internals or prohibitive computational costs, making prompt optimization the only viable approach for personalization.\n    - Automatic prompt rewriter using chained SL and RL: The authors propose a prompt rewriting method that first uses supervised learning to adapt a sequence-to-sequence model to the rewriting task, then applies reinforcement learning with generation performance as reward to optimize prompts end-to-end, addressing the large search space challenge.\n    - Writing style synthesis as additional personalization signal: The method introduces writing style synthesis as a new component in prompts, using the LLM itself to extract consistent stylistic patterns from a user's historical documents to better guide personalized generation.\n\nPaper 38:\n  Title: Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment\n  Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method approaches the Pareto-optimal solution for multiple objectives. Empirical evidence demonstrates the efficacy of our method in aligning both Large Language Models (LLMs) and diffusion models to accommodate diverse rewards with only around 10% GPU hours compared with multi-objective RL baseline.\n  Structured Summary:\n    - Multi-objective alignment of foundation models with heterogeneous and conflicting human preferences: The paper addresses the challenge of aligning foundation models with multiple, diverse human preferences that are often heterogeneous and conflicting (e.g., harmlessness vs helpfulness). Existing RLHF methods are computationally expensive and unstable, especially when dealing with multiple objectives simultaneously.\n    - Need for scalable and adaptive multi-objective alignment without expensive RL training: The authors are motivated by the inadequacy of single-reward alignment and the prohibitive computational cost of existing multi-objective RLHF approaches. They seek a solution that can accommodate diverse preferences efficiently while maintaining alignment quality across multiple objectives.\n    - Rewards-in-Context (RiC) with multi-reward conditional supervised fine-tuning: RiC restructures multi-objective alignment into three stages: offline training using multi-reward conditional SFT where prompts are augmented with reward values, online training that augments data near the Pareto front using multi-objective rejection sampling, and an inference stage with dynamic preference-to-reward mapping.\n    - Analytical preference-to-reward mapping based on convex optimization: The authors derive a family of preference-to-reward mappings by formulating and solving a constrained convex optimization problem. This provides closed-form solutions that map user preferences to desired reward values in prompts, enabling dynamic inference-time adaptation toward Pareto-optimal outcomes.\n\n\nBefore returning, check that:\n- Every paper title appears exactly once.\n- Only leaf nodes have a \"papers\" field.\n- All intermediate nodes have a \"subtopics\" field.\n- The JSON is strictly valid and parsable.\n\n### Output:", "hierarchy_response": "{\n  \"name\": \"Personalized Alignment for Large Language Models in Real-World Applications\",\n  \"subtopics\": [\n    {\n      \"name\": \"Multi-Objective and Controllable Alignment\",\n      \"subtopics\": [\n        {\n          \"name\": \"Decoding-Time Multi-Objective Alignment\",\n          \"papers\": [\n            \"Decoding-Time Language Model Alignment with Multiple Objectives\",\n            \"PAD: Personalized Alignment of LLMs at Decoding-Time\",\n            \"MetaAligner: Towards Generalizable Multi-Objective Alignment of Language Models\"\n          ]\n        },\n        {\n          \"name\": \"Training-Time Multi-Objective Optimization\",\n          \"papers\": [\n            \"Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment\",\n            \"Conditional Language Policy: A General Framework for Steerable Multi-Objective Finetuning\",\n            \"Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment\"\n          ]\n        },\n        {\n          \"name\": \"Parameter Merging for Multi-Objective Alignment\",\n          \"papers\": [\n            \"Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Personalized Preference Learning and Modeling\",\n      \"subtopics\": [\n        {\n          \"name\": \"Heterogeneous Preference Modeling\",\n          \"papers\": [\n            \"RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation\",\n            \"Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF\",\n            \"Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning\"\n          ]\n        },\n        {\n          \"name\": \"Federated and Privacy-Preserving Personalization\",\n          \"papers\": [\n            \"FedRLHF: A Convergence-Guaranteed Federated Framework for Privacy-Preserving and Personalized RLHF\",\n            \"FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning\",\n            \"FDLoRA: Personalized Federated Learning of Large Language Model via Dual LoRA Tuning\"\n          ]\n        },\n        {\n          \"name\": \"User Preference Elicitation\",\n          \"papers\": [\n            \"Eliciting User Preferences for Personalized Multi-Objective Decision Making through Comparative Feedback\",\n            \"Differentiation of Multi-objective Data-driven Decision Pipeline\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Personalization via User Representation and Context\",\n      \"subtopics\": [\n        {\n          \"name\": \"User Embedding and History Encoding\",\n          \"papers\": [\n            \"User-LLM: Efficient LLM Contextualization with User Embeddings\",\n            \"PERSOMA: PERsonalized SOft ProMpt Adapter Architecture for Personalized Language Prompting\",\n            \"One PEFT Per User (OPPU): Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning\"\n          ]\n        },\n        {\n          \"name\": \"Prompt-Based Personalization\",\n          \"papers\": [\n            \"Learning to Rewrite Prompts for Personalized Text Generation\",\n            \"Selective Prompting Tuning for Personalized Conversations with LLMs\"\n          ]\n        },\n        {\n          \"name\": \"System Message and Attribute Conditioning\",\n          \"papers\": [\n            \"Aligning to Thousands of Preferences via System Message Generalization\",\n            \"SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Collective and Democratic Alignment\",\n      \"subtopics\": [\n        {\n          \"name\": \"Public Input Integration\",\n          \"papers\": [\n            \"Collective Constitutional AI: Aligning a Language Model with Public Input\"\n          ]\n        },\n        {\n          \"name\": \"Large-Scale Personalized Preference Datasets\",\n          \"papers\": [\n            \"From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Fine-Grained and Interpretable Feedback\",\n      \"subtopics\": [\n        {\n          \"name\": \"Fine-Grained Human Feedback\",\n          \"papers\": [\n            \"Fine-Grained Human Feedback Gives Better Rewards for Language Model Training\"\n          ]\n        },\n        {\n          \"name\": \"Process Feedback and Modular Agents\",\n          \"papers\": [\n            \"AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback\"\n          ]\n        },\n        {\n          \"name\": \"Personality-Aware Personalization\",\n          \"papers\": [\n            \"P-React: Synthesizing Topic-Adaptive Reactions of Personality Traits via Mixture of Specialized LoRA Experts\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Application Domains and Task-Specific Personalization\",\n      \"subtopics\": [\n        {\n          \"name\": \"Conversational AI and Dialogue Systems\",\n          \"papers\": [\n            \"Beyond Goldfish Memory: Long-Term Open-Domain Conversation\",\n            \"UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems\"\n          ]\n        },\n        {\n          \"name\": \"Recommendation Systems\",\n          \"papers\": [\n            \"Topic-Level Bayesian Surprise and Serendipity for Recommender Systems\",\n            \"MACRec: a Multi-Agent Collaboration Framework for Recommendation\",\n            \"Is ChatGPT a Good Recommender? A Preliminary Study\",\n            \"Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction\"\n          ]\n        },\n        {\n          \"name\": \"Planning and Task Execution\",\n          \"papers\": [\n            \"JumpStarter: Human-AI Planning with Task-Structured Context Curation\",\n            \"TravelPlanner: A Benchmark for Real-World Planning with Language Agents\"\n          ]\n        },\n        {\n          \"name\": \"Agent Platforms and Ecosystems\",\n          \"papers\": [\n            \"Prospect Personalized Recommendation on Large Language Model-based Agent Platform\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Evaluation and Simulation of Personalization\",\n      \"subtopics\": [\n        {\n          \"name\": \"Human Behavior Simulation\",\n          \"papers\": [\n            \"Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies\"\n          ]\n        }\n      ]\n    }\n  ]\n}\n", "hierarchy_tree": {"name": "Personalized Alignment for Large Language Models in Real-World Applications", "subtopics": [{"name": "Multi-Objective and Controllable Alignment", "subtopics": [{"name": "Decoding-Time Multi-Objective Alignment", "papers": ["Decoding-Time Language Model Alignment with Multiple Objectives", "PAD: Personalized Alignment of LLMs at Decoding-Time", "MetaAligner: Towards Generalizable Multi-Objective Alignment of Language Models"]}, {"name": "Training-Time Multi-Objective Optimization", "papers": ["Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment", "Conditional Language Policy: A General Framework for Steerable Multi-Objective Finetuning", "Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment"]}, {"name": "Parameter Merging for Multi-Objective Alignment", "papers": ["Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging"]}]}, {"name": "Personalized Preference Learning and Modeling", "subtopics": [{"name": "Heterogeneous Preference Modeling", "papers": ["RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation", "Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF", "Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning"]}, {"name": "Federated and Privacy-Preserving Personalization", "papers": ["FedRLHF: A Convergence-Guaranteed Federated Framework for Privacy-Preserving and Personalized RLHF", "FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning", "FDLoRA: Personalized Federated Learning of Large Language Model via Dual LoRA Tuning"]}, {"name": "User Preference Elicitation", "papers": ["Eliciting User Preferences for Personalized Multi-Objective Decision Making through Comparative Feedback", "Differentiation of Multi-objective Data-driven Decision Pipeline"]}]}, {"name": "Personalization via User Representation and Context", "subtopics": [{"name": "User Embedding and History Encoding", "papers": ["User-LLM: Efficient LLM Contextualization with User Embeddings", "PERSOMA: PERsonalized SOft ProMpt Adapter Architecture for Personalized Language Prompting", "One PEFT Per User (OPPU): Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning"]}, {"name": "Prompt-Based Personalization", "papers": ["Learning to Rewrite Prompts for Personalized Text Generation", "Selective Prompting Tuning for Personalized Conversations with LLMs"]}, {"name": "System Message and Attribute Conditioning", "papers": ["Aligning to Thousands of Preferences via System Message Generalization", "SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF"]}]}, {"name": "Collective and Democratic Alignment", "subtopics": [{"name": "Public Input Integration", "papers": ["Collective Constitutional AI: Aligning a Language Model with Public Input"]}, {"name": "Large-Scale Personalized Preference Datasets", "papers": ["From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment"]}]}, {"name": "Fine-Grained and Interpretable Feedback", "subtopics": [{"name": "Fine-Grained Human Feedback", "papers": ["Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"]}, {"name": "Process Feedback and Modular Agents", "papers": ["AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback"]}, {"name": "Personality-Aware Personalization", "papers": ["P-React: Synthesizing Topic-Adaptive Reactions of Personality Traits via Mixture of Specialized LoRA Experts"]}]}, {"name": "Application Domains and Task-Specific Personalization", "subtopics": [{"name": "Conversational AI and Dialogue Systems", "papers": ["Beyond Goldfish Memory: Long-Term Open-Domain Conversation", "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems"]}, {"name": "Recommendation Systems", "papers": ["Topic-Level Bayesian Surprise and Serendipity for Recommender Systems", "MACRec: a Multi-Agent Collaboration Framework for Recommendation", "Is ChatGPT a Good Recommender? A Preliminary Study", "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction"]}, {"name": "Planning and Task Execution", "papers": ["JumpStarter: Human-AI Planning with Task-Structured Context Curation", "TravelPlanner: A Benchmark for Real-World Planning with Language Agents"]}, {"name": "Agent Platforms and Ecosystems", "papers": ["Prospect Personalized Recommendation on Large Language Model-based Agent Platform"]}]}, {"name": "Evaluation and Simulation of Personalization", "subtopics": [{"name": "Human Behavior Simulation", "papers": ["Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies"]}]}]}}
{"id": 2, "survey": "Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives", "pdfs": [{"title": "Artificial Intelligence: the global landscape of ethics guidelines", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Macro Level/Moral Foundation/Beneficence/Artificial Intelligence: the global landscape of ethics guidelines.pdf", "summary": {"paper_id": "survey-2_paper-0_Artificial Intelligence: the global landscape of ethics guidelines", "core_task": {"text": "ethical principles and guidelines for artificial intelligence", "query_variants": ["ethical principles and guidelines for artificial intelligence", "moral standards and frameworks for AI systems", "normative rules and policies governing artificial intelligence"]}, "contributions": [{"name": "Lack of global consensus on ethical AI principles and implementation", "description": "The paper addresses the challenge that despite widespread efforts to develop ethical AI guidelines, there is ongoing disagreement about the definition of ethical AI and the specific requirements, standards, and practices necessary to achieve it.", "author_claim_text": "there is debate about both what constitutes 'ethical AI' and which ethical requirements, technical standards and best practices are needed for its realization", "source_hint": "Abstract"}, {"name": "Need to determine if global convergence on ethical AI is emerging", "description": "The authors are motivated to understand whether the diverse stakeholders issuing AI ethics guidelines are converging toward shared principles and implementation strategies, which is critical for developing a coherent global agenda for ethical AI.", "author_claim_text": "To investigate whether a global agreement on these questions is emerging, we mapped and analyzed the current corpus of principles and guidelines on ethical AI", "source_hint": "Abstract"}, {"name": "Scoping review of gray literature on ethical AI guidelines", "description": "The authors performed a systematic scoping review using a multi-stage screening strategy involving link hubs, keyword-based web searches, and citation chaining to identify 84 policy documents containing ethical AI principles and guidelines.", "author_claim_text": "We conducted a scoping review of the gray literature reporting principles and guidelines for ethical AI", "source_hint": "Methods section"}, {"name": "Inductive content analysis and thematic coding", "description": "The authors applied qualitative content analysis methods, including inductive coding and code mapping, to systematically categorize and interpret ethical principles and recommendations across the corpus of documents.", "author_claim_text": "Content analysis of the 84 sources was independently conducted by two researchers in two cycles of manual coding and one cycle of code mapping", "source_hint": "Methods section"}]}, "abs": "In the last five years, private companies, research institutions as well as public sector organisations have issued principles and guidelines for ethical AI, yet there is debate about both what constitutes \"ethical AI\" and which ethical requirements, technical standards and best practices are needed for its realization. To investigate whether a global agreement on these questions is emerging, we mapped and analyzed the current corpus of principles and guidelines on ethical AI. Our results reveal a global convergence emerging around five ethical principles (transparency, justice and fairness, non-maleficence, responsibility and privacy), with substantive divergence in relation to how these principles are interpreted; why they are deemed important; what issue, domain or actors they pertain to; and how they should be implemented. Our findings highlight the importance of integrating guideline-development efforts with substantive ethical analysis and adequate implementation strategies."}, {"title": "The five Is: Key principles for interpretable and safe conversational AI", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Macro Level/Moral Foundation/Beneficence/The five Is: Key principles for interpretable and safe conversational AI.pdf", "summary": {"paper_id": "survey-2_paper-1_The five Is: Key principles for interpretable and safe conversational AI", "core_task": {"text": "Developing transparent and accountable conversational AI systems", "query_variants": ["Developing transparent and accountable conversational AI systems", "Building interpretable and responsible dialogue agents", "Creating explainable and trustworthy conversational models"]}, "contributions": [{"name": "Black box conversational AI systems lack transparency and safety", "description": "Current conversational AI systems based on deep neural networks are opaque and prone to spectacular failures, making them unsafe for high-stakes applications. These black box systems require massive training data that may contain unintended biases and sensitive content that is nearly impossible to quality-check.", "author_claim_text": "there are considerable drawbacks with using black box DNN-based systems, especially in high-stakes applications (such as, for example, healthcare). Due to their distributed nature, black box systems must generally be trained with machine learning methods (deep learning) that require massive amounts of data.", "source_hint": "Introduction, paragraph 3"}, {"name": "Need for transparent and accountable conversational AI", "description": "The authors are motivated by the lack of alternative approaches to black box systems for building conversational AI that is transparent and accountable by design, especially given rising legal trends favoring transparency and end-user preferences for accountable systems.", "author_claim_text": "To the best of our knowledge, no proposals exist yet on what could be radically different ways to move forward to create conversational AI that is transparent and accountable by construction.", "source_hint": "Introduction, paragraph 5"}, {"name": "Five design principles for interpretable conversational AI", "description": "The authors propose five key principles (the 'five Is'): interpretability using human-understandable primitives, independent separation of procedural and declarative memory, inherent capability to explain reasoning, interactive learning with human tutors, and inquisitiveness to actively seek information while checking with users.", "author_claim_text": "we propose five key design principles that next-generation conversational AI should adhere to, in our view, to ensure transparency and accountability. These are interpretability, independent data, inherent capability to explain, interactive learning, and inquisitiveness", "source_hint": "Introduction, paragraph 5"}, {"name": "Pipeline model with interpretable cognitive processing", "description": "The authors propose a pipeline architecture where input processing, cognitive processing, and output processing are clearly separated. The cognitive processing step uses generic, interpretable primitives (like sorting, fetching, comparing) that can be combined to perform complex operations while remaining human-readable.", "author_claim_text": "We also suggest following the so called pipeline model whereby input processing, cognitive processing (decision-making), and output processing are clearly separated steps.", "source_hint": "Section III-A"}]}, "abs": "In this position paper, we present five key principles, namely interpretability, inherent capability to explain, independent data, interactive learning, and inquisitiveness, for the development of conversational AI that, unlike the currently popular black box approaches, is transparent and accountable. At present, there is a growing concern with the use of black box statistical language models: While displaying impressive average performance, such systems are also prone to occasional spectacular failures, for which there is no clear remedy. In an effort to initiate a discussion on possible alternatives, we outline and exemplify how our five principles enable the development of conversational AI systems that are transparent and thus safer for use. We also present some of the challenges inherent in the implementation of those principles."}, {"title": "Artificial Intelligence Governance and Ethics: Global Perspectives", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Macro Level/Moral Foundation/Dignity/Artificial Intelligence Governance and Ethics: Global Perspectives.pdf", "summary": {"paper_id": "survey-2_paper-2_Artificial Intelligence Governance and Ethics: Global Perspectives", "core_task": {"text": "Global governance and ethics of artificial intelligence", "query_variants": ["Global governance and ethics of artificial intelligence", "International regulation and moral principles for AI systems", "Worldwide policy frameworks and ethical considerations in artificial intelligence"]}, "contributions": [{"name": "Fragmented global approaches to AI governance and ethics", "description": "The paper addresses the challenge of understanding and comparing how different major jurisdictions and international bodies are approaching AI governance and ethics, given the fragmented and rapidly evolving landscape of AI regulation and ethical frameworks worldwide.", "author_claim_text": "This Report provides an overview of the current state of play in AI governance and ethics across different jurisdictions and at the global level", "source_hint": "Introduction"}, {"name": "Need for comparative understanding of AI governance approaches", "description": "The authors are motivated by the need to map and understand the diverse approaches to AI governance and ethics emerging globally, recognizing that AI's impact crosses borders and requires awareness of different regulatory and ethical frameworks being developed simultaneously.", "author_claim_text": "This Report provides an overview of the current state of play in AI governance and ethics across different jurisdictions and at the global level", "source_hint": "Introduction"}, {"name": "Comparative jurisdictional analysis of AI governance frameworks", "description": "The authors conduct a systematic comparative analysis examining AI governance and ethics initiatives across multiple levels: global organizations, regional bodies (EU, Council of Europe), and individual countries (Germany, Austria, UK, India, China, USA, Australia), documenting their respective approaches, principles, and frameworks.", "author_claim_text": "This Report provides an overview of the current state of play in AI governance and ethics across different jurisdictions and at the global level", "source_hint": "Introduction and Table of Contents"}, {"name": "Significant variations exist in AI governance approaches globally", "description": "The report identifies both commonalities and significant differences in how various jurisdictions approach AI governance and ethics, revealing a complex landscape where competition and collaboration coexist, and where questions of implementation, enforceability, and stakeholder involvement remain critical challenges.", "author_claim_text": "Similarities and differences", "source_hint": "Section 7: Reflections, issues and next steps"}]}, "abs": "Artificial intelligence (AI) is a technology which is increasingly being utilised in society and the economy worldwide, and its implementation is planned to become more prevalent in coming years. AI is increasingly being embedded in our lives, supplementing our pervasive use of digital technologies. But this is being accompanied by disquiet over problematic and dangerous implementations of AI, or indeed, even AI itself deciding to do dangerous and problematic actions, especially in fields such as the military, medicine and criminal justice. These developments have led to concerns about whether and how AI systems adhere, and will adhere to ethical standards. These concerns have stimulated a global conversation on AI ethics, and have resulted in various actors from different countries and sectors issuing ethics and governance initiatives and guidelines for AI. Such developments form the basis for our research in this report, combining our international and interdisciplinary expertise to give an insight into what is happening in Australia, China, Europe, India and the US."}, {"title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Macro Level/Moral Foundation/Honesty/Improving Factuality and Reasoning in Language Models through Multiagent Debate.pdf", "summary": {"paper_id": "survey-2_paper-3_Improving Factuality and Reasoning in Language Models through Multiagent Debate", "core_task": {"text": "improving factuality and reasoning in language models through multiagent debate", "query_variants": ["improving factuality and reasoning in language models through multiagent debate", "enhancing accuracy and logical inference in LLMs via multi-agent discussion", "boosting truthfulness and deductive capabilities in language models using collaborative agent debate"]}, "contributions": [{"name": "Hallucinations and reasoning errors in large language models", "description": "The paper addresses the problem that contemporary large language models frequently generate factually incorrect information and make flawed reasoning steps, despite appearing confident in their outputs.", "author_claim_text": "current models may suffer from confidently hallucinating facts or making implausible jumps in chains of reasoning", "source_hint": "Introduction paragraph 1"}, {"name": "Improving factual accuracy and reasoning beyond single-model approaches", "description": "The authors are motivated to develop a complementary method to existing single-model prompting techniques by leveraging multi-agent interactions, inspired by how multiple perspectives and cross-examination can improve reasoning and factuality.", "author_claim_text": "An extensive body of recent work has focused on improving factual accuracy and reasoning in language models... we propose a complementary approach inspired by The Society of Mind and multi-agent settings", "source_hint": "Introduction paragraph 1"}, {"name": "Multiagent debate framework for language generation", "description": "The core method involves multiple instances of a language model independently generating candidate answers, then iteratively critiquing and refining each other's responses through a debate process over several rounds until converging on a consensus answer.", "author_claim_text": "we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer", "source_hint": "Abstract"}, {"name": "Consensus prompting mechanism", "description": "A prompting technique where agents receive concatenated responses from other agents and are instructed to revise their own answers based on this collective information, enabling iterative refinement through debate.", "author_claim_text": "we feed each agent a consensus prompt... where each agent is instructed to update their responses based on the responses of other agents", "source_hint": "Section 2.1"}]}, "abs": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such \"society of minds\" approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding."}, {"title": "From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Macro Level/Moral Foundation/Honesty/From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models.pdf", "summary": {"paper_id": "survey-2_paper-4_From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models", "core_task": {"text": "aligning large language models with human values and preferences", "query_variants": ["aligning large language models with human values and preferences", "human preference alignment for large language models", "value alignment of large language models with human feedback"]}, "contributions": [{"name": "Lack of clarity on appropriate alignment goals for big models", "description": "The paper identifies that while many efforts have been made to align large language models with humans, there is insufficient discussion about what the most appropriate and essential alignment goal should be, and choosing wrong goals could have negative consequences.", "author_claim_text": "the basic question 'what to align with' has not been fully discussed, and inappropriate alignment goals might even backfire", "source_hint": "Abstract"}, {"name": "Growing risks and social harm from big models", "description": "The authors are motivated by the increasing integration of big models into daily life and the associated risks, including emergent problems and inverse scaling laws where some risks become more serious with larger model sizes.", "author_claim_text": "the growing intertwining of big models with everyday human lives poses potential risks and might cause serious social harm", "source_hint": "Abstract and Introduction §1"}, {"name": "Comprehensive survey of alignment goals across three levels", "description": "The authors systematically review and categorize existing alignment work into three distinct levels: human instructions, human preferences, and human values, analyzing each from the perspectives of goal definition and evaluation methods.", "author_claim_text": "we conduct a comprehensive survey of different alignment goals in existing work and trace their evolution paths to help identify the most essential goal", "source_hint": "Abstract"}, {"name": "Taxonomy of alignment goals and evaluation methods", "description": "The paper organizes existing research into a structured taxonomy covering how alignment goals are defined and represented for training, and how alignment performance is evaluated through benchmarks and methods.", "author_claim_text": "we summarize related works in the three levels of alignment goals from two essential perspectives. (1) Definition of alignment goals... (2) Evaluation of alignment", "source_hint": "Introduction §1"}]}, "abs": "Big models, exemplified by Large Language Models (LLMs), are models typically pre-trained on massive data and comprised of enormous parameters, which not only obtain significantly improved performance across diverse tasks but also present emergent capabilities absent in smaller models. However, the growing intertwining of big models with everyday human lives poses potential risks and might cause serious social harm. Therefore, many efforts have been made to align LLMs with humans to make them better follow user instructions and satisfy human preferences. Nevertheless, `what to align with' has not been fully discussed, and inappropriate alignment goals might even backfire. In this paper, we conduct a comprehensive survey of different alignment goals in existing work and trace their evolution paths to help identify the most essential goal. Particularly, we investigate related works from two perspectives: the definition of alignment goals and alignment evaluation. Our analysis encompasses three distinct levels of alignment goals and reveals a goal transformation from fundamental abilities to value orientation, indicating the potential of intrinsic human values as the alignment goal for enhanced LLMs. Based on such results, we further discuss the challenges of achieving such intrinsic value alignment and provide a collection of available resources for future research on the alignment of big models."}, {"title": "Achieving Fairness in Multi-Agent Markov Decision Processes Using Reinforcement Learning", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Macro Level/Moral Foundation/Justice & Fairness/Achieving Fairness in Multi-Agent Markov Decision Processes Using Reinforcement Learning.pdf", "summary": {"paper_id": "survey-2_paper-5_Achieving Fairness in Multi-Agent Markov Decision Processes Using Reinforcement Learning", "core_task": {"text": "Achieving fairness in multi-agent reinforcement learning using value function optimization", "query_variants": ["Achieving fairness in multi-agent reinforcement learning using value function optimization", "Fairness through value function optimization in multi-agent RL", "Value-based optimization for fairness in multi-agent reinforcement learning systems"]}, "contributions": [{"name": "Achieving fairness in multi-agent MDPs with unknown environments", "description": "The paper addresses the challenge of achieving fairness among multiple agents in Markov Decision Processes when the environment parameters (transition probabilities and rewards) are unknown, requiring a reinforcement learning approach rather than traditional planning methods.", "author_claim_text": "While existing research has focused on studying fairness in known environments, the exploration of fairness in such systems for unknown environments remains open.", "source_hint": "Abstract"}, {"name": "Ensuring equitable treatment across agents in sequential decision-making", "description": "The authors are motivated by real-world scenarios where maximizing aggregate rewards may lead to unfair outcomes for individual agents, such as in wireless networks or job scheduling, necessitating fairness-aware policies that balance returns across all agents.", "author_claim_text": "In this paper, our objective is to identify fair decision-making strategies for multi-agent MDP problems, ensuring that all agents are treated equitably.", "source_hint": "Introduction §1"}, {"name": "Optimism-based convex optimization using occupancy measures", "description": "The authors propose an algorithm that constructs confidence intervals for unknown MDP parameters and formulates a convex optimization problem over state-action occupancy measures with optimistic reward and transition estimates to encourage exploration while ensuring fairness.", "author_claim_text": "We maintain a confidence bound of the unknown environment and then propose an online convex optimization based approach to obtain a policy constrained to this confidence region.", "source_hint": "Abstract and Section 5.2"}, {"name": "Pessimistic offline RL algorithm for fairness", "description": "For offline settings where new data cannot be collected, the authors develop a pessimistic approach that constructs an MDP with lower-bound rewards based on available data and solves a convex optimization to obtain a fair policy with theoretical guarantees on suboptimality.", "author_claim_text": "We also propose an offline RL algorithm and bound the optimality gap with respect to the optimal fair solution.", "source_hint": "Abstract and Section 6"}]}, "abs": "Fairness plays a crucial role in various multi-agent systems (e.g., communication networks, financial markets, etc.). Many multi-agent dynamical interactions can be cast as Markov Decision Processes (MDPs). While existing research has focused on studying fairness in known environments, the exploration of fairness in such systems for unknown environments remains open. In this paper, we propose a Reinforcement Learning (RL) approach to achieve fairness in multi-agent finite-horizon episodic MDPs. Instead of maximizing the sum of individual agents' value functions, we introduce a fairness function that ensures equitable rewards across agents. Since the classical Bellman's equation does not hold when the sum of individual value functions is not maximized, we cannot use traditional approaches. Instead, in order to explore, we maintain a confidence bound of the unknown environment and then propose an online convex optimization based approach to obtain a policy constrained to this confidence region. We show that such an approach achieves sub-linear regret in terms of the number of episodes. Additionally, we provide a probably approximately correct (PAC) guarantee based on the obtained regret bound. We also propose an offline RL algorithm and bound the optimality gap with respect to the optimal fair solution. To mitigate computational complexity, we introduce a policy-gradient type method for the fair objective. Simulation experiments also demonstrate the efficacy of our approach."}, {"title": "Cultural Bias and Cultural Alignment of Large Language Models", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Macro Level/Moral Foundation/Justice & Fairness/Cultural Bias and Cultural Alignment of Large Language Models.pdf", "summary": {"paper_id": "survey-2_paper-6_Cultural Bias and Cultural Alignment of Large Language Models", "core_task": {"text": "cultural bias and cultural alignment of large language models", "query_variants": ["cultural bias and cultural alignment of large language models", "cultural fairness and cultural adaptation in LLMs", "cross-cultural bias and alignment in large language models"]}, "contributions": [{"name": "Cultural bias in large language models", "description": "Large language models exhibit cultural bias by overrepresenting certain cultural values, particularly Western values, due to training on corpora that overrepresent certain parts of the world. This bias can influence how people communicate and express themselves when using these models.", "author_claim_text": "LLMs trained on predominantly English text exhibit a latent bias favoring Western cultural values, especially when prompted in English", "source_hint": "Introduction paragraph 2"}, {"name": "Impact of AI on cultural reproduction and authentic expression", "description": "The widespread adoption of LLMs in daily communication and professional tasks raises concerns that embedded cultural values may bias users' authentic expression and contribute to cultural dominance, affecting how language is produced and transmitted across generations.", "author_claim_text": "As people increasingly use generative artificial intelligence (AI) to expedite and automate personal and professional tasks, cultural values embedded in AI models may bias people's authentic expression and contribute to the dominance of certain cultures", "source_hint": "Abstract"}, {"name": "Disaggregated evaluation using World Values Survey benchmark", "description": "The authors systematically evaluate cultural bias across 107 countries and territories by prompting five GPT models with the same ten questions from the Integrated Values Surveys and mapping responses onto the Inglehart-Welzel Cultural Map to quantify cultural distance from ground truth survey data.", "author_claim_text": "We conduct a disaggregated evaluation of cultural bias for five widely used large language models (OpenAI's GPT-4o/4-turbo/4/3.5-turbo/3) by comparing the models' responses to nationally representative survey data", "source_hint": "Abstract and Introduction paragraph 3"}, {"name": "Cultural prompting as control strategy", "description": "The authors propose and test cultural prompting, which instructs the LLM to respond like a person from a specific country or territory, as a flexible and accessible approach to improve cultural alignment. They evaluate this across ten prompt variants to ensure robustness.", "author_claim_text": "We test cultural prompting as a control strategy to increase cultural alignment for each country/territory", "source_hint": "Abstract and Introduction paragraph 3"}]}, "abs": "Culture fundamentally shapes people's reasoning, behavior, and communication. As people increasingly use generative artificial intelligence (AI) to expedite and automate personal and professional tasks, cultural values embedded in AI models may bias people's authentic expression and contribute to the dominance of certain cultures. We conduct a disaggregated evaluation of cultural bias for five widely used large language models (OpenAI's GPT-4o/4-turbo/4/3.5-turbo/3) by comparing the models' responses to nationally representative survey data. All models exhibit cultural values resembling English-speaking and Protestant European countries. We test cultural prompting as a control strategy to increase cultural alignment for each country/territory. For recent models (GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models' output for 71-81% of countries and territories. We suggest using cultural prompting and ongoing evaluation to reduce cultural bias in the output of generative AI."}, {"title": "Ethics of AI: A Systematic Literature Review of Principles and Challenges", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Macro Level/Moral Foundation/Justice & Fairness/Ethics of AI: A Systematic Literature Review of Principles and Challenges.pdf", "summary": {"paper_id": "survey-2_paper-7_Ethics of AI: A Systematic Literature Review of Principles and Challenges", "core_task": {"text": "Identifying ethical principles and challenges in artificial intelligence systems", "query_variants": ["Identifying ethical principles and challenges in artificial intelligence systems", "Recognizing moral guidelines and issues in AI systems", "Determining ethical frameworks and problems in artificial intelligence applications"]}, "contributions": [{"name": "Lack of clarity on AI ethics principles and adoption challenges", "description": "Despite various organizations developing AI ethics guidelines and principles, there remains significant debate and uncertainty about how these principles should be interpreted and implemented in practice, creating a gap between ethical guidelines and their real-world adoption.", "author_claim_text": "there is still debate about the implications of these principles", "source_hint": "Abstract"}, {"name": "Need for systematic understanding of AI ethics for maturity model development", "description": "The authors aim to systematically identify agreed-upon AI ethics principles and adoption challenges to provide foundational inputs for developing a maturity model that can evaluate ethical capabilities of AI systems and guide improvements in practice.", "author_claim_text": "The findings of this study are the preliminary inputs for proposing a maturity model that assess the ethical capabilities of AI systems and provide best practices for further improvements.", "source_hint": "Abstract"}, {"name": "Systematic literature review following Kitchenham guidelines", "description": "The authors conducted a systematic literature review following Kitchenham and Charters guidelines, searching five major digital libraries and applying inclusion/exclusion criteria to select 27 primary studies, which were then analyzed to extract AI ethics principles and challenges.", "author_claim_text": "We conducted a systematic literature review (SLR) study to investigate the agreement on the significance of AI principles and identify the challenging factors", "source_hint": "Abstract and Section 3"}, {"name": "Four core principles dominate AI ethics discourse", "description": "The review reveals that among 22 identified ethical principles, transparency, privacy, accountability, and fairness emerge as the most frequently cited and significant principles that should guide AI system design and operation.", "author_claim_text": "Transparency, privacy, accountability and fairness are identified as the most common AI ethics principles", "source_hint": "Abstract and Section 5.1"}]}, "abs": "Ethics in AI becomes a global topic of interest for both policymakers and academic researchers. In the last few years, various research organizations, lawyers, think tankers and regulatory bodies get involved in developing AI ethics guidelines and principles. However, there is still debate about the implications of these principles. We conducted a systematic literature review (SLR) study to investigate the agreement on the significance of AI principles and identify the challenging factors that could negatively impact the adoption of AI ethics principles. The results reveal that the global convergence set consists of 22 ethical principles and 15 challenges. Transparency, privacy, accountability and fairness are identified as the most common AI ethics principles. Similarly, lack of ethical knowledge and vague principles are reported as the significant challenges for considering ethics in AI. The findings of this study are the preliminary inputs for proposing a maturity model that assess the ethical capabilities of AI systems and provide best practices for further improvements."}, {"title": "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Macro Level/Moral Foundation/Justice & Fairness/BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset.pdf", "summary": {"paper_id": "survey-2_paper-8_BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset", "core_task": {"text": "safety alignment of large language models via human preference dataset", "query_variants": ["safety alignment of large language models via human preference dataset", "aligning large language models for safety using human preference data", "human preference based safety alignment for LLMs"]}, "contributions": [{"name": "Lack of comprehensive human feedback data for safety alignment in LLMs", "description": "The paper addresses the challenge that existing safety alignment techniques for large language models require extensive human feedback data, which is expensive and difficult to obtain at scale. Current datasets do not adequately separate helpfulness from harmlessness annotations.", "author_claim_text": "their effectiveness hinges on the availability of comprehensive human feedback, which necessitates costly large-scale data labeling operations", "source_hint": "Introduction paragraph 2"}, {"name": "Ensuring safe deployment of LLMs to prevent societal harm", "description": "The authors are motivated by the urgent need to align LLMs with human values and safety standards to prevent harmful outputs such as misinformation, offensive content, and unintended dangerous responses that could negatively impact society.", "author_claim_text": "If left unchecked, LLMs can amplify misinformation, enable harmful content, or yield unintended responses that can cause significant negative societal impact", "source_hint": "Introduction paragraph 1"}, {"name": "Two-stage annotation process for QA pairs", "description": "The authors introduce a two-stage annotation methodology where QA pairs are first classified across 14 harm categories to assign safety meta-labels, then responses are separately ranked for helpfulness and harmlessness. This process assesses risk neutralization rather than individual utterance toxicity.", "author_claim_text": "The task of annotating a QA pair in the BEAVERTAILS dataset involves a two-stage annotation process", "source_hint": "Section 3.2"}, {"name": "Decoupled human preference annotations for helpfulness and harmlessness", "description": "The dataset uniquely separates human preference annotations into two independent dimensions: helpfulness (how well a response addresses a prompt) and harmlessness (risk neutrality across harm categories). This allows for independent optimization of both attributes during model training.", "author_claim_text": "BEAVERTAILS stands as the first dataset to disentangle harmlessness and helpfulness from the human-preference score, thus providing separate ranking data for the two metrics", "source_hint": "Introduction paragraph 3"}]}, "abs": "In this paper, we introduce the BeaverTails dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have gathered safety meta-labels for 333,963 question-answer (QA) pairs and 361,903 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our project page is available at the following URL: this https URL."}, {"title": "Value Compass Benchmarks: A Platform for Fundamental and Validated Evaluation of LLMs Values", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Macro Level/Moral Foundation/Virtue/Value Compass Benchmarks: A Platform for Fundamental and Validated Evaluation of LLMs Values.pdf", "summary": {"paper_id": "survey-2_paper-9_Value Compass Benchmarks: A Platform for Fundamental and Validated Evaluation of LLMs Values", "core_task": {"text": "value evaluation and alignment assessment of large language models", "query_variants": ["value evaluation and alignment assessment of large language models", "assessing value alignment and evaluation in large language models", "alignment and value assessment for large language models"]}, "contributions": [{"name": "Evaluation validity and result interpretation challenges in LLM value assessment", "description": "Existing value evaluation benchmarks suffer from two main issues: they fail to accurately measure LLMs' true value conformity (measuring knowledge rather than behavior, producing indistinguishable results), and they oversimplify pluralistic values into single scores that hinder meaningful comparison and interpretation.", "author_claim_text": "current value evaluation methods face two key challenges. C1. Evaluation Validity: Static benchmarks fail to reflect intended values or yield informative results due to data contamination or ceiling effect. C2. Result Interpretation: They typically reduce the pluralistic and often incommensurable values to one-dimensional scores", "source_hint": "Abstract"}, {"name": "Need for comprehensive value assessment as LLMs integrate into society", "description": "The authors are motivated by the growing integration of LLMs into human society and the need to assess their values to understand potential risks, harmful content generation, and alignment with diverse cultural preferences beyond predefined risk categories.", "author_claim_text": "As large language models (LLMs) are gradually integrated into human daily life, assessing their underlying values becomes essential for understanding their risks and alignment with specific customized preferences", "source_hint": "Abstract and Introduction"}, {"name": "Generative self-evolving evaluation paradigm", "description": "The platform employs a novel evaluation approach that automatically generates value-evoking test items through an adaptive generator optimized to maximize informativeness and value elicitation, and dynamically updates items as LLMs evolve, assessing value conformity through generative behavior rather than discriminative knowledge.", "author_claim_text": "develops a generative evolving evaluation paradigm that automatically creates real-world test items that co-evolve with ever-advancing LLMs", "source_hint": "Abstract and Section 2.2"}, {"name": "Multiple basic value systems framework", "description": "The benchmarks incorporate four distinct value systems covering 27 dimensions: two established theories from social science (Schwartz Basic Values and Moral Foundation Theory) and two designed for LLMs (LLMs' Unique Value System and Safety Taxonomy), providing a holistic view of value orientations.", "author_claim_text": "grounds evaluations in multiple basic value systems from social science", "source_hint": "Abstract and Section 2.1"}]}, "abs": "As Large Language Models (LLMs) achieve remarkable breakthroughs, aligning their values with humans has become imperative for their responsible development and customized applications. However, there still lack evaluations of LLMs values that fulfill three desirable goals. (1) Value Clarification: We expect to clarify the underlying values of LLMs precisely and comprehensively, while current evaluations focus narrowly on safety risks such as bias and toxicity. (2) Evaluation Validity: Existing static, open-source benchmarks are prone to data contamination and quickly become obsolete as LLMs evolve. Additionally, these discriminative evaluations uncover LLMs' knowledge about values, rather than valid assessments of LLMs' behavioral conformity to values. (3) Value Pluralism: The pluralistic nature of human values across individuals and cultures is largely ignored in measuring LLMs value alignment. To address these challenges, we presents the Value Compass Benchmarks, with three correspondingly designed modules. It (i) grounds the evaluation on motivationally distinct \\textit{basic values to clarify LLMs' underlying values from a holistic view; (ii) applies a \\textit{generative evolving evaluation framework with adaptive test items for evolving LLMs and direct value recognition from behaviors in realistic scenarios; (iii) propose a metric that quantifies LLMs alignment with a specific value as a weighted sum over multiple dimensions, with weights determined by pluralistic values."}, {"title": "DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Macro Level/Moral Foundation/Virtue/DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life.pdf", "summary": {"paper_id": "survey-2_paper-10_DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life", "core_task": {"text": "evaluating value preferences of language models in moral dilemmas", "query_variants": ["evaluating value preferences of language models in moral dilemmas", "assessing ethical value alignment of LLMs in moral decision-making scenarios", "measuring moral preference judgments of language models in ethical dilemmas"]}, "contributions": [{"name": "Value conflicts in LLM decision-making for daily life dilemmas", "description": "The paper addresses the challenge that LLMs face when providing guidance on everyday decisions that involve moral dilemmas without clear-cut answers, where outcomes depend on personal values and ethical standards rather than universally accepted norms.", "author_claim_text": "As users increasingly seek guidance from LLMs for decision-making in daily life, many of these decisions are not clear-cut and depend significantly on the personal values and ethical standards of people.", "source_hint": "Abstract, first sentence"}, {"name": "Understanding LLM value prioritization in realistic moral scenarios", "description": "The authors are motivated by the need to understand how AI systems prioritize competing values when facing complex real-world dilemmas, moving beyond simple clear-cut scenarios to ensure better alignment with diverse human preferences in practical deployment contexts.", "author_claim_text": "We propose to explore everyday moral dilemmas to examine how AI systems prioritize values in conflicts, ensuring alignment with human preferences.", "source_hint": "Introduction, paragraph 2"}, {"name": "DAILY DILEMMAS dataset of 1,360 moral dilemmas", "description": "The authors introduce a synthetically generated dataset using GPT-4 containing 1,360 moral dilemmas from daily life. Each dilemma includes a background, conflict point, binary action choices, negative consequences for each action, affected parties, and associated human values mapped to five theoretical frameworks.", "author_claim_text": "We present DAILY DILEMMAS, a dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma presents two possible actions, along with affected parties and relevant human values for each action.", "source_hint": "Abstract, second and third sentences"}, {"name": "Mapping values to five theoretical frameworks", "description": "The authors map the 301 human values extracted from dilemmas onto five established theoretical frameworks: World Values Survey, Moral Foundations Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and Plutchik's Wheel of Emotions, enabling systematic analysis of LLM value preferences.", "author_claim_text": "With DAILY DILEMMAS, we evaluate LLMs on these dilemmas to determine what action they will choose and the values represented by these action choices. Then, we analyze values through the lens of five theoretical frameworks inspired by sociology, psychology, and philosophy.", "source_hint": "Abstract, fourth sentence"}]}, "abs": "As users increasingly seek guidance from LLMs for decision-making in daily life, many of these decisions are not clear-cut and depend significantly on the personal values and ethical standards of people. We present DailyDilemmas, a dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma presents two possible actions, along with affected parties and relevant human values for each action. Based on these dilemmas, we gather a repository of human values covering diverse everyday topics, such as interpersonal relationships, workplace, and environmental issues. With DailyDilemmas, we evaluate LLMs on these dilemmas to determine what action they will choose and the values represented by these action choices. Then, we analyze values through the lens of five theoretical frameworks inspired by sociology, psychology, and philosophy, including the World Values Survey, Moral Foundations Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and Plutchik's Wheel of Emotions. For instance, we find LLMs are most aligned with self-expression over survival in World Values Survey and care over loyalty in Moral Foundations Theory. Interestingly, we find substantial preference differences in models for some core values. For example, for truthfulness, Mixtral-8x7B neglects it by 9.7% while GPT-4-turbo selects it by 9.4%. We also study the recent guidance released by OpenAI (ModelSpec), and Anthropic (Constitutional AI) to understand how their designated principles reflect their models' actual value prioritization when facing nuanced moral reasoning in daily-life settings. Finally, we find that end users cannot effectively steer such prioritization using system prompts."}, {"title": "HydraGAN A Multi-head, Multi-objective Approach to Synthetic Data Generation", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Macro Level/Rights Protection/Privacy/HydraGAN A Multi-head, Multi-objective Approach to Synthetic Data Generation.pdf", "summary": {"paper_id": "survey-2_paper-11_HydraGAN A Multi-head, Multi-objective Approach to Synthetic Data Generation", "core_task": {"text": "privacy-preserving synthetic data generation with multiple objectives", "query_variants": ["privacy-preserving synthetic data generation with multiple objectives", "multi-objective generation of privacy-protected synthetic datasets", "synthetic data creation preserving privacy under multiple criteria"]}, "contributions": [{"name": "Multi-objective synthetic data generation for privacy preservation", "description": "Existing synthetic data generation methods optimize only data realism, but when real data must be replaced by synthetic data for privacy preservation, multiple competing criteria (realism, model accuracy, privacy) must be simultaneously optimized. This creates a fundamental challenge in balancing the privacy-utility trade-off.", "author_claim_text": "Traditional methods are valuable for augmenting costly datasets but only optimize one criterion: realism. In this paper, we tackle the problem of generating synthetic data that optimize multiple criteria.", "source_hint": "Abstract"}, {"name": "Privacy-preserving data sharing while maintaining utility", "description": "The rapid increase in personal data collection from IoT devices creates a need to share data for collaboration and research while protecting sensitive information. Existing privacy-preserving methods face a trade-off where increased privacy reduces data utility and model accuracy, motivating a method that can optimize both simultaneously.", "author_claim_text": "Sharing data can support collaboration and promote new findings and advances, yet sharing sensitive data may violate privacy constraints and even endanger individuals whose personal information can be derived from the data.", "source_hint": "Introduction paragraph 1-2"}, {"name": "Multi-agent GAN with privacy-preserving discriminator", "description": "HydraGAN introduces a novel multi-agent GAN architecture that includes both a realism discriminator and a privacy-preserving re-identification discriminator. The re-identification discriminator actively attempts to identify sensitive attributes, training the generator to produce data resistant to such attacks through adversarial learning rather than static noise addition.", "author_claim_text": "We introduce HydraGAN, a multi-agent generative adversarial network (GAN) that game-theoretically optimizes multiple criteria to generate privacy-preserving, realistic synthetic data.", "source_hint": "Introduction paragraph 3"}, {"name": "Multi-headed generator for data diversity", "description": "To prevent mode collapse, HydraGAN partitions real data into clusters using k-means and creates a generator with multiple output heads. Each head originates from shared layers and is responsible for simulating one cluster, simplifying the loss landscape and allowing poorly performing heads to be corrected through updates from better performing ones.", "author_claim_text": "HydraGAN divides the data space among multiple generators to promote data diversity.", "source_hint": "Introduction contributions list and Multi-head Generator section"}]}, "abs": "Synthetic data generation overcomes limitations of real-world machine learning. Traditional methods are valuable for augmenting costly datasets but only optimize one criterion: realism. In this paper, we tackle the problem of generating synthetic data that optimize multiple criteria. This goal is necessary when real data are replaced by synthetic for privacy preservation. We introduce HydraGAN, a new approach to synthetic data generation that introduces multiple generator and discriminator agents into the system. The multi-agent GAN optimizes the goal of privacy-preservation as well as data realism. To facilitate multi-agent training, we adapt game-theoretic principles to offer equilibrium guarantees. We observe that HydraGAN outperforms baseline methods for three datasets for multiple criteria of maximizing data realism, maximizing model accuracy, and minimizing re-identification risk."}, {"title": "Empowering Users in Digital Privacy Management through Interactive LLM-Based Agents", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Macro Level/Rights Protection/Privacy/Empowering Users in Digital Privacy Management through Interactive LLM-Based Agents.pdf", "summary": {"paper_id": "survey-2_paper-12_Empowering Users in Digital Privacy Management through Interactive LLM-Based Agents", "core_task": {"text": "privacy policy comprehension and user empowerment through interactive agents", "query_variants": ["privacy policy comprehension and user empowerment through interactive agents", "understanding privacy policies and empowering users via interactive systems", "user understanding of privacy terms and empowerment using conversational agents"]}, "contributions": [{"name": "User comprehension gap in privacy policies", "description": "The paper addresses the challenge that users cannot effectively understand privacy policies due to complex legal language, length, and density, which prevents informed consent and exposes users to privacy risks while complicating regulatory compliance efforts.", "author_claim_text": "privacy policies are notoriously difficult for the average user to comprehend due to their legal complexity, dense language, and considerable length. Consequently, they are frequently ignored or misunderstood, undermining the principle of informed consent", "source_hint": "Introduction paragraph 1"}, {"name": "Empowering users through improved privacy policy interaction", "description": "The authors are motivated by the need to enable users to make informed decisions about their personal data by bridging the comprehension gap in privacy policies, particularly as regulations become more stringent and policies more intricate.", "author_claim_text": "Our research aims to empower users by improving their ability to navigate and comprehend privacy agreements, thus fostering better control over their personal data", "source_hint": "Introduction paragraph 3"}, {"name": "LLM benchmarking on privacy policy tasks", "description": "The authors systematically evaluate large language models (GPT-4o to GPT-3.5) on four key privacy policy tasks using zero-shot and few-shot approaches, establishing new performance benchmarks that demonstrate LLMs significantly outperform traditional models.", "author_claim_text": "we evaluate the performance of LLMs across several core tasks relevant to privacy policies, including Data Practice Identification, Choice Identification, Policy Summarization, and Privacy Question Answering", "source_hint": "Introduction paragraph 4"}, {"name": "Interactive LLM-based agent with heuristic interaction model", "description": "The authors develop an AI agent based on GPT-4o-mini that functions as an expert system, autonomously processing privacy policies, identifying key information, and guiding users through complex legal language without requiring them to pose specific questions.", "author_claim_text": "we introduce an innovative LLM-based agent built upon GPT-4o-mini to assist users in comprehending website privacy policies. The agent employs a heuristic interaction model that simplifies complex legal language without requiring users to know what specific questions to ask", "source_hint": "Introduction paragraph 5"}]}, "abs": "This paper presents a novel application of large language models (LLMs) to enhance user comprehension of privacy policies through an interactive dialogue agent. We demonstrate that LLMs significantly outperform traditional models in tasks like Data Practice Identification, Choice Identification, Policy Summarization, and Privacy Question Answering, setting new benchmarks in privacy policy analysis. Building on these findings, we introduce an innovative LLM-based agent that functions as an expert system for processing website privacy policies, guiding users through complex legal language without requiring them to pose specific questions. A user study with 100 participants showed that users assisted by the agent had higher comprehension levels (mean score of 2.6 out of 3 vs. 1.8 in the control group), reduced cognitive load (task difficulty ratings of 3.2 out of 10 vs. 7.8), increased confidence in managing privacy, and completed tasks in less time (5.5 minutes vs. 15.8 minutes). This work highlights the potential of LLM-based agents to transform user interaction with privacy policies, leading to more informed consent and empowering users in the digital services landscape."}, {"title": "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Macro Level/Sustainability/Sustainability/Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents.pdf", "summary": {"paper_id": "survey-2_paper-13_Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents", "core_task": {"text": "Sustainable cooperation in multi-agent resource sharing dilemmas", "query_variants": ["Sustainable cooperation in multi-agent resource sharing dilemmas", "Long-term collaborative strategies in multi-agent resource allocation problems", "Persistent coordination among multiple agents in shared resource dilemmas"]}, "contributions": [{"name": "LLMs fail to achieve sustainable cooperation in multi-agent resource-sharing scenarios", "description": "Current large language models struggle to maintain sustainable cooperation when managing shared resources in multi-agent settings, with most models unable to prevent resource collapse through coordinated decision-making and negotiation.", "author_claim_text": "all but the most powerful LLM agents fail to achieve a sustainable equilibrium in GOVSIM, with the highest survival rate below 54%", "source_hint": "Abstract"}, {"name": "Need for safe and cooperative AI in multi-agent contexts", "description": "As LLMs increasingly take on complex decision-making roles in multi-agent contexts, understanding their ability to cooperate safely and reliably becomes critical, especially since cooperation challenges mirror those faced by humans in real-world scenarios.", "author_claim_text": "As AI systems pervade human life, ensuring that large language models (LLMs) make safe decisions remains a significant challenge", "source_hint": "Abstract and Introduction"}, {"name": "GOVSIM simulation platform", "description": "A novel simulation environment where AI agents must collectively balance exploiting a common resource with sustaining it for future use, enabling study of ethical considerations, strategic planning, and negotiation skills in multi-agent resource-sharing scenarios inspired by game-theoretic research on cooperation.", "author_claim_text": "We introduce the GOVernance of the Commons SIMulation (GOVSIM), a generative simulation platform designed to study strategic interactions and cooperative decision-making in LLMs", "source_hint": "Abstract and Introduction"}, {"name": "LLM-based generative agent architecture", "description": "An agent architecture based on the generative agents framework that enables LLMs to operate in phase-based environments, making decisions about resource extraction and engaging in open-ended discussion with other agents.", "author_claim_text": "We develop an LLM-based agent architecture and test it with the leading open and closed LLMs", "source_hint": "Abstract and Section 3.1"}]}, "abs": "As AI systems pervade human life, ensuring that large language models (LLMs) make safe decisions remains a significant challenge. We introduce the Governance of the Commons Simulation (GovSim), a generative simulation platform designed to study strategic interactions and cooperative decision-making in LLMs. In GovSim, a society of AI agents must collectively balance exploiting a common resource with sustaining it for future use. This environment enables the study of how ethical considerations, strategic planning, and negotiation skills impact cooperative outcomes. We develop an LLM-based agent architecture and test it with the leading open and closed LLMs. We find that all but the most powerful LLM agents fail to achieve a sustainable equilibrium in GovSim, with the highest survival rate below 54%. Ablations reveal that successful multi-agent communication between agents is critical for achieving cooperation in these cases. Furthermore, our analyses show that the failure to achieve sustainable cooperation in most LLMs stems from their inability to formulate and analyze hypotheses about the long-term effects of their actions on the equilibrium of the group. Finally, we show that agents that leverage \"Universalization\"-based reasoning, a theory of moral thinking, are able to achieve significantly better sustainability. Taken together, GovSim enables us to study the mechanisms that underlie sustainable self-government with specificity and scale. We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface."}, {"title": "Training Socially Aligned Language Models on Simulated Social Interactions", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Macro Level/System Governance/Harmlessness/Training Socially Aligned Language Models on Simulated Social Interactions.pdf", "summary": {"paper_id": "survey-2_paper-14_Training Socially Aligned Language Models on Simulated Social Interactions", "core_task": {"text": "training socially aligned language models through simulated social interactions", "query_variants": ["training socially aligned language models through simulated social interactions", "learning language models with social alignment via simulated interaction environments", "developing socially aligned LMs using simulated conversational interactions"]}, "contributions": [{"name": "Subpar generalization and vulnerability to adversarial attacks in current language models", "description": "Existing language models memorize training data in isolation without social interaction, resulting in poor generalization to new scenarios and susceptibility to adversarial prompts like jailbreaking.", "author_claim_text": "current language models (LMs) are trained to rigidly replicate their training corpus in isolation, leading to subpar generalization in unfamiliar scenarios and vulnerability to adversarial attacks", "source_hint": "Abstract"}, {"name": "Achieving robust social alignment through simulated social interactions", "description": "The authors aim to enable language models to learn social norms and values through simulated social interactions, mirroring how humans acquire societal behaviors, rather than through rigid memorization or proxy reward models.", "author_claim_text": "unlike humans, who derive consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly replicate their training corpus in isolation", "source_hint": "Abstract and Introduction"}, {"name": "SandBox: a simulated society for modeling social interactions", "description": "SandBox is an open-source platform where LM-based agents interact in a simulated society, guided by a latent rule incentive. Agents generate responses, receive collective feedback, and iteratively revise their outputs, producing fine-grained interaction data including ratings, feedback, and revisions.", "author_claim_text": "we create a simulated human society, SandBox, comprising numerous LM-based social agents interacting and we record their behaviors", "source_hint": "Introduction and Section 3.1"}, {"name": "Stable Alignment: a three-stage alignment learning framework", "description": "Stable Alignment trains language models in three stages: Imitation Learning (learning from high-rated responses), Self-Critic (learning to generate feedback), and Realignment (learning to revise misaligned responses). It uses Contrastive Preference Optimization (CPO) to directly optimize policies without an additional reward model.", "author_claim_text": "we present a new alignment learning framework, Stable Alignment, which learns from simulated social interactions in three stages", "source_hint": "Introduction and Section 3.2"}]}, "abs": "Social alignment in AI systems aims to ensure that these models behave according to established societal values. However, unlike humans, who derive consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly replicate their training corpus in isolation, leading to subpar generalization in unfamiliar scenarios and vulnerability to adversarial attacks. This work presents a novel training paradigm that permits LMs to learn from simulated social interactions. In comparison to existing methodologies, our approach is considerably more scalable and efficient, demonstrating superior performance in alignment benchmarks and human evaluations. This paradigm shift in the training of LMs brings us a step closer to developing AI systems that can robustly and accurately reflect societal norms and values."}, {"title": "SafeSora: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Macro Level/System Governance/Harmlessness/SafeSora: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset.pdf", "summary": {"paper_id": "survey-2_paper-15_SafeSora: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset", "core_task": {"text": "safety alignment of text-to-video generation", "query_variants": ["safety alignment of text-to-video generation", "aligning text-to-video models for safe content generation", "safe alignment in text-conditioned video synthesis"]}, "contributions": [{"name": "Lack of datasets for text-to-video alignment research", "description": "The paper identifies a significant gap in the text-to-video generation field: while text-to-text alignment is well-supported by extensive research and datasets, the text-to-video domain lacks datasets for studying human value alignment and safety.", "author_claim_text": "the text-to-video domain remains underdeveloped, notably lacking in available datasets", "source_hint": "Introduction paragraph 2"}, {"name": "Mitigating risks of harmful outputs from large vision models", "description": "The authors are motivated by concerns that misalignment of AI systems with human values could lead to severe consequences, especially as capabilities expand to video generation. They aim to address potential misuse of powerful text-to-video models like Sora.", "author_claim_text": "To mitigate the risk of harmful outputs from large vision models (LVMs), we introduce the SAFE SORA dataset to promote research on aligning text-to-video generation with human values", "source_hint": "Abstract"}, {"name": "Two-stage human annotation process with decoupled dimensions", "description": "The authors design a structured annotation process that separates helpfulness and harmlessness into parallel dimensions. The first heuristic stage guides annotators through sub-dimensions (4 for helpfulness, 12 harm categories for harmlessness), followed by overall preference judgments in the second stage.", "author_claim_text": "we have developed a two-stage annotation process that guides crowdworkers to interpret the concepts of helpfulness and harmlessness according to their own perceptions", "source_hint": "Introduction paragraph 4, Section 3.3"}, {"name": "Prompt augmentation and multi-model video generation pipeline", "description": "The authors employ a video generation pipeline that uses both original and LLM-refined prompts to generate multiple videos per prompt using four distinct large vision models (both open-source and closed-source), creating diverse text-video pairs for annotation.", "author_claim_text": "our dataset includes both the direct use of original user instructions for video generation and the utilization of LLMs (such as GPT-4 and Llama) as a prompt refiner", "source_hint": "Section 3.2"}]}, "abs": "To mitigate the risk of harmful outputs from large vision models (LVMs), we introduce the SafeSora dataset to promote research on aligning text-to-video generation with human values. This dataset encompasses human preferences in text-to-video generation tasks along two primary dimensions: helpfulness and harmlessness. To capture in-depth human preferences and facilitate structured reasoning by crowdworkers, we subdivide helpfulness into 4 sub-dimensions and harmlessness into 12 sub-categories, serving as the basis for pilot annotations. The SafeSora dataset includes 14,711 unique prompts, 57,333 unique videos generated by 4 distinct LVMs, and 51,691 pairs of preference annotations labeled by humans. We further demonstrate the utility of the SafeSora dataset through several applications, including training the text-video moderation model and aligning LVMs with human preference by fine-tuning a prompt augmentation module or the diffusion model. These applications highlight its potential as the foundation for text-to-video alignment research, such as human preference modeling and the development and validation of alignment algorithms."}, {"title": "A Multimodal Automated Interpretability Agent", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Macro Level/System Governance/Transparency/A Multimodal Automated Interpretability Agent.pdf", "summary": {"paper_id": "survey-2_paper-16_A Multimodal Automated Interpretability Agent", "core_task": {"text": "Automated interpretability of neural network features and behaviors", "query_variants": ["Automated interpretability of neural network features and behaviors", "Automatic explanation of neural network representations and learned patterns", "Mechanistic understanding of deep learning model components and activations"]}, "contributions": [{"name": "Expensive and slow neural model understanding", "description": "Understanding neural models (e.g., identifying when classifiers rely on sensitive features, discovering systematic errors, or improving robustness) currently demands substantial manual effort from researchers, making it slow and costly even for widely deployed systems.", "author_claim_text": "this kind of understanding requires significant effort on the part of researchers—involving exploratory data analysis, formulation of hypotheses, and controlled experimentation. As a consequence, this kind of understanding is slow and expensive to obtain even about the most widely used models.", "source_hint": "Introduction paragraph 1"}, {"name": "Combining flexibility of human experimentation with scalability of automation", "description": "The authors aim to develop tools that preserve the flexible, hypothesis-driven nature of human interpretability research while achieving the scalability of automated methods, addressing limitations of current automated interpretability approaches that are low-precision and useful mainly for hypothesis generation.", "author_claim_text": "How can we build tools that help users understand models, while combining the flexibility of human experimentation with the scalability of automated techniques?", "source_hint": "Introduction paragraph 2"}, {"name": "MAIA: Multimodal Automated Interpretability Agent", "description": "MAIA is a system that uses a pretrained vision-language model (GPT-4V) equipped with an API of interpretability tools (for synthesizing/editing inputs, computing maximally activating exemplars, and summarizing results) to autonomously design and execute iterative experiments on neural network subcomponents, composing these tools into Python programs to explain system behavior.", "author_claim_text": "This paper introduces a prototype system we call the Multimodal Automated Interpretability Agent (MAIA), which combines a pretrained vision-language model backbone with an API containing tools designed for conducting experiments on deep networks.", "source_hint": "Abstract and Introduction paragraph 3"}, {"name": "System and Tools API for modular experimentation", "description": "The MAIA API consists of a System class that instruments target models and makes subcomponents callable, and a Tools class providing functions for dataset exemplar generation, image synthesis/editing (via Stable Diffusion and Instruct-Pix2Pix), image description/summarization, and experiment logging, enabling modular composition of interpretability procedures.", "author_claim_text": "We enable the VLM to design and run interpretability experiments using the MAIA API, which defines two classes: the System class and the Tools class", "source_hint": "Section 3, subsections 3.1 and 3.2"}]}, "abs": "This paper describes MAIA, a Multimodal Automated Interpretability Agent. MAIA is a system that uses neural models to automate neural model understanding tasks like feature interpretation and failure mode discovery. It equips a pre-trained vision-language model with a set of tools that support iterative experimentation on subcomponents of other models to explain their behavior. These include tools commonly used by human interpretability researchers: for synthesizing and editing inputs, computing maximally activating exemplars from real-world datasets, and summarizing and describing experimental results. Interpretability experiments proposed by MAIA compose these tools to describe and explain system behavior. We evaluate applications of MAIA to computer vision models. We first characterize MAIA's ability to describe (neuron-level) features in learned representations of images. Across several trained models and a novel dataset of synthetic vision neurons with paired ground-truth descriptions, MAIA produces descriptions comparable to those generated by expert human experimenters. We then show that MAIA can aid in two additional interpretability tasks: reducing sensitivity to spurious features, and automatically identifying inputs likely to be mis-classified."}, {"title": "Situation-Aware Approach to Improve Context-based Recommender System", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Macro Level/System Governance/Transparency/Situation-Aware Approach to Improve Context-based Recommender System.pdf", "summary": {"paper_id": "survey-2_paper-17_Situation-Aware Approach to Improve Context-based Recommender System", "core_task": {"text": "context-aware recommendation for mobile users", "query_variants": ["context-aware recommendation for mobile users", "contextual recommendation systems for smartphone users", "mobile user recommendation with contextual information"]}, "contributions": [{"name": "Static user profiles in mobile recommender systems fail to adapt to dynamic contextual changes", "description": "Existing recommender systems for mobile users rely on static user profiles based solely on computational behavior, ignoring the user's surrounding environment (location, time, nearby people) and failing to adapt to changing user interests across different situations.", "author_claim_text": "The main limitation of such approaches is that they do not take into account the dynamicity of user interests regarding his situation.", "source_hint": "Section 2, Related work"}, {"name": "Mobile users' information needs depend on contextual factors requiring situation-aware recommendations", "description": "The authors are motivated by evidence that mobile users' information needs are strongly tied to contextual factors including interests, social groups, location, and time. This necessitates recommender systems that can provide information tailored to individual users and adapted to their current situation.", "author_claim_text": "Studies on logs of mobile Internet user interactions show that most of the information needs of mobile users are related to contextual factors such as the user's interests, his social group, the location and the time of the interaction.", "source_hint": "Introduction, Section 1"}, {"name": "Situation-aware user profile representation using social-spatiotemporal ontologies", "description": "The authors propose representing user situations as combinations of semantic concepts extracted from social, spatial, and temporal ontologies, abstracting low-level sensor data (GPS coordinates, time, agenda) into meaningful semantic information characterizing the user's interaction context.", "author_claim_text": "A situation is represented as a combination of social-spatiotemporal concepts inferred from ontological knowledge given social group, location and time information.", "source_hint": "Abstract and Section 3.2"}, {"name": "Dynamic user profile modeling with situation-specific preferences", "description": "The method learns user preferences for each identified situation based on past user interactions within that situation. The user model consists of situations paired with corresponding preferences, where preferences are built from navigation activities and documents the user engaged with in specific contexts.", "author_claim_text": "A user preference is learnt for each identified situation, on the basis of past interaction activities occurred in this situation.", "source_hint": "Introduction and Section 3.1"}]}, "abs": "In this paper, we introduce a novel situation aware approach to improve a context based recommender system. To build situation aware user profiles, we rely on evidence issued from retrieval situations. A retrieval situation refers to the social spatio temporal context of the user when he interacts with the recommender system. A situation is represented as a combination of social spatio temporal concepts inferred from ontological knowledge given social group, location and time information. User's interests are inferred from past user's interaction with the recommender system related to the identified situations. They are represented using concepts issued from a domain ontology. We also propose a method to dynamically adapt the system to the user's interest's evolution."}, {"title": "BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Macro Level/System Governance/Transparency/BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments.pdf", "summary": {"paper_id": "survey-2_paper-18_BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments", "core_task": {"text": "designing genetic perturbation experiments", "query_variants": ["designing genetic perturbation experiments", "planning genetic intervention experimental designs", "constructing experimental designs for genetic manipulations"]}, "contributions": [{"name": "Inefficient design of genetic perturbation experiments", "description": "The paper addresses the challenge of efficiently identifying a small subset of genes from thousands of candidates that, when perturbed, produce a desired phenotype. Existing approaches require training bespoke machine learning models that are difficult to interpret and cannot leverage biological prior knowledge from scientific literature.", "author_claim_text": "experimentally perturbing every single gene is costly. A perturbation screen typically perturbs around 19,000 protein-coding genes, yet anywhere between a handful of genes to a few thousand may exhibit the desired phenotype.", "source_hint": "Introduction paragraph 2"}, {"name": "Leveraging LLM biological knowledge for experiment design", "description": "The authors are motivated by the potential of large language models to leverage vast biological knowledge from scientific literature for closed-loop experiment design, which has not been explored before. This could overcome limitations of conventional Bayesian optimization methods that cannot access such prior knowledge and are difficult to interpret.", "author_claim_text": "LLMs have been exposed to the scientific literature and can recall biomedical knowledge. Such models have shown state of the art performance in information retrieval for clinical question answering, patient matching for clinical trials, gene set identification, gene function prediction and cell type annotation using gene expression data.", "source_hint": "Introduction paragraph 3"}, {"name": "BioDiscoveryAgent framework", "description": "The authors propose an AI agent that uses large language models to design genetic perturbation experiments iteratively. At each round, the agent receives a prompt describing the experimental setup and previous results, then predicts genes to perturb next using structured reasoning (Reflection, Research Plan, Solution) without requiring a trained machine learning model or explicit acquisition function.", "author_claim_text": "we introduce BioDiscoveryAgent, an agent that designs new experiments, reasons about their outcomes, and efficiently navigates the hypothesis space to reach desired solutions.", "source_hint": "Abstract and Section 3"}, {"name": "Agent tools for enhanced decision-making", "description": "BioDiscoveryAgent is equipped with three tools: literature search via PubMed API to retrieve relevant papers, gene search using biological databases (Reactome) to identify genes in similar pathways, and an AI critic agent to review and refine predictions. These tools enable the agent to access information beyond its training data.", "author_claim_text": "The agent also has access to tools for searching the biomedical literature, executing code to analyze biological datasets, and prompting another agent to critically evaluate its predictions.", "source_hint": "Abstract and Section 3.1"}]}, "abs": "Agents based on large language models have shown great potential in accelerating scientific discovery by leveraging their rich background knowledge and reasoning capabilities. In this paper, we introduce BioDiscoveryAgent, an agent that designs new experiments, reasons about their outcomes, and efficiently navigates the hypothesis space to reach desired solutions. We demonstrate our agent on the problem of designing genetic perturbation experiments, where the aim is to find a small subset out of many possible genes that, when perturbed, result in a specific phenotype (e.g., cell growth). Utilizing its biological knowledge, BioDiscoveryAgent can uniquely design new experiments without the need to train a machine learning model or explicitly design an acquisition function as in Bayesian optimization. Moreover, BioDiscoveryAgent, using Claude 3.5 Sonnet, achieves an average of 21% improvement in predicting relevant genetic perturbations across six datasets, and a 46% improvement in the harder task of non-essential gene perturbation, compared to existing Bayesian optimization baselines specifically trained for this task. Our evaluation includes one dataset that is unpublished, ensuring it is not part of the language model's training data. Additionally, BioDiscoveryAgent predicts gene combinations to perturb more than twice as accurately as a random baseline, a task so far not explored in the context of closed-loop experiment design. The agent also has access to tools for searching the biomedical literature, executing code to analyze biological datasets, and prompting another agent to critically evaluate its predictions. Overall, BioDiscoveryAgent is interpretable at every stage, representing an accessible new paradigm in the computational design of biological experiments with the potential to augment scientists' efficacy."}, {"title": "LLM-GLOBE: A Benchmark Evaluating the Cultural Values Embedded in LLM Output", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Meso Level/Cultural Level/Individualism/Collectivism/LLM-GLOBE: A Benchmark Evaluating the Cultural Values Embedded in LLM Output.pdf", "summary": {"paper_id": "survey-2_paper-19_LLM-GLOBE: A Benchmark Evaluating the Cultural Values Embedded in LLM Output", "core_task": {"text": "evaluating cultural values embedded in large language model output", "query_variants": ["evaluating cultural values embedded in large language model output", "assessing cultural biases reflected in LLM generated text", "measuring cultural norms encoded in large language model responses"]}, "contributions": [{"name": "Limited research on cultural values embedded in LLM output", "description": "While significant effort has been dedicated to minimizing harmful or biased content in AI, research examining the cultural values reflected in large language model outputs remains underdeveloped, despite culture's fundamental role in shaping societal norms and decision-making.", "author_claim_text": "research investigating the cultural values of LLMs is still in very early stages", "source_hint": "Abstract"}, {"name": "Need for culturally-adept AI systems to prevent cultural encapsulation", "description": "As LLMs become increasingly integrated into people's routines and decision-making processes, understanding and aligning AI cultural values with diverse human cultural contexts is imperative to prevent cultural encapsulation, ensure appropriate representation, and facilitate effective cross-cultural deployment.", "author_claim_text": "cultural competencies and humility are critical in establishing meaningful relationships and creating open discourse", "source_hint": "Introduction §1"}, {"name": "LLM-GLOBE benchmark based on GLOBE framework", "description": "The authors adapt the Global Leadership and Organizational Behavior Effectiveness (GLOBE) framework, which defines nine cultural dimensions validated across 62 societies, to create a comprehensive benchmark for systematically evaluating cultural values embedded in LLM outputs through both closed-ended and open-generation prompts.", "author_claim_text": "we draw upon cultural psychology theory and the empirically-validated GLOBE framework to propose the LLM-GLOBE benchmark for evaluating the cultural value systems of LLMs", "source_hint": "Abstract"}, {"name": "LLMs-as-a-Jury automated evaluation pipeline", "description": "A novel automated scoring protocol where a panel of leading LLMs (GPT-4, Claude 3 Opus, Ernie 4.0, Qwen-72b) evaluate open-ended responses along cultural dimensions, with their ratings aggregated via least-squares regression trained on human-labeled data to produce final cultural value scores.", "author_claim_text": "Our methodology includes a novel 'LLMs-as-a-Jury' pipeline which automates the evaluation of open-ended content to enable large-scale analysis at a conceptual level", "source_hint": "Abstract"}]}, "abs": "Immense effort has been dedicated to minimizing the presence of harmful or biased generative content and better aligning AI output to human intention; however, research investigating the cultural values of LLMs is still in very early stages. Cultural values underpin how societies operate, providing profound insights into the norms, priorities, and decision making of their members. In recognition of this need for further research, we draw upon cultural psychology theory and the empirically-validated GLOBE framework to propose the LLM-GLOBE benchmark for evaluating the cultural value systems of LLMs, and we then leverage the benchmark to compare the values of Chinese and US LLMs. Our methodology includes a novel \"LLMs-as-a-Jury\" pipeline which automates the evaluation of open-ended content to enable large-scale analysis at a conceptual level. Results clarify similarities and differences that exist between Eastern and Western cultural value systems and suggest that open-generation tasks represent a more promising direction for evaluation of cultural values. We interpret the implications of this research for subsequent model development, evaluation, and deployment efforts as they relate to LLMs, AI cultural alignment more broadly, and the influence of AI cultural value systems on human-AI collaboration outcomes."}, {"title": "Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Meso Level/Cultural Level/Individualism/Collectivism/Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions.pdf", "summary": {"paper_id": "survey-2_paper-20_Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions", "core_task": {"text": "cultural alignment in large language models", "query_variants": ["cultural alignment in large language models", "aligning LLMs with cultural values and norms", "cultural adaptation of large-scale language models"]}, "contributions": [{"name": "Cultural misalignment in large language models", "description": "LLMs predominantly reflect Western cultural values while failing to account for cultural variances among diverse user groups, leading to potential misunderstandings and exacerbated cultural tensions.", "author_claim_text": "The deployment of large language models (LLMs) raises concerns regarding their cultural misalignment and potential ramifications on individuals and societies with diverse cultural backgrounds.", "source_hint": "Abstract"}, {"name": "Need for explanatory cultural alignment assessment", "description": "Existing research lacks a comprehensive framework for explaining and quantifying how well LLMs align with different cultural values across diverse regions, necessitating a systematic approach to measure cultural alignment.", "author_claim_text": "While some research explores the cultural bias of LLMs for specific countries like the US, there remains a gap presenting an explanatory reasoning of cultural alignment.", "source_hint": "Introduction"}, {"name": "Cultural Alignment Test based on Hofstede's framework", "description": "The authors propose Hofstede's CAT, which uses the VSM13 questionnaire with 24 Likert-scale questions to quantify LLM alignment across six cultural dimensions (PDI, IDV, MAS, UAI, LTO, IVR) using Kendall Tau correlation coefficients.", "author_claim_text": "We introduce a novel method for assessing LLMs' alignment with countries' cultural values using Hofstede's dimensions, offering insights into explanatory cultural differences.", "source_hint": "Abstract and Section 2"}, {"name": "Multiple prompting strategies for cultural assessment", "description": "The methodology includes model-level comparison (direct questioning in multiple languages), country-level comparison (instructing models to act as specific nationalities), hyperparameter comparison (varying temperature and top-p), and language correlation analysis (comparing models fine-tuned on different languages).", "author_claim_text": "We design four ways to prompt LLMs to evaluate their intrinsic cultural values and their cultural alignment within three regions: the United States, China, and Arab countries.", "source_hint": "Introduction and Section 2"}]}, "abs": "The deployment of large language models (LLMs) raises concerns regarding their cultural misalignment and potential ramifications on individuals and societies with diverse cultural backgrounds. While the discourse has focused mainly on political and social biases, our research proposes a Cultural Alignment Test (Hoftede's CAT) to quantify cultural alignment using Hofstede's cultural dimension framework, which offers an explanatory cross-cultural comparison through the latent variable analysis. We apply our approach to quantitatively evaluate LLMs, namely Llama 2, GPT-3.5, and GPT-4, against the cultural dimensions of regions like the United States, China, and Arab countries, using different prompting styles and exploring the effects of language-specific fine-tuning on the models' behavioural tendencies and cultural values. Our results quantify the cultural alignment of LLMs and reveal the difference between LLMs in explanatory cultural dimensions. Our study demonstrates that while all LLMs struggle to grasp cultural values, GPT-4 shows a unique capability to adapt to cultural nuances, particularly in Chinese settings. However, it faces challenges with American and Arab cultures. The research also highlights that fine-tuning LLama 2 models with different languages changes their responses to cultural questions, emphasizing the need for culturally diverse development in AI for worldwide acceptance and ethical use. For more details or to contribute to this research, visit our GitHub page this https URL"}, {"title": "CDEval: A Benchmark for Measuring the Cultural Dimensions of Large Language Models", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Meso Level/Cultural Level/Individualism/Collectivism/CDEval: A Benchmark for Measuring the Cultural Dimensions of Large Language Models.pdf", "summary": {"paper_id": "survey-2_paper-21_CDEval: A Benchmark for Measuring the Cultural Dimensions of Large Language Models", "core_task": {"text": "Evaluating cultural dimensions of large language models", "query_variants": ["Evaluating cultural dimensions of large language models", "Assessing cultural aspects in large language models", "Measuring cultural characteristics of large language models"]}, "contributions": [{"name": "Lack of cultural dimension evaluation in LLM alignment", "description": "Current LLM alignment research focuses primarily on universal values like helpfulness, honesty, and harmlessness, but neglects the pluralistic and diverse nature of culture. This gap means LLMs may not adequately respect or reflect cultural diversity in their responses.", "author_claim_text": "While existing alignment efforts predominantly concentrate on universal values such as the HHH (helpfulness, honesty, and harmlessness), the aspect of culture, which is inherently pluralistic and diverse, has not received adequate attention.", "source_hint": "Abstract"}, {"name": "Importance of cultural diversity in LLM development", "description": "The authors are motivated by the recognition that human values are pluralistic and individuals from different cultural backgrounds hold varied viewpoints. LLMs need to respect cultural diversity beyond universal values to serve diverse populations appropriately.", "author_claim_text": "LLMs should not only align with universal human values, demonstrating the capability to discern between right and wrong, but also honor and respect the rich tapestry of cultural diversity.", "source_hint": "Introduction §1"}, {"name": "CDEval benchmark construction pipeline", "description": "The authors propose a three-step pipeline to construct CDEval: (1) defining a taxonomy based on Hofstede's six cultural dimensions across seven domains with multiple-choice format, (2) generating questions using GPT-4 with zero-shot and few-shot prompting, and (3) manually verifying generated data under specific rules.", "author_claim_text": "The construction pipeline includes three steps. The first step is schema definition, which involves defining the taxonomy and the format of questions related to diverse culture dimensions. The second step is data generation using GPT-4, employing both zero-shot and few-shot prompts. The final step is checking the generated data manually under verification rules.", "source_hint": "Section 3.1, Figure 2"}, {"name": "Questionnaire-based evaluation using LLMs as respondents", "description": "The authors use a questionnaire-based approach where LLMs act as respondents to multiple-choice questions. Each question presents two options representing opposing cultural orientations, allowing measurement of LLMs' cultural tendencies across different dimensions.", "author_claim_text": "In this work, we employ LLMs as respondents, as discussed in (Scherrer et al., 2023), to investigate the culture of LLMs by administering questionnaires.", "source_hint": "Section 3, Introduction to Section 3.1"}]}, "abs": "As the scaling of Large Language Models (LLMs) has dramatically enhanced their capabilities, there has been a growing focus on the alignment problem to ensure their responsible and ethical use. While existing alignment efforts predominantly concentrate on universal values such as the HHH principle, the aspect of culture, which is inherently pluralistic and diverse, has not received adequate attention. This work introduces a new benchmark, CDEval, aimed at evaluating the cultural dimensions of LLMs. CDEval is constructed by incorporating both GPT-4's automated generation and human verification, covering six cultural dimensions across seven domains. Our comprehensive experiments provide intriguing insights into the culture of mainstream LLMs, highlighting both consistencies and variations across different dimensions and domains. The findings underscore the importance of integrating cultural considerations in LLM development, particularly for applications in diverse cultural settings. Through CDEval, we aim to broaden the horizon of LLM alignment research by including cultural dimensions, thus providing a more holistic framework for the future development and evaluation of LLMs. This benchmark serves as a valuable resource for cultural studies in LLMs, paving the way for more culturally aware and sensitive models."}, {"title": "How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Meso Level/Cultural Level/Individualism/Collectivism/How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions.pdf", "summary": {"paper_id": "survey-2_paper-22_How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions", "core_task": {"text": "cultural value representation in large language models across different countries", "query_variants": ["cultural value representation in large language models across different countries", "cross-national cultural value encoding in LLMs", "representation of cultural values in LLMs from multiple nations"]}, "contributions": [{"name": "LLMs fail to consistently uphold cultural values when giving advice", "description": "The paper identifies that although large language models can recognize differences in cultural values across countries, they do not reliably reflect these values in their responses when providing advice to users from diverse cultural backgrounds.", "author_claim_text": "Our analysis reveals that while LLMs can differentiate cultural values, they often fail to consistently uphold them when giving advice.", "source_hint": "Abstract"}, {"name": "Need for culturally appropriate advice to prevent cultural conflicts", "description": "The authors are motivated by the concern that LLMs serving diverse global users may provide advice conflicting with users' cultural values, potentially causing community disapproval and lasting ramifications. They aim to ensure LLMs can deliver culturally sensitive advice aligned with users' societal values.", "author_claim_text": "Users should receive culturally appropriate advice to prevent cultural conflicts. In our work, we investigate whether LLMs embody Hofstede cultural dimensions when providing advice to users.", "source_hint": "Introduction paragraph 2"}, {"name": "Systematic evaluation using Hofstede cultural dimensions with personas and multilingual prompts", "description": "The authors create 50 unique prompts for each of five Hofstede cultural dimensions, testing them with personas declaring nationality and multilingual translations across 36 languages. They use five state-of-the-art LLMs to generate responses and classify whether responses adhere to expected country values.", "author_claim_text": "We prompt different LLMs with advice requests based on Hofstede Cultural Dimensions, incorporating personas representing 36 countries and languages.", "source_hint": "Abstract and Methodology section 3"}, {"name": "Framework for understanding cultural alignment issues", "description": "The authors develop an empirical framework that analyzes quantifiable cultural values through balanced binary questions, examining whether models adhere to specific values across languages and resource levels, and evaluating justifications to distinguish cultural understanding from stereotypes.", "author_claim_text": "We present recommendations for training culturally sensitive LLMs and introduce a framework for understanding cultural alignment issues.", "source_hint": "Abstract and Discussion section 5"}]}, "abs": "Large Language Models (LLMs) attempt to imitate human behavior by responding to humans in a way that pleases them, including by adhering to their values. However, humans come from diverse cultures with different values. It is critical to understand whether LLMs showcase different values to the user based on the stereotypical values of a user's known country. We prompt different LLMs with a series of advice requests based on 5 Hofstede Cultural Dimensions -- a quantifiable way of representing the values of a country. Throughout each prompt, we incorporate personas representing 36 different countries and, separately, languages predominantly tied to each country to analyze the consistency in the LLMs' cultural understanding. Through our analysis of the responses, we found that LLMs can differentiate between one side of a value and another, as well as understand that countries have differing values, but will not always uphold the values when giving advice, and fail to understand the need to answer differently based on different cultural values. Rooted in these findings, we present recommendations for training value-aligned and culturally sensitive LLMs. More importantly, the methodology and the framework developed here can help further understand and mitigate culture and language alignment issues with LLMs."}, {"title": "The Utility of Large Language Models and Generative AI for Education Research", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Meso Level/Industry Level/Education/The Utility of Large Language Models and Generative AI for Education Research.pdf", "summary": {"paper_id": "survey-2_paper-23_The Utility of Large Language Models and Generative AI for Education Research", "core_task": {"text": "thematic analysis of unstructured text data in engineering education", "query_variants": ["thematic analysis of unstructured text data in engineering education", "qualitative coding of free-form textual data in engineering pedagogy", "theme extraction from unstructured textual information in engineering teaching contexts"]}, "contributions": [{"name": "Labor-intensive manual analysis of qualitative text data in engineering education", "description": "The paper addresses the challenge that analyzing qualitative data such as student essays, reflections, and interview transcripts in engineering education is time-consuming and resource-intensive, often causing valuable insights to go unobserved due to lack of efficient analysis methods.", "author_claim_text": "While accessing these insights can be labor-intensive if done manually, recent advances in NLP and large language models have made it a realistic option for individuals.", "source_hint": "Abstract"}, {"name": "Scaling qualitative analysis to extract insights from large text datasets", "description": "The authors are motivated by the need to enable educators and researchers to efficiently analyze large volumes of qualitative data (e.g., thousands of student essays or survey responses) to uncover patterns and themes that would otherwise remain hidden, thereby informing teaching strategies and program development.", "author_claim_text": "How do they analyze those data at scale? Often, valuable insights from those data go unobserved due to a lack of efficient analysis methods.", "source_hint": "Introduction §1"}, {"name": "Inductive clustering and summarization using text embeddings and LLMs", "description": "The authors propose an inductive approach that embeds student essay sentences into vector space, clusters semantically similar sentences, and uses a generative language model (GPT-3.5) to summarize clusters at multiple levels of abstraction, ultimately generating a preliminary codebook of themes.", "author_claim_text": "This study explores and evaluates a combination of clustering, summarization, and prompting techniques to analyze over 1,000 student essays in which students discussed their career interests.", "source_hint": "Abstract and Methods §3.3.1"}, {"name": "Deductive labeling with a priori codes using LLM prompting", "description": "The authors demonstrate a deductive approach where pre-existing coding schemes (O*NET job titles and career satisfaction factors) are applied to student essays via prompts to a generative model, enabling researchers to identify specific topics or themes without manual dictionary creation.", "author_claim_text": "We also used a set of a priori codes about career satisfaction and sectors to demonstrate an alternative approach to using these generative text models to analyze student writing.", "source_hint": "Abstract and Methods §3.3.2"}]}, "abs": "The use of natural language processing (NLP) techniques in engineering education can provide valuable insights into the underlying processes involved in generating text. While accessing these insights can be labor-intensive if done manually, recent advances in NLP and large language models have made it a realistic option for individuals. This study explores and evaluates a combination of clustering, summarization, and prompting techniques to analyze over 1,000 student essays in which students discussed their career interests. The specific assignment prompted students to define and explain their career goals as engineers. Using text embedding representations of student responses, we clustered the responses together to identify thematically similar statements from students. The clustered responses were then summarized to quickly identify career interest themes. We also used a set of a priori codes about career satisfaction and sectors to demonstrate an alternative approach to using these generative text models to analyze student writing. The results of this study demonstrate the feasibility and usefulness of NLP techniques in engineering education research. By automating the initial analysis of student essays, researchers and educators can more efficiently and accurately identify key themes and patterns in student writing. The methods presented in this paper have broader applications for engineering education and research purposes beyond analyzing student essays. By explaining these methods to the engineering education community, readers can utilize them in their own contexts."}, {"title": "The Ethics of AI in Games", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Meso Level/Industry Level/Gaming/The Ethics of AI in Games.pdf", "summary": {"paper_id": "survey-2_paper-24_The Ethics of AI in Games", "core_task": {"text": "ethical considerations of AI in video games", "query_variants": ["ethical considerations of AI in video games", "moral implications of artificial intelligence in gaming", "ethics of AI systems in interactive entertainment"]}, "contributions": [{"name": "Lack of comprehensive review on ethics of AI in games", "description": "The paper addresses the absence of a thorough examination of ethical concerns arising from the adoption of AI tools in the video game industry, particularly in the context of player modeling and affective computing systems.", "author_claim_text": "Motivated by the lack of a comprehensive review on the ethics of AI as applied to games, we survey the current state of the art in this area", "source_hint": "Abstract"}, {"name": "Critical role of games in understanding human behavior at scale", "description": "The authors are motivated by the importance of video games as a medium for studying human behavior and emotion, combined with the growing ethical concerns as AI tools become more prevalent in game development and player experience management.", "author_claim_text": "Video games are one of the richest and most popular forms of human-computer interaction and, hence, their role is critical for our understanding of human behaviour and affect at a large scale.", "source_hint": "Abstract"}, {"name": "Structuring ethical analysis through the affective game loop", "description": "The authors organize their ethical analysis by examining AI systems through the phases of the affective game loop: elicitation (emotion induction), sensing (data capture), detection (affect modeling), and adaptation (content adjustment). This framework allows systematic examination of ethical challenges at each stage of affect-driven game systems.", "author_claim_text": "we structure the discussion of AI ethics in games around the affective game loop. The affective game loop describes the relationships between emotion expression, elicitation, detection, prediction, and subsequent reaction.", "source_hint": "Introduction §1"}, {"name": "Need for open dialogue and action on game AI ethics", "description": "The paper concludes that addressing ethical concerns in game AI requires active dialogue among all stakeholders—researchers, developers, policymakers, and players—to establish frameworks that protect users while enabling developers to create better, safer gaming experiences.", "author_claim_text": "This paper calls for an open dialogue and action for the games of today and the virtual spaces of the future. By setting an appropriate framework we aim to protect users and to guide developers towards safer and better experiences", "source_hint": "Abstract"}]}, "abs": "Video games are one of the richest and most popular forms of human-computer interaction and, hence, their role is critical for our understanding of human behaviour and affect at a large scale. As artificial intelligence (AI) tools are gradually adopted by the game industry a series of ethical concerns arise. Such concerns, however, have so far not been extensively discussed in a video game context. Motivated by the lack of a comprehensive review of the ethics of AI as applied to games, we survey the current state of the art in this area and discuss ethical considerations of these systems from the holistic perspective of the affective loop. Through the components of this loop, we study the ethical challenges that AI faces in video game development. Elicitation highlights the ethical boundaries of artificially induced emotions; sensing showcases the trade-off between privacy and safe gaming spaces; and detection, as utilised during in-game adaptation, poses challenges to transparency and ownership. This paper calls for an open dialogue and action for the games of today and the virtual spaces of the future. By setting an appropriate framework we aim to protect users and to guide developers towards safer and better experiences for their customers."}, {"title": "Towards Accountability in the Use of Artificial Intelligence for Public Administrations", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Meso Level/National Level/China/Towards Accountability in the Use of Artificial Intelligence for Public Administrations.pdf", "summary": {"paper_id": "survey-2_paper-25_Towards Accountability in the Use of Artificial Intelligence for Public Administrations", "core_task": {"text": "accountability in the use of artificial intelligence for public administrations", "query_variants": ["accountability in the use of artificial intelligence for public administrations", "responsible AI deployment in government and public sector organizations", "transparency and oversight of AI systems in public governance"]}, "contributions": [{"name": "Imperfect delegation in AI systems challenges human accountability", "description": "The paper identifies that when public administrations delegate tasks to AI systems, three phenomena (distributed responsibility, induced acceptance, and acceptance through ignorance) create imperfect delegation scenarios that undermine the ability to hold human agents accountable for decisions and outcomes.", "author_claim_text": "the phenomena of distributed responsibility, induced acceptance, and acceptance through ignorance constitute instances of imperfect delegation when tasks are delegated to computationally-driven systems. Imperfect delegation challenges human accountability.", "source_hint": "Abstract"}, {"name": "Democratic self-government requires accountability in public AI use", "description": "The authors are motivated by the principle that democratic self-government demands accountability when public administrations use AI. They argue this is not merely instrumentally valuable but a deontological requirement, meaning citizens have a right to control how they are governed regardless of outcomes.", "author_claim_text": "both direct public accountability via public transparency and indirect public accountability via transparency to auditors in public organizations can be both instrumentally ethically valuable and required as a matter of deontology from the principle of democratic self-government.", "source_hint": "Abstract"}, {"name": "Philosophical framework distinguishing direct and indirect public accountability", "description": "The authors develop a conceptual framework that distinguishes two forms of public accountability: direct accountability through public transparency and indirect accountability through auditors. They ground this framework in democratic theory, providing both instrumental and non-instrumental (deontological) justifications.", "author_claim_text": "Our main contribution to the debate on AI accountability is twofold: first, we consider non-instrumental arguments for accountability grounded in democratic theory; second, we distinguish also between direct public accountability via public transparency and indirect public accountability via transparency to auditors.", "source_hint": "Introduction §1"}, {"name": "Analysis of automation as delegation with three accountability challenges", "description": "The authors analyze AI automation as a delegation process from human agents to artificial agents, identifying three specific challenges (distributed responsibility, induced acceptance, and acceptance through ignorance) that prevent ideal delegation and threaten accountability elements like responsibility identification and answerability.", "author_claim_text": "We argue that the phenomena of distributed responsibility, induced acceptance, and acceptance through ignorance constitute instances of imperfect delegation", "source_hint": "Abstract and Section 2"}]}, "abs": "We argue that the phenomena of distributed responsibility, induced acceptance, and acceptance through ignorance constitute instances of imperfect delegation when tasks are delegated to computationally-driven systems. Imperfect delegation challenges human accountability. We hold that both direct public accountability via public transparency and indirect public accountability via transparency to auditors in public organizations can be both instrumentally ethically valuable and required as a matter of deontology from the principle of democratic self-government. We analyze the regulatory content of 16 guideline documents about the use of AI in the public sector, by mapping their requirements to those of our philosophical account of accountability, and conclude that while some guidelines refer to processes that amount to auditing, it seems that the debate would benefit from more clarity about the nature of the entitlement of auditors and the goals of auditing, also in order to develop ethically meaningful standards with respect to which different forms of auditing can be evaluated and compared."}, {"title": "Can Science and Technology Capacity be Measured?", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Meso Level/National Level/China/Can Science and Technology Capacity be Measured?.pdf", "summary": {"paper_id": "survey-2_paper-26_Can Science and Technology Capacity be Measured?", "core_task": {"text": "measuring science and technology capacity across countries", "query_variants": ["measuring science and technology capacity across countries", "assessing scientific and technological capabilities among nations", "evaluating research and innovation capacity in different countries"]}, "contributions": [{"name": "Lack of updated and accurate measurement of science and technology capacity across countries", "description": "The paper addresses the need to update and improve an existing index that measures science and technology capacity across nations. The original index required more recent data and improved accuracy to better assess countries' abilities to participate in the global knowledge economy.", "author_claim_text": "In an effort to assess the capacity of different countries in science and technology, this article updates a classification scheme developed by RAND to measure science and technology capacity for 150 countries of the world.", "source_hint": "Abstract"}, {"name": "Supporting policymaking for S&T capacity enhancement and innovation", "description": "The authors are motivated by the need to provide policymakers at international institutions with a tool to assess and enhance science and technology capacity. S&T capacity has been shown to correlate positively with economic growth, making it important for development policy.", "author_claim_text": "This index is designed as input to the policymaking process. A number of international institutions, including the World Bank and the United Nations, have policies that include the goal of enhancing S&T capacity or otherwise tapping capacity to encourage innovation.", "source_hint": "Introduction"}, {"name": "Three-step construction of composite S&T Capacity Index", "description": "The authors construct the index through a systematic three-step process: selecting appropriate variables, combining indicators by standardizing them to a common format and checking consistency, and validating results against alternative evidence. This ensures transparency and reproducibility.", "author_claim_text": "The STCI is constructed in three steps: Selecting variables based on an understanding of the process that determines the composite number. Combining the individual indicators into a single index by converting them to a common format, checking their correlation and consistency, weighting them into an aggregate number and testing different weighting schemes. Checking related but different variables to see if the outcome matches alternative evidence.", "source_hint": "Section on Constructing a composite index"}, {"name": "Eight quantitative indicators organized into three domains", "description": "The index uses eight indicators grouped into three domains: preconditions (environment for knowledge absorption), resources (direct S&T capacity measures like scientists, institutions, and R&D funding), and output (knowledge production and diffusion). This structure captures different aspects of S&T capacity.", "author_claim_text": "We have selected eight quantitative indicators and have divided them into three distinct domains of S&T capacity: preconditions that help create an environment conducive to the absorption, retention, production and diffusion of knowledge, resources of S&T activities, which concerns the indicators that relate most directly to S&T capacity, output of scientific and technological knowledge and its diffusion to the larger world.", "source_hint": "Section on Selection of S&T indicators"}]}, "abs": "The ability of a nation to participate in the global knowledge economy depends to some extent on its capacities in science and technology. In an effort to assess the capacity of different countries in science and technology, this article updates a classification scheme developed by RAND to measure science and technology capacity for 150 countries of the world."}, {"title": "In Oxford Handbook on AI Governance: The Role of Workers in AI Ethics and Governance", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Meso Level/National Level/China/In Oxford Handbook on AI Governance: The Role of Workers in AI Ethics and Governance.pdf", "summary": {"paper_id": "survey-2_paper-27_In Oxford Handbook on AI Governance: The Role of Workers in AI Ethics and Governance", "core_task": {"text": "role of workers in AI ethics and governance", "query_variants": ["role of workers in AI ethics and governance", "labor perspectives on artificial intelligence ethical oversight and regulation", "employee participation in AI accountability and policy frameworks"]}, "contributions": [{"name": "Limited attention to workers' role in AI governance", "description": "The paper addresses the gap that while states, corporations, and international organizations have been extensively studied in AI governance, the role of workers in identifying and mitigating AI harms has not been adequately examined.", "author_claim_text": "While the role of states, corporations, and international organizations in AI governance has been extensively theorized, the role of workers has received comparatively little attention.", "source_hint": "Abstract, paragraph 1"}, {"name": "Understanding worker activism in AI governance", "description": "The authors are motivated to understand how workers are positioned in AI workplaces and how they use collective action to stake claims on AI governance questions, particularly when there is disagreement about identifying or treating harms.", "author_claim_text": "Examining the past decade of AI-related worker activism allows us to understand how different types of workers are positioned within a workplace that produces AI systems, how their position informs their claims, and the place of collective action in staking their claims.", "source_hint": "Abstract, paragraph 2"}, {"name": "Model of harm reporting processes in AI workplaces", "description": "The authors construct a three-step model of harm reporting in AI workplaces (identification, governance decision, response) involving three stakeholders (management, AI workers, AI ethicists), drawing on archival data of 25 collective actions from 2010 to 2020.", "author_claim_text": "We theorize the role of workers within AI governance and construct a model of harm reporting processes in AI workplaces. The harm reporting process involves three steps: identification, the governance decision, and the response.", "source_hint": "Abstract, paragraph 2; Introduction §3"}, {"name": "Typology of worker claims to AI governance jurisdiction", "description": "The authors identify three types of claims workers use to assert jurisdiction over AI governance: subjection (being subject to harms), control over the product of one's labor, and proximate knowledge of systems.", "author_claim_text": "Workers draw upon three types of claims to argue for jurisdiction over questions of AI governance: subjection, control over the product of one's labor, and proximate knowledge of systems.", "source_hint": "Abstract, paragraph 2; Introduction §1"}]}, "abs": " While the role of states, corporations, and international organizations in AI governance has been extensively theorized, the role of workers has received comparatively little attention. This chapter looks at the role that workers play in identifying and mitigating harms from AI technologies. Harms are the causally assessed impacts of technologies. They arise despite technical reliability and are not a result of technical negligence but rather of normative uncertainty around questions of safety and fairness in complex social systems. There is high consensus in the AI ethics community on the benefits of reducing harms but less consensus on mechanisms for determining or addressing harms. This lack of consensus has resulted in a number of collective actions by workers protesting how harms are identified and addressed in their workplace. We theorize the role of workers within AI governance and construct a model of harm reporting processes in AI workplaces. The harm reporting process involves three steps, identification, the governance decision, and the response. Workers draw upon three types of claims to argue for jurisdiction over questions of AI governance, subjection, control over the product of labor, and proximate knowledge of systems. Examining the past decade of AI related worker activism allows us to understand how different types of workers are positioned within a workplace that produces AI systems, how their position informs their claims, and the place of collective action in staking their claims. This chapter argues that workers occupy a unique role in identifying and mitigating harms caused by AI systems."}, {"title": "The Artificial Intelligence Act: critical overview", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Meso Level/National Level/European Union/The Artificial Intelligence Act: critical overview.pdf", "summary": {"paper_id": "survey-2_paper-28_The Artificial Intelligence Act: critical overview", "core_task": {"text": "Regulation of artificial intelligence systems based on risk classification", "query_variants": ["Regulation of artificial intelligence systems based on risk classification", "Risk-based regulatory frameworks for AI systems", "AI governance through risk categorization approaches"]}, "contributions": [{"name": "Complexity and ambiguity in the AI Act's regulatory framework", "description": "The paper identifies that the AI Act, despite aiming to regulate artificial intelligence responsibly, suffers from excessive complexity, unclear definitions, and overlapping rules that may hinder its effectiveness and the innovation it seeks to promote.", "author_claim_text": "the approach is so complex that it risks defeating its own purpose of promoting responsible innovation within the European Union and beyond its borders", "source_hint": "Abstract, final sentence"}, {"name": "Need for critical assessment of the AI Act's adequacy and balance", "description": "The author is motivated to evaluate whether the overall framework of the AI Act is adequate and balanced, given the rapid technological evolution of AI and the numerous legal and ethical challenges it poses across multiple domains of law.", "author_claim_text": "This article provides a critical overview of the recently approved Artificial Intelligence Act", "source_hint": "Abstract, opening sentence; Introduction"}, {"name": "Structured analysis of the AI Act's key components", "description": "The paper employs a systematic method of dissecting the AI Act by examining its structure, objectives, definitions, scope, principles, prohibited practices, high-risk system rules, transparency obligations, general-purpose models, and certification/supervision mechanisms.", "author_claim_text": "It starts by presenting the main structure, objectives, and approach of Regulation (EU) 2024/1689. A definition of key concepts follows, and then the material and territorial scope, as well as the timing of application, are analyzed", "source_hint": "Abstract, second and third sentences"}, {"name": "Adequate but overly complex regulatory framework", "description": "The author concludes that while the AI Act's framework is conceptually sound and balanced in addressing AI risks, its excessive complexity and intricate rules may undermine its goal of fostering responsible innovation both within and beyond the EU.", "author_claim_text": "even if the overall framework can be deemed adequate and balanced, the approach is so complex that it risks defeating its own purpose of promoting responsible innovation", "source_hint": "Abstract, final sentence; Conclusion section"}]}, "abs": "This article provides a critical overview of the recently approved Artificial Intelligence Act. It starts by presenting the main structure, objectives, and approach of Regulation (EU) 2024/1689. A definition of key concepts follows, and then the material and territorial scope, as well as the timing of application, are analyzed. Although the Regulation does not explicitly set out principles, the main ideas of fairness, accountability, transparency, and equity in AI underly a set of rules of the regulation. This is discussed before looking at the ill-defined set of forbidden AI practices (manipulation and e exploitation of vulnerabilities, social scoring, biometric identification and classification, and predictive policing). It is highlighted that those rules deal with behaviors rather than AI systems. The qualification and regulation of high-risk AI systems are tackled, alongside the obligation of transparency for certain systems, the regulation of general-purpose models, and the rules on certification, supervision, and sanctions. The text concludes that even if the overall framework can be deemed adequate and balanced, the approach is so complex that it risks defeating its own purpose of promoting responsible innovation within the European Union and beyond its borders."}, {"title": "Unpacking the Ethical Value Alignment in Big Models", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Meso Level/National Level/European Union/Unpacking the Ethical Value Alignment in Big Models.pdf", "summary": {"paper_id": "survey-2_paper-29_Unpacking the Ethical Value Alignment in Big Models", "core_task": {"text": "ethical value alignment in big models", "query_variants": ["ethical value alignment in big models", "aligning large language models with ethical values", "value alignment for large-scale models with ethical considerations"]}, "contributions": [{"name": "Ethical risks and value misalignment in big models", "description": "The paper addresses the challenge that big models, despite their powerful capabilities, carry inherent ethical values and biases that can lead to societal harms such as discrimination, misinformation, privacy violations, and malicious use, posing unforeseen risks as they integrate into daily life.", "author_claim_text": "as these models become increasingly integrated into everyday life, their inherent ethical values and potential biases pose unforeseen risks to society", "source_hint": "Abstract"}, {"name": "Need for ethical governance of powerful AI systems", "description": "The authors are motivated by the urgency to ensure that big models, which possess human-level intelligence and unprecedented capabilities, are ethically governed and responsibly employed to prevent societal harms and to benefit humanity's sustainable development.", "author_claim_text": "it's crucial to not only expand their capabilities but also address the associated social risks. Researchers and developers should proactively prioritize minimizing these models' negative implications, adhere to responsible development principles, and align them with intrinsic human values", "source_hint": "Introduction, final paragraph"}, {"name": "AI Categorical Imperative framework for reviewing ethical guidelines", "description": "The authors propose a normative ethics-based framework, the AI Categorical Imperative, to systematically review and assess existing AI ethical guidelines. This framework uses two formulations to determine which propositions should become universal moral maxims for AI, emphasizing universality, absolute necessity, consensus, and anthropocentrism.", "author_claim_text": "we propose a reassessment of recent normative guidelines... we propose to review existing guidelines from the perspective of Normative Ethics... we also provide two formulations for AI Categorical Imperative", "source_hint": "Abstract; Section 2.3"}, {"name": "Moral Foundations Theory-based evaluation of LLMs", "description": "The authors employ the Moral Foundations Theory, which includes five moral foundations (Care, Fairness, Loyalty, Authority, Sanctity), to empirically assess the ethical value tendencies of mainstream large language models through questionnaire-based evaluation.", "author_claim_text": "we investigate the moral inclinations of current mainstream LLMs using the Moral Foundation theory", "source_hint": "Abstract; Section 3.2"}]}, "abs": "Big models have greatly advanced AI's ability to understand, generate, and manipulate information and content, enabling numerous applications. However, as these models become increasingly integrated into everyday life, their inherent ethical values and potential biases pose unforeseen risks to society. This paper provides an overview of the risks and challenges associated with big models, surveys existing AI ethics guidelines, and examines the ethical implications arising from the limitations of these models. Taking a normative ethics perspective, we propose a reassessment of recent normative guidelines, highlighting the importance of collaborative efforts in academia to establish a unified and universal AI ethics framework. Furthermore, we investigate the moral inclinations of current mainstream LLMs using the Moral Foundation theory, analyze existing alignment algorithms, and outline the unique challenges encountered in aligning ethical values within them. To address these challenges, we introduce a novel conceptual paradigm for aligning the ethical values of big models and discuss promising research directions for alignment criteria, evaluation, and method, representing an initial step towards the interdisciplinary construction of the ethically aligned AI\nThis paper is a modified English version of our Chinese paper this https URL, intended to help non-Chinese native speakers better understand our work."}, {"title": "DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Meso Level/National Level/European Union/DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker.pdf", "summary": {"paper_id": "survey-2_paper-30_DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker", "core_task": {"text": "imperfect information game playing using deep learning and continual re-solving", "query_variants": ["imperfect information game playing using deep learning and continual re-solving", "deep learning methods for continual re-solving in partial observability games", "neural network approaches to iterative re-solving under incomplete information game settings"]}, "contributions": [{"name": "Imperfect information game reasoning in large-scale settings", "description": "The paper addresses the challenge of developing AI that can reason effectively in large-scale imperfect information games, where players have asymmetric information about the game state. Prior approaches using abstraction have shown massive flaws and fail to achieve expert-level performance.", "author_claim_text": "Poker is the quintessential game of imperfect information, and a longstanding challenge problem in artificial intelligence.", "source_hint": "Abstract"}, {"name": "Bridging the gap between perfect and imperfect information game AI", "description": "The authors are motivated by the need to develop AI that can handle real-world scenarios involving information asymmetry, deception, and recursive reasoning about opponents' beliefs. Mastering imperfect information games like poker represents a fundamental step toward AI applicable to realistic decision-making problems.", "author_claim_text": "Real life consists of bluffing, of little tactics of deception, of asking yourself what is the other man going to think I mean to do. And that is what games are about in my theory.", "source_hint": "Introduction paragraph 2"}, {"name": "Continual re-solving with depth-limited lookahead", "description": "DeepStack computes strategies during play only for states that actually arise, using continual re-solving that maintains the agent's range and opponent counterfactual values. It limits lookahead depth by substituting deeper computation with learned value function estimates, avoiding the need to reason about the entire game.", "author_claim_text": "DeepStack takes a fundamentally different approach. It continues to use the recursive reasoning of CFR to handle information asymmetry. However, it does not compute and store a complete strategy prior to play and so has no need for explicit abstraction.", "source_hint": "Introduction paragraph 5"}, {"name": "Deep counterfactual value networks as intuition", "description": "The algorithm uses deep neural networks trained via self-play on randomly generated poker situations to approximate counterfactual values beyond the lookahead depth. These networks take as input the public state and player ranges, outputting value estimates for each possible hand, serving as a form of learned intuition.", "author_claim_text": "DeepStack's intuition, much like human intuition, needs to be trained. We train it with deep learning using examples generated from random poker situations.", "source_hint": "Introduction paragraph 5"}]}, "abs": "Artificial intelligence has seen several breakthroughs in recent years, with games often serving as milestones. A common feature of these games is that players have perfect information. Poker is the quintessential game of imperfect information, and a longstanding challenge problem in artificial intelligence. We introduce DeepStack, an algorithm for imperfect information settings. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition that is automatically learned from self-play using deep learning. In a study involving 44,000 hands of poker, DeepStack defeated with statistical significance professional poker players in heads-up no-limit Texas hold'em. The approach is theoretically sound and is shown to produce more difficult to exploit strategies than prior approaches."}, {"title": "Response to NITRD, NCO, NSF Request for Information on \"Update to the 2016 National Artificial Intelligence Research and Development Strategic Plan\"", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Meso Level/National Level/The United States/Response to NITRD, NCO, NSF Request for Information on \"Update to the 2016 National Artificial Intelligence Research and Development Strategic Plan\".pdf", "summary": {"paper_id": "survey-2_paper-31_Response to NITRD, NCO, NSF Request for Information on \"Update to the 2016 National Artificial Intelligence Research and Development Strategic Plan\"", "core_task": {"text": "Application of artificial intelligence methods to high energy physics research", "query_variants": ["Application of artificial intelligence methods to high energy physics research", "Machine learning techniques applied to particle physics experiments", "AI approaches for analyzing high energy particle collision data"]}, "contributions": [{"name": "Limitations of traditional data analysis for large-scale HEP data", "description": "The paper identifies that current traditional algorithms and models cannot adequately handle the enormous scale and complexity of modern high-energy physics datasets, necessitating new approaches for data analysis and discovery.", "author_claim_text": "the scale and complexity of modern data sets are exceeding the capacity of traditional data analysis algorithms and models", "source_hint": "Introduction, Section 2.1"}, {"name": "Maintaining US leadership in AI and physical sciences through HEP applications", "description": "The authors are motivated by the need to preserve and extend US leadership in both AI technologies and physical sciences by leveraging HEP's unique capabilities and infrastructure to advance AI research and applications.", "author_claim_text": "Thus far the US has been a leader in AI technologies, and we believe as a national Laboratory it is crucial to help maintain and extend this leadership. Moreover, investments in AI will be important for maintaining US leadership in the physical sciences.", "source_hint": "Introduction, paragraph 1"}, {"name": "Integration of statistical models with AI algorithms for bias reduction", "description": "The authors propose combining traditional statistical modeling approaches with AI algorithms to enable bias estimation and removal while producing uncertainty measures that have clear statistical interpretations, addressing interpretability challenges in scientific AI.", "author_claim_text": "we must integrate statistical models with AI algorithms. This will permit us to estimate and remove bias, and to produce statistically interpretable measures of uncertainty.", "source_hint": "Section 2.1.1"}, {"name": "Matching neural network symmetries to physical problem symmetries", "description": "The paper proposes designing neural network architectures that incorporate the same mathematical symmetries (rotational, spherical, etc.) present in the physical systems being studied, enabling AI to better capture underlying physics from data.", "author_claim_text": "By matching the symmetry of the neural network architecture to the symmetry of the physical problem, we have a new opportunity for AI to learn the underlying physics in nature from data.", "source_hint": "Section 2.1.1"}]}, "abs": "We present a response to the 2018 Request for Information (RFI) from the NITRD, NCO, NSF regarding the \"Update to the 2016 National Artificial Intelligence Research and Development Strategic Plan.\" Through this document, we provide a response to the question of whether and how the National Artificial Intelligence Research and Development Strategic Plan (NAIRDSP) should be updated from the perspective of Fermilab, America's premier national laboratory for High Energy Physics (HEP). We believe the NAIRDSP should be extended in light of the rapid pace of development and innovation in the field of Artificial Intelligence (AI) since 2016, and present our recommendations below. AI has profoundly impacted many areas of human life, promising to dramatically reshape society --- e.g., economy, education, science --- in the coming years. We are still early in this process. It is critical to invest now in this technology to ensure it is safe and deployed ethically. Science and society both have a strong need for accuracy, efficiency, transparency, and accountability in algorithms, making investments in scientific AI particularly valuable. Thus far the US has been a leader in AI technologies, and we believe as a national Laboratory it is crucial to help maintain and extend this leadership. Moreover, investments in AI will be important for maintaining US leadership in the physical sciences."}, {"title": "Who Followed the Blueprint? Analyzing the Responses of U.S. Federal Agencies to the Blueprint for an AI Bill of Rights", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Meso Level/National Level/The United States/Who Followed the Blueprint? Analyzing the Responses of U.S. Federal Agencies to the Blueprint for an AI Bill of Rights.pdf", "summary": {"paper_id": "survey-2_paper-32_Who Followed the Blueprint? Analyzing the Responses of U.S. Federal Agencies to the Blueprint for an AI Bill of Rights", "core_task": {"text": "Analyzing federal agency responses to AI governance principles", "query_variants": ["Analyzing federal agency responses to AI governance principles", "Examining how federal agencies implement AI governance frameworks", "Evaluating government agency compliance with artificial intelligence governance standards"]}, "contributions": [{"name": "Unclear impact of the Blueprint for an AI Bill of Rights on federal agencies", "description": "The paper addresses the lack of systematic understanding about whether and how U.S. federal agencies responded to the White House's non-binding Blueprint for an AI Bill of Rights after its October 2022 release.", "author_claim_text": "In October 2023, a year after the Blueprint's publication, we began to examine the extent to which Biden's agencies heeded its call.", "source_hint": "Introduction, paragraph 4"}, {"name": "Assessing federal government adherence to ethical AI governance framework", "description": "The authors are motivated by the importance of understanding whether federal departments followed the President's vision for ethical AI governance, given the government's significant impact on citizens' rights and access to services.", "author_claim_text": "No other entity in American life has a greater impact on its citizens than the U.S. government.", "source_hint": "Introduction, paragraph 3"}, {"name": "Analysis of publicly accessible records across 15 federal departments", "description": "The authors conducted an exploratory study examining public documents from 15 U.S. federal departments to identify explicit mentions of the Blueprint and actions aligned with its five core principles after October 2022.", "author_claim_text": "we compare whether and how executive branch departments have engaged with the Blueprint, through an analysis of their publicly accessible records.", "source_hint": "Introduction, paragraph 4"}, {"name": "Blueprint had minimal impact on federal AI governance", "description": "The authors conclude that the non-binding Blueprint did not significantly shape federal agencies' approach to ethical AI governance, with other factors like executive orders and public concerns likely carrying more influence.", "author_claim_text": "Overall, the Blueprint seems to have had a limited to non-existent impact across federal departments.", "source_hint": "Conclusion, paragraph 1"}]}, "abs": "This study examines the extent to which U.S. federal agencies responded to and implemented the principles outlined in the White House's October 2022 \"Blueprint for an AI Bill of Rights.\" The Blueprint provided a framework for the ethical governance of artificial intelligence systems, organized around five core principles: safety and effectiveness, protection against algorithmic discrimination, data privacy, notice and explanation about AI systems, and human alternatives and fallback.\nThrough an analysis of publicly available records across 15 federal departments, the authors found limited evidence that the Blueprint directly influenced agency actions after its release. Only five departments explicitly mentioned the Blueprint, while 12 took steps aligned with one or more of its principles. However, much of this work appeared to have precedents predating the Blueprint or motivations disconnected from it, such as compliance with prior executive orders on trustworthy AI. Departments' activities often emphasized priorities like safety, accountability and transparency that overlapped with Blueprint principles, but did not necessarily stem from it.\nThe authors conclude that the non-binding Blueprint seems to have had minimal impact on shaping the U.S. government's approach to ethical AI governance in its first year. Factors like public concerns after high-profile AI releases and obligations to follow direct executive orders likely carried more influence over federal agencies. More rigorous study would be needed to definitively assess the Blueprint's effects within the federal bureaucracy and broader society."}, {"title": "LawLuo: A Multi-Agent Collaborative Framework for Multi-Round Chinese Legal Consultation", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Micro Level/Legal Consultation/LawLuo: A Multi-Agent Collaborative Framework for Multi-Round Chinese Legal Consultation.pdf", "summary": {"paper_id": "survey-2_paper-33_LawLuo: A Multi-Agent Collaborative Framework for Multi-Round Chinese Legal Consultation", "core_task": {"text": "multi-agent collaborative framework for multi-turn Chinese legal consultation", "query_variants": ["multi-agent collaborative framework for multi-turn Chinese legal consultation", "collaborative multi-agent system for multi-step legal consultation in Chinese", "multi-agent cooperation framework for iterative Chinese legal advisory dialogue"]}, "contributions": [{"name": "Limitations of single-agent legal consultation systems", "description": "Existing Chinese legal LLMs use single-agent architectures that fail to replicate the collaborative workflows of real law firms, limiting their ability to provide personalized and professional responses to users.", "author_claim_text": "most existing Chinese legal consultation models are based on single-agent systems, which differ from real-world legal consultations, where multiple professionals collaborate to offer more tailored responses", "source_hint": "Abstract"}, {"name": "Simulating real law firm operations for better legal advice", "description": "The authors aim to replicate the collaborative processes of real law firms, where multiple professionals work together, to deliver more personalized and professional legal consultation services to users.", "author_claim_text": "To better simulate real consultations, we propose LawLuo, a multi-agent framework for multi-turn Chinese legal consultations", "source_hint": "Abstract"}, {"name": "LawLuo multi-agent framework with four specialized agents", "description": "LawLuo comprises four agents: a receptionist that assesses user intent and assigns a specialized lawyer, a lawyer that engages in multi-turn dialogue, a secretary that organizes records and generates reports, and a boss that monitors and optimizes agent performance.", "author_claim_text": "LawLuo includes four agents: the receptionist agent, which assesses user intent and selects a lawyer agent; the lawyer agent, which interacts with the user; the secretary agent, which organizes conversation records and generates consultation reports; and the boss agent, which evaluates the performance", "source_hint": "Abstract"}, {"name": "Three distinct fine-tuning datasets for agent training", "description": "The authors constructed three specialized datasets: (Inquire, Lawyer description) pairs for the receptionist, a MUlti Rounds LEgal Dialogue (MURLED) dataset for the lawyer, and a Legal Consultation Report Generation (LCRG) dataset for the secretary, to enable each agent to follow domain-specific legal instructions.", "author_claim_text": "To train them to follow different legal instructions, we developed distinct fine-tuning datasets", "source_hint": "Abstract and Introduction"}]}, "abs": "Legal Large Language Models (LLMs) have shown promise in providing legal consultations to non-experts. However, most existing Chinese legal consultation models are based on single-agent systems, which differ from real-world legal consultations, where multiple professionals collaborate to offer more tailored responses. To better simulate real consultations, we propose LawLuo, a multi-agent framework for multi-turn Chinese legal consultations. LawLuo includes four agents: the receptionist agent, which assesses user intent and selects a lawyer agent; the lawyer agent, which interacts with the user; the secretary agent, which organizes conversation records and generates consultation reports; and the boss agent, which evaluates the performance of the lawyer and secretary agents to ensure optimal results. These agents' interactions mimic the operations of real law firms. To train them to follow different legal instructions, we developed distinct fine-tuning datasets. We also introduce a case graph-based RAG to help the lawyer agent address vague user inputs. Experimental results show that LawLuo outperforms baselines in generating more personalized and professional responses, handling ambiguous queries, and following legal instructions in multi-turn conversations. Our full code and constructed datasets will be open-sourced upon paper acceptance."}, {"title": "Operationalising AI governance through ethics-based auditing: An industry case study", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Micro Level/Pharmaceutical company governance/Operationalising AI governance through ethics-based auditing: An industry case study.pdf", "summary": {"paper_id": "survey-2_paper-34_Operationalising AI governance through ethics-based auditing: An industry case study", "core_task": {"text": "operationalising AI governance through ethics-based auditing", "query_variants": ["operationalising AI governance through ethics-based auditing", "implementing AI governance via ethics-driven audit processes", "deploying AI governance using ethical auditing frameworks"]}, "contributions": [{"name": "Gap between AI ethics principles and practice", "description": "The paper addresses the lack of empirical evidence on how ethics-based auditing procedures can be feasibly and effectively implemented in real-world organizational contexts to bridge the gap between abstract AI ethics principles and operational practice.", "author_claim_text": "important aspects of EBA – such as the feasibility and effectiveness of different auditing procedures – have yet to be substantiated by empirical research", "source_hint": "Abstract"}, {"name": "Need for empirical validation of AI governance mechanisms", "description": "The authors are motivated by the observation that while theoretical justifications for ethics-based auditing exist, there is insufficient understanding of how organizations actually implement EBA and what practical challenges they encounter, necessitating empirical case studies.", "author_claim_text": "the affordances and constraints of EBA procedures can only be investigated and evaluated in applied contexts", "source_hint": "Introduction §1"}, {"name": "Longitudinal industry case study using participant observation", "description": "The authors conducted a 12-month longitudinal case study of AstraZeneca, employing participant observation and semi-structured interviews to examine how the company prepared for and underwent an ethics-based AI audit, focusing on organizational processes and governance structures.", "author_claim_text": "Over 12 months, we observed and analysed the internal activities of AstraZeneca, a biopharmaceutical company, as it prepared for and underwent an ethics-based AI audit", "source_hint": "Abstract and Section 5"}, {"name": "Qualitative analysis combining participant observation and interviews", "description": "The research methodology combined direct participation in organizational meetings and activities with 18 semi-structured interviews across different stakeholder groups, using NVivo for coding and a parallel research design to triangulate findings.", "author_claim_text": "we leveraged two qualitative research methods: participant observation and semi-structured interviews", "source_hint": "Section 5"}]}, "abs": "Ethics based auditing (EBA) is a structured process whereby an entitys past or present behaviour is assessed for consistency with moral principles or norms. Recently, EBA has attracted much attention as a governance mechanism that may bridge the gap between principles and practice in AI ethics. However, important aspects of EBA (such as the feasibility and effectiveness of different auditing procedures) have yet to be substantiated by empirical research. In this article, we address this knowledge gap by providing insights from a longitudinal industry case study. Over 12 months, we observed and analysed the internal activities of AstraZeneca, a biopharmaceutical company, as it prepared for and underwent an ethics-based AI audit. While previous literature concerning EBA has focused on proposing evaluation metrics or visualisation techniques, our findings suggest that the main difficulties large multinational organisations face when conducting EBA mirror classical governance challenges. These include ensuring harmonised standards across decentralised organisations, demarcating the scope of the audit, driving internal communication and change management, and measuring actual outcomes. The case study presented in this article contributes to the existing literature by providing a detailed description of the organisational context in which EBA procedures must be integrated to be feasible and effective."}, {"title": "A Machine Learning Approach for Recruitment Prediction in Clinical Trial Design", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Micro Level/Recruitment/A Machine Learning Approach for Recruitment Prediction in Clinical Trial Design.pdf", "summary": {"paper_id": "survey-2_paper-35_A Machine Learning Approach for Recruitment Prediction in Clinical Trial Design", "core_task": {"text": "patient recruitment prediction in clinical trial design", "query_variants": ["patient recruitment prediction in clinical trial design", "forecasting participant enrollment for clinical study planning", "predicting subject accrual in clinical trial protocols"]}, "contributions": [{"name": "Imprecise patient recruitment prediction in clinical trial design", "description": "The paper addresses the challenge of accurately predicting patient enrollment in clinical trials during the design stage. Current methods lack precision and granularity, leading to recruitment delays and trial failures despite recent improvements in enrollment optimization.", "author_claim_text": "improved methods for patient recruitment prediction are needed to support trial site selection and to estimate appropriate enrollment timelines in the trial design stage", "source_hint": "Abstract"}, {"name": "Reducing trial recruitment delays and failures", "description": "The authors are motivated by the need to improve trial success rates by providing more accurate enrollment predictions. While recent efforts have improved enrollment timelines, studies suggest that better prediction methods are still needed to minimize recruitment-related risks.", "author_claim_text": "more precise and granular prediction of patient recruitment in the trial design stage is needed to further reduce the risk of trial recruitment delays and failures", "source_hint": "Introduction paragraph 1"}, {"name": "Data-driven machine learning approach for site-level monthly enrollment prediction", "description": "The authors develop a machine learning framework that predicts monthly patient enrollment counts at individual trial sites throughout the enrollment duration. The approach uses historical enrollment records from thousands of trials and incorporates study, country, site, and time-varying features to account for temporal variations and site characteristics.", "author_claim_text": "we propose a data driven, machine learning (ML) approach to predict site enrollment counts for every month over the course of a prospective clinical trial while accounting for variations in site initiation times, seasonality, and other site and study characteristics", "source_hint": "Introduction paragraph 3"}, {"name": "Feature engineering from historical trial operational data", "description": "The authors create approximately 245 features across multiple levels (study, country, site, and site-month) by extracting performance metrics from relevant historical trials. These features include enrollment rates, disease prevalence estimates, and temporal indicators, with hierarchical imputation strategies for missing values.", "author_claim_text": "We generate features at the study, country, and site levels, as well as time-varying features at the site-month level, using similar methods to capture performance from relevant historical trials", "source_hint": "Methods Section 3.2"}]}, "abs": "Significant advancements have been made in recent years to optimize patient recruitment for clinical trials, however, improved methods for patient recruitment prediction are needed to support trial site selection and to estimate appropriate enrollment timelines in the trial design stage. In this paper, using data from thousands of historical clinical trials, we explore machine learning methods to predict the number of patients enrolled per month at a clinical trial site over the course of a trial's enrollment duration. We show that these methods can reduce the error that is observed with current industry standards and propose opportunities for further improvement."}, {"title": "Analytical modelling of period spacings across the HR diagram", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Micro Level/Recruitment/Analytical modelling of period spacings across the HR diagram.pdf", "summary": {"paper_id": "survey-2_paper-36_Analytical modelling of period spacings across the HR diagram", "core_task": {"text": "Analytical modelling of period spacings in stellar oscillations", "query_variants": ["Analytical modelling of period spacings in stellar oscillations", "Theoretical models for oscillation period spacing patterns in stars", "Mathematical framework for stellar pulsation period interval analysis"]}, "contributions": [{"name": "Inadequate analytical expressions for period spacing with structural glitches", "description": "Previous analytical expressions for period spacing variations caused by buoyancy glitches were limited to small-glitch approximations producing sinusoidal variations, or required interpolation procedures, failing to capture the non-sinusoidal variations seen in models with strong glitches.", "author_claim_text": "no explicit expression for the period spacing variation was presented by these authors, except for the case of the small glitch limit, when the variation is sinusoidal", "source_hint": "Introduction, paragraph discussing prior work on buoyancy glitches"}, {"name": "Characterizing stellar cores through asteroseismic data", "description": "Understanding stellar cores via asteroseismology can reveal physical processes like nuclear burning, diffusion, and mixing. Accurate modeling of period spacings is essential to infer core properties and detect strong chemical gradients (buoyancy glitches).", "author_claim_text": "The characterisation of stellar cores may be accomplished through the modelling of asteroseismic data from stars exhibiting either gravity-mode or mixed-mode pulsations, potentially shedding light on the physical processes responsible for the production, mixing, and segregation of chemical elements.", "source_hint": "Abstract, first sentence"}, {"name": "Asymptotically-based analytical expression for period spacing with arbitrary-strength glitches", "description": "The authors derive and validate an analytical expression for period spacing that does not assume small glitches, allowing it to predict non-sinusoidal variations. The expression incorporates both mode coupling and structural glitches modeled by step or Gaussian-like functions.", "author_claim_text": "This asymptotically-based analytical expression is tested both in models with and without buoyancy glitches. It does not assume that glitches are small and, consequently, predicts non-sinusoidal glitch-induced period-spacing variations", "source_hint": "Abstract, third and fourth sentences"}, {"name": "Modeling glitches with step and Gaussian-like functions", "description": "The authors extend previous work by modeling buoyancy glitches using step functions and Gaussian-like functions, which better represent the variety of glitch shapes found in stellar models compared to Dirac delta approximations.", "author_claim_text": "Here we shall present, in addition, formulations for the cases of glitches modelled either by a step function or by a gaussian-like function, which describe more adequately the types of structural variations that are seen in the stellar models", "source_hint": "Section 2, paragraph introducing glitch modeling"}]}, "abs": "The characterisation of stellar cores may be accomplished through the modelling of asteroseismic data from stars exhibiting either gravity-mode or mixed-mode pulsations, potentially shedding light on the physical processes responsible for the production, mixing, and segregation of chemical elements. In this work we validate against model data an analytical expression for the period spacing that will facilitate the inference of the properties of stellar cores, including the detection and characterisation of buoyancy glitches (strong chemical gradients). This asymptotically-based analytical expression is tested both in models with and without buoyancy glitches. It does not assume that glitches are small and, consequently, predicts non-sinusoidal glitch-induced period-spacing variations, as often seen in model and real data. We show that the glitch position and width inferred from the fitting of the analytical expression to model data consisting of pure gravity modes are in close agreement (typically better than 7$\\%$ relative difference) with the properties measured directly from the stellar models. In the case of fitting mixed-mode model data, the same expression is shown to reproduce well the numerical results, when the glitch properties are known a priori. In addition, the fits performed to mixed-mode model data reveal a frequency dependence of the coupling coefficient, $q$, for a moderate-luminosity red-giant-branch model star. Finally, we find that fitting the analytical expression to the mixed-mode period spacings may provide a way to infer the frequencies of the pure acoustic dipole modes that would exist if no coupling took place between acoustic and gravity waves."}, {"title": "Ethics of generative AI and manipulation: a design-oriented research agenda", "path": "/mnt/data/zyh_survey_data/Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives/The principles of values alignment/Micro Level/Recruitment/Ethics of generative AI and manipulation: a design-oriented research agenda.pdf", "summary": {"paper_id": "survey-2_paper-37_Ethics of generative AI and manipulation: a design-oriented research agenda", "core_task": {"text": "ethics of generative AI and manipulation", "query_variants": ["ethics of generative AI and manipulation", "ethical concerns of generative models and deceptive practices", "moral implications of generative artificial intelligence and deception"]}, "contributions": [{"name": "Inadequate investigation of manipulation risks in generative AI", "description": "The paper identifies a gap in current research: while generative AI enables automated, effective manipulation at scale, the specific ethical risks related to manipulation have not been sufficiently studied or conceptualized.", "author_claim_text": "Despite the growing general ethical discussion around generative AI, the specific manipulation risks remain inadequately investigated.", "source_hint": "Abstract"}, {"name": "Manipulation as a fundamental threat to trustworthy AI and democracy", "description": "The authors are motivated by the legal and moral imperative to address manipulation, given its identification as a disvalue in AI regulation (e.g., EU AI Act) and its threat to democratic values and trustworthy AI systems.", "author_claim_text": "manipulation is considered a threat to democracy and trustworthiness, which means that it is a fundamental threat to the critical aim of responsible, trustworthy AI", "source_hint": "Introduction paragraph 4"}, {"name": "Design-for-values approach to non-manipulation", "description": "The paper proposes a design-for-values framework that systematically addresses manipulation through conceptual, empirical, and design stages, emphasizing that appropriate conceptualization of manipulation is critical for deriving concrete design requirements.", "author_claim_text": "this article discusses a research agenda studying manipulation in generative AI. I argue that good research on manipulation and generative AI depends significantly on our conceptualisation of manipulation.", "source_hint": "Introduction paragraph 5 and Design for values section"}, {"name": "Indifference criterion for identifying manipulation", "description": "The paper advocates for the indifference criterion as a more appropriate conceptualization of manipulation than alternatives like hidden influence or bypassing rationality. This criterion identifies manipulation based on the influencer's indifference to revealing reasons, rather than malicious intent.", "author_claim_text": "manipulation is an influence that aims to be effective but is not explained by the aim to reveal reasons to the interlocutor", "source_hint": "The indifference criterion section"}]}, "abs": "Generative AI enables automated, effective manipulation at scale. Despite the growing general ethical discussion around generative AI, the specific manipulation risks remain inadequately investigated. This article outlines essential inquiries encompassing conceptual, empirical, and design dimensions of manipulation, pivotal for comprehending and curbing manipulation risks. By highlighting these questions, the article underscores the necessity of an appropriate conceptualisation of manipulation to ensure the responsible development of Generative AI technologies."}], "survey_topic_path": "Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives", "survey_topic": "Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives", "gt_paper_count": 38, "gt": {"name": "Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives", "subtopics": [{"name": "The principles of values alignment", "subtopics": [{"name": "Macro Level", "subtopics": [{"name": "Moral Foundation", "subtopics": [{"name": "Beneficence", "papers": ["Artificial Intelligence: the global landscape of ethics guidelines", "The five Is: Key principles for interpretable and safe conversational AI"]}, {"name": "Dignity", "papers": ["Artificial Intelligence Governance and Ethics: Global Perspectives"]}, {"name": "Honesty", "papers": ["Improving Factuality and Reasoning in Language Models through Multiagent Debate", "From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models"]}, {"name": "Justice & Fairness", "papers": ["Achieving Fairness in Multi-Agent Markov Decision Processes Using Reinforcement Learning", "Cultural Bias and Cultural Alignment of Large Language Models", "Ethics of AI: A Systematic Literature Review of Principles and Challenges", "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset"]}, {"name": "Virtue", "papers": ["Value Compass Benchmarks: A Platform for Fundamental and Validated Evaluation of LLMs Values", "DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life"]}]}, {"name": "Rights Protection", "subtopics": [{"name": "Privacy", "papers": ["HydraGAN A Multi-head, Multi-objective Approach to Synthetic Data Generation", "Empowering Users in Digital Privacy Management through Interactive LLM-Based Agents"]}]}, {"name": "Sustainability", "subtopics": [{"name": "Sustainability", "papers": ["Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents"]}]}, {"name": "System Governance", "subtopics": [{"name": "Harmlessness", "papers": ["Training Socially Aligned Language Models on Simulated Social Interactions", "SafeSora: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset"]}, {"name": "Transparency", "papers": ["A Multimodal Automated Interpretability Agent", "Situation-Aware Approach to Improve Context-based Recommender System", "BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments"]}]}]}, {"name": "Meso Level", "subtopics": [{"name": "Cultural Level", "subtopics": [{"name": "Individualism", "subtopics": [{"name": "Collectivism", "papers": ["LLM-GLOBE: A Benchmark Evaluating the Cultural Values Embedded in LLM Output", "Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions", "CDEval: A Benchmark for Measuring the Cultural Dimensions of Large Language Models", "How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions"]}]}]}, {"name": "Industry Level", "subtopics": [{"name": "Education", "papers": ["The Utility of Large Language Models and Generative AI for Education Research"]}, {"name": "Gaming", "papers": ["The Ethics of AI in Games"]}]}, {"name": "National Level", "subtopics": [{"name": "China", "papers": ["Towards Accountability in the Use of Artificial Intelligence for Public Administrations", "Can Science and Technology Capacity be Measured?", "In Oxford Handbook on AI Governance: The Role of Workers in AI Ethics and Governance"]}, {"name": "European Union", "papers": ["The Artificial Intelligence Act: critical overview", "Unpacking the Ethical Value Alignment in Big Models", "DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker"]}, {"name": "The United States", "papers": ["Response to NITRD, NCO, NSF Request for Information on \"Update to the 2016 National Artificial Intelligence Research and Development Strategic Plan\"", "Who Followed the Blueprint? Analyzing the Responses of U.S. Federal Agencies to the Blueprint for an AI Bill of Rights"]}]}]}, {"name": "Micro Level", "subtopics": [{"name": "Legal Consultation", "papers": ["LawLuo: A Multi-Agent Collaborative Framework for Multi-Round Chinese Legal Consultation"]}, {"name": "Pharmaceutical company governance", "papers": ["Operationalising AI governance through ethics-based auditing: An industry case study"]}, {"name": "Recruitment", "papers": ["A Machine Learning Approach for Recruitment Prediction in Clinical Trial Design", "Analytical modelling of period spacings across the HR diagram", "Ethics of generative AI and manipulation: a design-oriented research agenda"]}]}]}]}, "input_paper_count": 38, "input_content": "SYSTEM PROMPT:\nYou are a senior researcher and survey-author with deep experience in structuring\nhigh-quality academic survey papers.\n\nYour task is to organize a set of research papers into a **hierarchical topic tree**\n(bottom-up), given:\n- a survey topic,\n- for each paper: title, abstract, and an structured summary\n  (which may include research problem, motivation, methodology, and findings).\n\nYour goal is NOT just to cluster by surface similarity, but to produce a taxonomy\nthat would be considered **reasonable, informative, and defensible** in a top-tier survey paper.\n\n### Hard Constraints\n1. Output must be **strictly valid JSON**.\n2. Only leaf nodes may contain `\"papers\"`; all internal nodes must contain `\"subtopics\"`.\n3. **Every paper must appear exactly once** in the entire tree.\n4. NO duplicate papers anywhere.\n5. The tree must eventually merge into **one single root node**.\n\n### CLASSIFICATION RULES\n- Group papers by semantic similarity using both title + abstract.\n- Create meaningful names for leaf-level themes.\n\n### ANTI-DUPLICATION PROCEDURE (MANDATORY)\nBefore constructing the tree:\n1. Produce an internal list of all given paper titles.\n2. Assign each paper to exactly one leaf node.\n3. After assignment, verify that:\n   - the number of assigned papers equals the number of input papers,\n   - no paper appears in more than one group.\n\n### Output Format\nUse a JSON structure like this (replace placeholders with actual paper titles) and\nthe output you produce MUST be wrapped inside a fenced code block:\n\n```json\n{\n  \"name\": \"AI Research\",\n  \"subtopics\": [\n    {\n      \"name\": \"NLP\",\n      \"subtopics\": [\n        {\n          \"name\": \"Text Summarization\",\n          \"papers\": [\"<actual paper titles>\"]\n        },\n        {\n          \"name\": \"Machine Translation\",\n          \"papers\": [\"<actual paper titles>\"]\n        }\n      ]\n    }\n  ]\n}```\n\nUSER PROMPT:\nPerform a bottom-up hierarchical clustering of the following 38 papers and produce a JSON research topic tree.\n\nSurvey Topic: Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives\n\nPaper List:\nPaper 1:\n  Title: Artificial Intelligence: the global landscape of ethics guidelines\n  Abstract: In the last five years, private companies, research institutions as well as public sector organisations have issued principles and guidelines for ethical AI, yet there is debate about both what constitutes \"ethical AI\" and which ethical requirements, technical standards and best practices are needed for its realization. To investigate whether a global agreement on these questions is emerging, we mapped and analyzed the current corpus of principles and guidelines on ethical AI. Our results reveal a global convergence emerging around five ethical principles (transparency, justice and fairness, non-maleficence, responsibility and privacy), with substantive divergence in relation to how these principles are interpreted; why they are deemed important; what issue, domain or actors they pertain to; and how they should be implemented. Our findings highlight the importance of integrating guideline-development efforts with substantive ethical analysis and adequate implementation strategies.\n  Structured Summary:\n    - Lack of global consensus on ethical AI principles and implementation: The paper addresses the challenge that despite widespread efforts to develop ethical AI guidelines, there is ongoing disagreement about the definition of ethical AI and the specific requirements, standards, and practices necessary to achieve it.\n    - Need to determine if global convergence on ethical AI is emerging: The authors are motivated to understand whether the diverse stakeholders issuing AI ethics guidelines are converging toward shared principles and implementation strategies, which is critical for developing a coherent global agenda for ethical AI.\n    - Scoping review of gray literature on ethical AI guidelines: The authors performed a systematic scoping review using a multi-stage screening strategy involving link hubs, keyword-based web searches, and citation chaining to identify 84 policy documents containing ethical AI principles and guidelines.\n    - Inductive content analysis and thematic coding: The authors applied qualitative content analysis methods, including inductive coding and code mapping, to systematically categorize and interpret ethical principles and recommendations across the corpus of documents.\n\nPaper 2:\n  Title: The five Is: Key principles for interpretable and safe conversational AI\n  Abstract: In this position paper, we present five key principles, namely interpretability, inherent capability to explain, independent data, interactive learning, and inquisitiveness, for the development of conversational AI that, unlike the currently popular black box approaches, is transparent and accountable. At present, there is a growing concern with the use of black box statistical language models: While displaying impressive average performance, such systems are also prone to occasional spectacular failures, for which there is no clear remedy. In an effort to initiate a discussion on possible alternatives, we outline and exemplify how our five principles enable the development of conversational AI systems that are transparent and thus safer for use. We also present some of the challenges inherent in the implementation of those principles.\n  Structured Summary:\n    - Black box conversational AI systems lack transparency and safety: Current conversational AI systems based on deep neural networks are opaque and prone to spectacular failures, making them unsafe for high-stakes applications. These black box systems require massive training data that may contain unintended biases and sensitive content that is nearly impossible to quality-check.\n    - Need for transparent and accountable conversational AI: The authors are motivated by the lack of alternative approaches to black box systems for building conversational AI that is transparent and accountable by design, especially given rising legal trends favoring transparency and end-user preferences for accountable systems.\n    - Five design principles for interpretable conversational AI: The authors propose five key principles (the 'five Is'): interpretability using human-understandable primitives, independent separation of procedural and declarative memory, inherent capability to explain reasoning, interactive learning with human tutors, and inquisitiveness to actively seek information while checking with users.\n    - Pipeline model with interpretable cognitive processing: The authors propose a pipeline architecture where input processing, cognitive processing, and output processing are clearly separated. The cognitive processing step uses generic, interpretable primitives (like sorting, fetching, comparing) that can be combined to perform complex operations while remaining human-readable.\n\nPaper 3:\n  Title: Artificial Intelligence Governance and Ethics: Global Perspectives\n  Abstract: Artificial intelligence (AI) is a technology which is increasingly being utilised in society and the economy worldwide, and its implementation is planned to become more prevalent in coming years. AI is increasingly being embedded in our lives, supplementing our pervasive use of digital technologies. But this is being accompanied by disquiet over problematic and dangerous implementations of AI, or indeed, even AI itself deciding to do dangerous and problematic actions, especially in fields such as the military, medicine and criminal justice. These developments have led to concerns about whether and how AI systems adhere, and will adhere to ethical standards. These concerns have stimulated a global conversation on AI ethics, and have resulted in various actors from different countries and sectors issuing ethics and governance initiatives and guidelines for AI. Such developments form the basis for our research in this report, combining our international and interdisciplinary expertise to give an insight into what is happening in Australia, China, Europe, India and the US.\n  Structured Summary:\n    - Fragmented global approaches to AI governance and ethics: The paper addresses the challenge of understanding and comparing how different major jurisdictions and international bodies are approaching AI governance and ethics, given the fragmented and rapidly evolving landscape of AI regulation and ethical frameworks worldwide.\n    - Need for comparative understanding of AI governance approaches: The authors are motivated by the need to map and understand the diverse approaches to AI governance and ethics emerging globally, recognizing that AI's impact crosses borders and requires awareness of different regulatory and ethical frameworks being developed simultaneously.\n    - Comparative jurisdictional analysis of AI governance frameworks: The authors conduct a systematic comparative analysis examining AI governance and ethics initiatives across multiple levels: global organizations, regional bodies (EU, Council of Europe), and individual countries (Germany, Austria, UK, India, China, USA, Australia), documenting their respective approaches, principles, and frameworks.\n    - Significant variations exist in AI governance approaches globally: The report identifies both commonalities and significant differences in how various jurisdictions approach AI governance and ethics, revealing a complex landscape where competition and collaboration coexist, and where questions of implementation, enforceability, and stakeholder involvement remain critical challenges.\n\nPaper 4:\n  Title: Improving Factuality and Reasoning in Language Models through Multiagent Debate\n  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such \"society of minds\" approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.\n  Structured Summary:\n    - Hallucinations and reasoning errors in large language models: The paper addresses the problem that contemporary large language models frequently generate factually incorrect information and make flawed reasoning steps, despite appearing confident in their outputs.\n    - Improving factual accuracy and reasoning beyond single-model approaches: The authors are motivated to develop a complementary method to existing single-model prompting techniques by leveraging multi-agent interactions, inspired by how multiple perspectives and cross-examination can improve reasoning and factuality.\n    - Multiagent debate framework for language generation: The core method involves multiple instances of a language model independently generating candidate answers, then iteratively critiquing and refining each other's responses through a debate process over several rounds until converging on a consensus answer.\n    - Consensus prompting mechanism: A prompting technique where agents receive concatenated responses from other agents and are instructed to revise their own answers based on this collective information, enabling iterative refinement through debate.\n\nPaper 5:\n  Title: From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models\n  Abstract: Big models, exemplified by Large Language Models (LLMs), are models typically pre-trained on massive data and comprised of enormous parameters, which not only obtain significantly improved performance across diverse tasks but also present emergent capabilities absent in smaller models. However, the growing intertwining of big models with everyday human lives poses potential risks and might cause serious social harm. Therefore, many efforts have been made to align LLMs with humans to make them better follow user instructions and satisfy human preferences. Nevertheless, `what to align with' has not been fully discussed, and inappropriate alignment goals might even backfire. In this paper, we conduct a comprehensive survey of different alignment goals in existing work and trace their evolution paths to help identify the most essential goal. Particularly, we investigate related works from two perspectives: the definition of alignment goals and alignment evaluation. Our analysis encompasses three distinct levels of alignment goals and reveals a goal transformation from fundamental abilities to value orientation, indicating the potential of intrinsic human values as the alignment goal for enhanced LLMs. Based on such results, we further discuss the challenges of achieving such intrinsic value alignment and provide a collection of available resources for future research on the alignment of big models.\n  Structured Summary:\n    - Lack of clarity on appropriate alignment goals for big models: The paper identifies that while many efforts have been made to align large language models with humans, there is insufficient discussion about what the most appropriate and essential alignment goal should be, and choosing wrong goals could have negative consequences.\n    - Growing risks and social harm from big models: The authors are motivated by the increasing integration of big models into daily life and the associated risks, including emergent problems and inverse scaling laws where some risks become more serious with larger model sizes.\n    - Comprehensive survey of alignment goals across three levels: The authors systematically review and categorize existing alignment work into three distinct levels: human instructions, human preferences, and human values, analyzing each from the perspectives of goal definition and evaluation methods.\n    - Taxonomy of alignment goals and evaluation methods: The paper organizes existing research into a structured taxonomy covering how alignment goals are defined and represented for training, and how alignment performance is evaluated through benchmarks and methods.\n\nPaper 6:\n  Title: Achieving Fairness in Multi-Agent Markov Decision Processes Using Reinforcement Learning\n  Abstract: Fairness plays a crucial role in various multi-agent systems (e.g., communication networks, financial markets, etc.). Many multi-agent dynamical interactions can be cast as Markov Decision Processes (MDPs). While existing research has focused on studying fairness in known environments, the exploration of fairness in such systems for unknown environments remains open. In this paper, we propose a Reinforcement Learning (RL) approach to achieve fairness in multi-agent finite-horizon episodic MDPs. Instead of maximizing the sum of individual agents' value functions, we introduce a fairness function that ensures equitable rewards across agents. Since the classical Bellman's equation does not hold when the sum of individual value functions is not maximized, we cannot use traditional approaches. Instead, in order to explore, we maintain a confidence bound of the unknown environment and then propose an online convex optimization based approach to obtain a policy constrained to this confidence region. We show that such an approach achieves sub-linear regret in terms of the number of episodes. Additionally, we provide a probably approximately correct (PAC) guarantee based on the obtained regret bound. We also propose an offline RL algorithm and bound the optimality gap with respect to the optimal fair solution. To mitigate computational complexity, we introduce a policy-gradient type method for the fair objective. Simulation experiments also demonstrate the efficacy of our approach.\n  Structured Summary:\n    - Achieving fairness in multi-agent MDPs with unknown environments: The paper addresses the challenge of achieving fairness among multiple agents in Markov Decision Processes when the environment parameters (transition probabilities and rewards) are unknown, requiring a reinforcement learning approach rather than traditional planning methods.\n    - Ensuring equitable treatment across agents in sequential decision-making: The authors are motivated by real-world scenarios where maximizing aggregate rewards may lead to unfair outcomes for individual agents, such as in wireless networks or job scheduling, necessitating fairness-aware policies that balance returns across all agents.\n    - Optimism-based convex optimization using occupancy measures: The authors propose an algorithm that constructs confidence intervals for unknown MDP parameters and formulates a convex optimization problem over state-action occupancy measures with optimistic reward and transition estimates to encourage exploration while ensuring fairness.\n    - Pessimistic offline RL algorithm for fairness: For offline settings where new data cannot be collected, the authors develop a pessimistic approach that constructs an MDP with lower-bound rewards based on available data and solves a convex optimization to obtain a fair policy with theoretical guarantees on suboptimality.\n\nPaper 7:\n  Title: Cultural Bias and Cultural Alignment of Large Language Models\n  Abstract: Culture fundamentally shapes people's reasoning, behavior, and communication. As people increasingly use generative artificial intelligence (AI) to expedite and automate personal and professional tasks, cultural values embedded in AI models may bias people's authentic expression and contribute to the dominance of certain cultures. We conduct a disaggregated evaluation of cultural bias for five widely used large language models (OpenAI's GPT-4o/4-turbo/4/3.5-turbo/3) by comparing the models' responses to nationally representative survey data. All models exhibit cultural values resembling English-speaking and Protestant European countries. We test cultural prompting as a control strategy to increase cultural alignment for each country/territory. For recent models (GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models' output for 71-81% of countries and territories. We suggest using cultural prompting and ongoing evaluation to reduce cultural bias in the output of generative AI.\n  Structured Summary:\n    - Cultural bias in large language models: Large language models exhibit cultural bias by overrepresenting certain cultural values, particularly Western values, due to training on corpora that overrepresent certain parts of the world. This bias can influence how people communicate and express themselves when using these models.\n    - Impact of AI on cultural reproduction and authentic expression: The widespread adoption of LLMs in daily communication and professional tasks raises concerns that embedded cultural values may bias users' authentic expression and contribute to cultural dominance, affecting how language is produced and transmitted across generations.\n    - Disaggregated evaluation using World Values Survey benchmark: The authors systematically evaluate cultural bias across 107 countries and territories by prompting five GPT models with the same ten questions from the Integrated Values Surveys and mapping responses onto the Inglehart-Welzel Cultural Map to quantify cultural distance from ground truth survey data.\n    - Cultural prompting as control strategy: The authors propose and test cultural prompting, which instructs the LLM to respond like a person from a specific country or territory, as a flexible and accessible approach to improve cultural alignment. They evaluate this across ten prompt variants to ensure robustness.\n\nPaper 8:\n  Title: Ethics of AI: A Systematic Literature Review of Principles and Challenges\n  Abstract: Ethics in AI becomes a global topic of interest for both policymakers and academic researchers. In the last few years, various research organizations, lawyers, think tankers and regulatory bodies get involved in developing AI ethics guidelines and principles. However, there is still debate about the implications of these principles. We conducted a systematic literature review (SLR) study to investigate the agreement on the significance of AI principles and identify the challenging factors that could negatively impact the adoption of AI ethics principles. The results reveal that the global convergence set consists of 22 ethical principles and 15 challenges. Transparency, privacy, accountability and fairness are identified as the most common AI ethics principles. Similarly, lack of ethical knowledge and vague principles are reported as the significant challenges for considering ethics in AI. The findings of this study are the preliminary inputs for proposing a maturity model that assess the ethical capabilities of AI systems and provide best practices for further improvements.\n  Structured Summary:\n    - Lack of clarity on AI ethics principles and adoption challenges: Despite various organizations developing AI ethics guidelines and principles, there remains significant debate and uncertainty about how these principles should be interpreted and implemented in practice, creating a gap between ethical guidelines and their real-world adoption.\n    - Need for systematic understanding of AI ethics for maturity model development: The authors aim to systematically identify agreed-upon AI ethics principles and adoption challenges to provide foundational inputs for developing a maturity model that can evaluate ethical capabilities of AI systems and guide improvements in practice.\n    - Systematic literature review following Kitchenham guidelines: The authors conducted a systematic literature review following Kitchenham and Charters guidelines, searching five major digital libraries and applying inclusion/exclusion criteria to select 27 primary studies, which were then analyzed to extract AI ethics principles and challenges.\n    - Four core principles dominate AI ethics discourse: The review reveals that among 22 identified ethical principles, transparency, privacy, accountability, and fairness emerge as the most frequently cited and significant principles that should guide AI system design and operation.\n\nPaper 9:\n  Title: BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset\n  Abstract: In this paper, we introduce the BeaverTails dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have gathered safety meta-labels for 333,963 question-answer (QA) pairs and 361,903 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our project page is available at the following URL: this https URL.\n  Structured Summary:\n    - Lack of comprehensive human feedback data for safety alignment in LLMs: The paper addresses the challenge that existing safety alignment techniques for large language models require extensive human feedback data, which is expensive and difficult to obtain at scale. Current datasets do not adequately separate helpfulness from harmlessness annotations.\n    - Ensuring safe deployment of LLMs to prevent societal harm: The authors are motivated by the urgent need to align LLMs with human values and safety standards to prevent harmful outputs such as misinformation, offensive content, and unintended dangerous responses that could negatively impact society.\n    - Two-stage annotation process for QA pairs: The authors introduce a two-stage annotation methodology where QA pairs are first classified across 14 harm categories to assign safety meta-labels, then responses are separately ranked for helpfulness and harmlessness. This process assesses risk neutralization rather than individual utterance toxicity.\n    - Decoupled human preference annotations for helpfulness and harmlessness: The dataset uniquely separates human preference annotations into two independent dimensions: helpfulness (how well a response addresses a prompt) and harmlessness (risk neutrality across harm categories). This allows for independent optimization of both attributes during model training.\n\nPaper 10:\n  Title: Value Compass Benchmarks: A Platform for Fundamental and Validated Evaluation of LLMs Values\n  Abstract: As Large Language Models (LLMs) achieve remarkable breakthroughs, aligning their values with humans has become imperative for their responsible development and customized applications. However, there still lack evaluations of LLMs values that fulfill three desirable goals. (1) Value Clarification: We expect to clarify the underlying values of LLMs precisely and comprehensively, while current evaluations focus narrowly on safety risks such as bias and toxicity. (2) Evaluation Validity: Existing static, open-source benchmarks are prone to data contamination and quickly become obsolete as LLMs evolve. Additionally, these discriminative evaluations uncover LLMs' knowledge about values, rather than valid assessments of LLMs' behavioral conformity to values. (3) Value Pluralism: The pluralistic nature of human values across individuals and cultures is largely ignored in measuring LLMs value alignment. To address these challenges, we presents the Value Compass Benchmarks, with three correspondingly designed modules. It (i) grounds the evaluation on motivationally distinct \\textit{basic values to clarify LLMs' underlying values from a holistic view; (ii) applies a \\textit{generative evolving evaluation framework with adaptive test items for evolving LLMs and direct value recognition from behaviors in realistic scenarios; (iii) propose a metric that quantifies LLMs alignment with a specific value as a weighted sum over multiple dimensions, with weights determined by pluralistic values.\n  Structured Summary:\n    - Evaluation validity and result interpretation challenges in LLM value assessment: Existing value evaluation benchmarks suffer from two main issues: they fail to accurately measure LLMs' true value conformity (measuring knowledge rather than behavior, producing indistinguishable results), and they oversimplify pluralistic values into single scores that hinder meaningful comparison and interpretation.\n    - Need for comprehensive value assessment as LLMs integrate into society: The authors are motivated by the growing integration of LLMs into human society and the need to assess their values to understand potential risks, harmful content generation, and alignment with diverse cultural preferences beyond predefined risk categories.\n    - Generative self-evolving evaluation paradigm: The platform employs a novel evaluation approach that automatically generates value-evoking test items through an adaptive generator optimized to maximize informativeness and value elicitation, and dynamically updates items as LLMs evolve, assessing value conformity through generative behavior rather than discriminative knowledge.\n    - Multiple basic value systems framework: The benchmarks incorporate four distinct value systems covering 27 dimensions: two established theories from social science (Schwartz Basic Values and Moral Foundation Theory) and two designed for LLMs (LLMs' Unique Value System and Safety Taxonomy), providing a holistic view of value orientations.\n\nPaper 11:\n  Title: DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life\n  Abstract: As users increasingly seek guidance from LLMs for decision-making in daily life, many of these decisions are not clear-cut and depend significantly on the personal values and ethical standards of people. We present DailyDilemmas, a dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma presents two possible actions, along with affected parties and relevant human values for each action. Based on these dilemmas, we gather a repository of human values covering diverse everyday topics, such as interpersonal relationships, workplace, and environmental issues. With DailyDilemmas, we evaluate LLMs on these dilemmas to determine what action they will choose and the values represented by these action choices. Then, we analyze values through the lens of five theoretical frameworks inspired by sociology, psychology, and philosophy, including the World Values Survey, Moral Foundations Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and Plutchik's Wheel of Emotions. For instance, we find LLMs are most aligned with self-expression over survival in World Values Survey and care over loyalty in Moral Foundations Theory. Interestingly, we find substantial preference differences in models for some core values. For example, for truthfulness, Mixtral-8x7B neglects it by 9.7% while GPT-4-turbo selects it by 9.4%. We also study the recent guidance released by OpenAI (ModelSpec), and Anthropic (Constitutional AI) to understand how their designated principles reflect their models' actual value prioritization when facing nuanced moral reasoning in daily-life settings. Finally, we find that end users cannot effectively steer such prioritization using system prompts.\n  Structured Summary:\n    - Value conflicts in LLM decision-making for daily life dilemmas: The paper addresses the challenge that LLMs face when providing guidance on everyday decisions that involve moral dilemmas without clear-cut answers, where outcomes depend on personal values and ethical standards rather than universally accepted norms.\n    - Understanding LLM value prioritization in realistic moral scenarios: The authors are motivated by the need to understand how AI systems prioritize competing values when facing complex real-world dilemmas, moving beyond simple clear-cut scenarios to ensure better alignment with diverse human preferences in practical deployment contexts.\n    - DAILY DILEMMAS dataset of 1,360 moral dilemmas: The authors introduce a synthetically generated dataset using GPT-4 containing 1,360 moral dilemmas from daily life. Each dilemma includes a background, conflict point, binary action choices, negative consequences for each action, affected parties, and associated human values mapped to five theoretical frameworks.\n    - Mapping values to five theoretical frameworks: The authors map the 301 human values extracted from dilemmas onto five established theoretical frameworks: World Values Survey, Moral Foundations Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and Plutchik's Wheel of Emotions, enabling systematic analysis of LLM value preferences.\n\nPaper 12:\n  Title: HydraGAN A Multi-head, Multi-objective Approach to Synthetic Data Generation\n  Abstract: Synthetic data generation overcomes limitations of real-world machine learning. Traditional methods are valuable for augmenting costly datasets but only optimize one criterion: realism. In this paper, we tackle the problem of generating synthetic data that optimize multiple criteria. This goal is necessary when real data are replaced by synthetic for privacy preservation. We introduce HydraGAN, a new approach to synthetic data generation that introduces multiple generator and discriminator agents into the system. The multi-agent GAN optimizes the goal of privacy-preservation as well as data realism. To facilitate multi-agent training, we adapt game-theoretic principles to offer equilibrium guarantees. We observe that HydraGAN outperforms baseline methods for three datasets for multiple criteria of maximizing data realism, maximizing model accuracy, and minimizing re-identification risk.\n  Structured Summary:\n    - Multi-objective synthetic data generation for privacy preservation: Existing synthetic data generation methods optimize only data realism, but when real data must be replaced by synthetic data for privacy preservation, multiple competing criteria (realism, model accuracy, privacy) must be simultaneously optimized. This creates a fundamental challenge in balancing the privacy-utility trade-off.\n    - Privacy-preserving data sharing while maintaining utility: The rapid increase in personal data collection from IoT devices creates a need to share data for collaboration and research while protecting sensitive information. Existing privacy-preserving methods face a trade-off where increased privacy reduces data utility and model accuracy, motivating a method that can optimize both simultaneously.\n    - Multi-agent GAN with privacy-preserving discriminator: HydraGAN introduces a novel multi-agent GAN architecture that includes both a realism discriminator and a privacy-preserving re-identification discriminator. The re-identification discriminator actively attempts to identify sensitive attributes, training the generator to produce data resistant to such attacks through adversarial learning rather than static noise addition.\n    - Multi-headed generator for data diversity: To prevent mode collapse, HydraGAN partitions real data into clusters using k-means and creates a generator with multiple output heads. Each head originates from shared layers and is responsible for simulating one cluster, simplifying the loss landscape and allowing poorly performing heads to be corrected through updates from better performing ones.\n\nPaper 13:\n  Title: Empowering Users in Digital Privacy Management through Interactive LLM-Based Agents\n  Abstract: This paper presents a novel application of large language models (LLMs) to enhance user comprehension of privacy policies through an interactive dialogue agent. We demonstrate that LLMs significantly outperform traditional models in tasks like Data Practice Identification, Choice Identification, Policy Summarization, and Privacy Question Answering, setting new benchmarks in privacy policy analysis. Building on these findings, we introduce an innovative LLM-based agent that functions as an expert system for processing website privacy policies, guiding users through complex legal language without requiring them to pose specific questions. A user study with 100 participants showed that users assisted by the agent had higher comprehension levels (mean score of 2.6 out of 3 vs. 1.8 in the control group), reduced cognitive load (task difficulty ratings of 3.2 out of 10 vs. 7.8), increased confidence in managing privacy, and completed tasks in less time (5.5 minutes vs. 15.8 minutes). This work highlights the potential of LLM-based agents to transform user interaction with privacy policies, leading to more informed consent and empowering users in the digital services landscape.\n  Structured Summary:\n    - User comprehension gap in privacy policies: The paper addresses the challenge that users cannot effectively understand privacy policies due to complex legal language, length, and density, which prevents informed consent and exposes users to privacy risks while complicating regulatory compliance efforts.\n    - Empowering users through improved privacy policy interaction: The authors are motivated by the need to enable users to make informed decisions about their personal data by bridging the comprehension gap in privacy policies, particularly as regulations become more stringent and policies more intricate.\n    - LLM benchmarking on privacy policy tasks: The authors systematically evaluate large language models (GPT-4o to GPT-3.5) on four key privacy policy tasks using zero-shot and few-shot approaches, establishing new performance benchmarks that demonstrate LLMs significantly outperform traditional models.\n    - Interactive LLM-based agent with heuristic interaction model: The authors develop an AI agent based on GPT-4o-mini that functions as an expert system, autonomously processing privacy policies, identifying key information, and guiding users through complex legal language without requiring them to pose specific questions.\n\nPaper 14:\n  Title: Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents\n  Abstract: As AI systems pervade human life, ensuring that large language models (LLMs) make safe decisions remains a significant challenge. We introduce the Governance of the Commons Simulation (GovSim), a generative simulation platform designed to study strategic interactions and cooperative decision-making in LLMs. In GovSim, a society of AI agents must collectively balance exploiting a common resource with sustaining it for future use. This environment enables the study of how ethical considerations, strategic planning, and negotiation skills impact cooperative outcomes. We develop an LLM-based agent architecture and test it with the leading open and closed LLMs. We find that all but the most powerful LLM agents fail to achieve a sustainable equilibrium in GovSim, with the highest survival rate below 54%. Ablations reveal that successful multi-agent communication between agents is critical for achieving cooperation in these cases. Furthermore, our analyses show that the failure to achieve sustainable cooperation in most LLMs stems from their inability to formulate and analyze hypotheses about the long-term effects of their actions on the equilibrium of the group. Finally, we show that agents that leverage \"Universalization\"-based reasoning, a theory of moral thinking, are able to achieve significantly better sustainability. Taken together, GovSim enables us to study the mechanisms that underlie sustainable self-government with specificity and scale. We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface.\n  Structured Summary:\n    - LLMs fail to achieve sustainable cooperation in multi-agent resource-sharing scenarios: Current large language models struggle to maintain sustainable cooperation when managing shared resources in multi-agent settings, with most models unable to prevent resource collapse through coordinated decision-making and negotiation.\n    - Need for safe and cooperative AI in multi-agent contexts: As LLMs increasingly take on complex decision-making roles in multi-agent contexts, understanding their ability to cooperate safely and reliably becomes critical, especially since cooperation challenges mirror those faced by humans in real-world scenarios.\n    - GOVSIM simulation platform: A novel simulation environment where AI agents must collectively balance exploiting a common resource with sustaining it for future use, enabling study of ethical considerations, strategic planning, and negotiation skills in multi-agent resource-sharing scenarios inspired by game-theoretic research on cooperation.\n    - LLM-based generative agent architecture: An agent architecture based on the generative agents framework that enables LLMs to operate in phase-based environments, making decisions about resource extraction and engaging in open-ended discussion with other agents.\n\nPaper 15:\n  Title: Training Socially Aligned Language Models on Simulated Social Interactions\n  Abstract: Social alignment in AI systems aims to ensure that these models behave according to established societal values. However, unlike humans, who derive consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly replicate their training corpus in isolation, leading to subpar generalization in unfamiliar scenarios and vulnerability to adversarial attacks. This work presents a novel training paradigm that permits LMs to learn from simulated social interactions. In comparison to existing methodologies, our approach is considerably more scalable and efficient, demonstrating superior performance in alignment benchmarks and human evaluations. This paradigm shift in the training of LMs brings us a step closer to developing AI systems that can robustly and accurately reflect societal norms and values.\n  Structured Summary:\n    - Subpar generalization and vulnerability to adversarial attacks in current language models: Existing language models memorize training data in isolation without social interaction, resulting in poor generalization to new scenarios and susceptibility to adversarial prompts like jailbreaking.\n    - Achieving robust social alignment through simulated social interactions: The authors aim to enable language models to learn social norms and values through simulated social interactions, mirroring how humans acquire societal behaviors, rather than through rigid memorization or proxy reward models.\n    - SandBox: a simulated society for modeling social interactions: SandBox is an open-source platform where LM-based agents interact in a simulated society, guided by a latent rule incentive. Agents generate responses, receive collective feedback, and iteratively revise their outputs, producing fine-grained interaction data including ratings, feedback, and revisions.\n    - Stable Alignment: a three-stage alignment learning framework: Stable Alignment trains language models in three stages: Imitation Learning (learning from high-rated responses), Self-Critic (learning to generate feedback), and Realignment (learning to revise misaligned responses). It uses Contrastive Preference Optimization (CPO) to directly optimize policies without an additional reward model.\n\nPaper 16:\n  Title: SafeSora: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset\n  Abstract: To mitigate the risk of harmful outputs from large vision models (LVMs), we introduce the SafeSora dataset to promote research on aligning text-to-video generation with human values. This dataset encompasses human preferences in text-to-video generation tasks along two primary dimensions: helpfulness and harmlessness. To capture in-depth human preferences and facilitate structured reasoning by crowdworkers, we subdivide helpfulness into 4 sub-dimensions and harmlessness into 12 sub-categories, serving as the basis for pilot annotations. The SafeSora dataset includes 14,711 unique prompts, 57,333 unique videos generated by 4 distinct LVMs, and 51,691 pairs of preference annotations labeled by humans. We further demonstrate the utility of the SafeSora dataset through several applications, including training the text-video moderation model and aligning LVMs with human preference by fine-tuning a prompt augmentation module or the diffusion model. These applications highlight its potential as the foundation for text-to-video alignment research, such as human preference modeling and the development and validation of alignment algorithms.\n  Structured Summary:\n    - Lack of datasets for text-to-video alignment research: The paper identifies a significant gap in the text-to-video generation field: while text-to-text alignment is well-supported by extensive research and datasets, the text-to-video domain lacks datasets for studying human value alignment and safety.\n    - Mitigating risks of harmful outputs from large vision models: The authors are motivated by concerns that misalignment of AI systems with human values could lead to severe consequences, especially as capabilities expand to video generation. They aim to address potential misuse of powerful text-to-video models like Sora.\n    - Two-stage human annotation process with decoupled dimensions: The authors design a structured annotation process that separates helpfulness and harmlessness into parallel dimensions. The first heuristic stage guides annotators through sub-dimensions (4 for helpfulness, 12 harm categories for harmlessness), followed by overall preference judgments in the second stage.\n    - Prompt augmentation and multi-model video generation pipeline: The authors employ a video generation pipeline that uses both original and LLM-refined prompts to generate multiple videos per prompt using four distinct large vision models (both open-source and closed-source), creating diverse text-video pairs for annotation.\n\nPaper 17:\n  Title: A Multimodal Automated Interpretability Agent\n  Abstract: This paper describes MAIA, a Multimodal Automated Interpretability Agent. MAIA is a system that uses neural models to automate neural model understanding tasks like feature interpretation and failure mode discovery. It equips a pre-trained vision-language model with a set of tools that support iterative experimentation on subcomponents of other models to explain their behavior. These include tools commonly used by human interpretability researchers: for synthesizing and editing inputs, computing maximally activating exemplars from real-world datasets, and summarizing and describing experimental results. Interpretability experiments proposed by MAIA compose these tools to describe and explain system behavior. We evaluate applications of MAIA to computer vision models. We first characterize MAIA's ability to describe (neuron-level) features in learned representations of images. Across several trained models and a novel dataset of synthetic vision neurons with paired ground-truth descriptions, MAIA produces descriptions comparable to those generated by expert human experimenters. We then show that MAIA can aid in two additional interpretability tasks: reducing sensitivity to spurious features, and automatically identifying inputs likely to be mis-classified.\n  Structured Summary:\n    - Expensive and slow neural model understanding: Understanding neural models (e.g., identifying when classifiers rely on sensitive features, discovering systematic errors, or improving robustness) currently demands substantial manual effort from researchers, making it slow and costly even for widely deployed systems.\n    - Combining flexibility of human experimentation with scalability of automation: The authors aim to develop tools that preserve the flexible, hypothesis-driven nature of human interpretability research while achieving the scalability of automated methods, addressing limitations of current automated interpretability approaches that are low-precision and useful mainly for hypothesis generation.\n    - MAIA: Multimodal Automated Interpretability Agent: MAIA is a system that uses a pretrained vision-language model (GPT-4V) equipped with an API of interpretability tools (for synthesizing/editing inputs, computing maximally activating exemplars, and summarizing results) to autonomously design and execute iterative experiments on neural network subcomponents, composing these tools into Python programs to explain system behavior.\n    - System and Tools API for modular experimentation: The MAIA API consists of a System class that instruments target models and makes subcomponents callable, and a Tools class providing functions for dataset exemplar generation, image synthesis/editing (via Stable Diffusion and Instruct-Pix2Pix), image description/summarization, and experiment logging, enabling modular composition of interpretability procedures.\n\nPaper 18:\n  Title: Situation-Aware Approach to Improve Context-based Recommender System\n  Abstract: In this paper, we introduce a novel situation aware approach to improve a context based recommender system. To build situation aware user profiles, we rely on evidence issued from retrieval situations. A retrieval situation refers to the social spatio temporal context of the user when he interacts with the recommender system. A situation is represented as a combination of social spatio temporal concepts inferred from ontological knowledge given social group, location and time information. User's interests are inferred from past user's interaction with the recommender system related to the identified situations. They are represented using concepts issued from a domain ontology. We also propose a method to dynamically adapt the system to the user's interest's evolution.\n  Structured Summary:\n    - Static user profiles in mobile recommender systems fail to adapt to dynamic contextual changes: Existing recommender systems for mobile users rely on static user profiles based solely on computational behavior, ignoring the user's surrounding environment (location, time, nearby people) and failing to adapt to changing user interests across different situations.\n    - Mobile users' information needs depend on contextual factors requiring situation-aware recommendations: The authors are motivated by evidence that mobile users' information needs are strongly tied to contextual factors including interests, social groups, location, and time. This necessitates recommender systems that can provide information tailored to individual users and adapted to their current situation.\n    - Situation-aware user profile representation using social-spatiotemporal ontologies: The authors propose representing user situations as combinations of semantic concepts extracted from social, spatial, and temporal ontologies, abstracting low-level sensor data (GPS coordinates, time, agenda) into meaningful semantic information characterizing the user's interaction context.\n    - Dynamic user profile modeling with situation-specific preferences: The method learns user preferences for each identified situation based on past user interactions within that situation. The user model consists of situations paired with corresponding preferences, where preferences are built from navigation activities and documents the user engaged with in specific contexts.\n\nPaper 19:\n  Title: BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments\n  Abstract: Agents based on large language models have shown great potential in accelerating scientific discovery by leveraging their rich background knowledge and reasoning capabilities. In this paper, we introduce BioDiscoveryAgent, an agent that designs new experiments, reasons about their outcomes, and efficiently navigates the hypothesis space to reach desired solutions. We demonstrate our agent on the problem of designing genetic perturbation experiments, where the aim is to find a small subset out of many possible genes that, when perturbed, result in a specific phenotype (e.g., cell growth). Utilizing its biological knowledge, BioDiscoveryAgent can uniquely design new experiments without the need to train a machine learning model or explicitly design an acquisition function as in Bayesian optimization. Moreover, BioDiscoveryAgent, using Claude 3.5 Sonnet, achieves an average of 21% improvement in predicting relevant genetic perturbations across six datasets, and a 46% improvement in the harder task of non-essential gene perturbation, compared to existing Bayesian optimization baselines specifically trained for this task. Our evaluation includes one dataset that is unpublished, ensuring it is not part of the language model's training data. Additionally, BioDiscoveryAgent predicts gene combinations to perturb more than twice as accurately as a random baseline, a task so far not explored in the context of closed-loop experiment design. The agent also has access to tools for searching the biomedical literature, executing code to analyze biological datasets, and prompting another agent to critically evaluate its predictions. Overall, BioDiscoveryAgent is interpretable at every stage, representing an accessible new paradigm in the computational design of biological experiments with the potential to augment scientists' efficacy.\n  Structured Summary:\n    - Inefficient design of genetic perturbation experiments: The paper addresses the challenge of efficiently identifying a small subset of genes from thousands of candidates that, when perturbed, produce a desired phenotype. Existing approaches require training bespoke machine learning models that are difficult to interpret and cannot leverage biological prior knowledge from scientific literature.\n    - Leveraging LLM biological knowledge for experiment design: The authors are motivated by the potential of large language models to leverage vast biological knowledge from scientific literature for closed-loop experiment design, which has not been explored before. This could overcome limitations of conventional Bayesian optimization methods that cannot access such prior knowledge and are difficult to interpret.\n    - BioDiscoveryAgent framework: The authors propose an AI agent that uses large language models to design genetic perturbation experiments iteratively. At each round, the agent receives a prompt describing the experimental setup and previous results, then predicts genes to perturb next using structured reasoning (Reflection, Research Plan, Solution) without requiring a trained machine learning model or explicit acquisition function.\n    - Agent tools for enhanced decision-making: BioDiscoveryAgent is equipped with three tools: literature search via PubMed API to retrieve relevant papers, gene search using biological databases (Reactome) to identify genes in similar pathways, and an AI critic agent to review and refine predictions. These tools enable the agent to access information beyond its training data.\n\nPaper 20:\n  Title: LLM-GLOBE: A Benchmark Evaluating the Cultural Values Embedded in LLM Output\n  Abstract: Immense effort has been dedicated to minimizing the presence of harmful or biased generative content and better aligning AI output to human intention; however, research investigating the cultural values of LLMs is still in very early stages. Cultural values underpin how societies operate, providing profound insights into the norms, priorities, and decision making of their members. In recognition of this need for further research, we draw upon cultural psychology theory and the empirically-validated GLOBE framework to propose the LLM-GLOBE benchmark for evaluating the cultural value systems of LLMs, and we then leverage the benchmark to compare the values of Chinese and US LLMs. Our methodology includes a novel \"LLMs-as-a-Jury\" pipeline which automates the evaluation of open-ended content to enable large-scale analysis at a conceptual level. Results clarify similarities and differences that exist between Eastern and Western cultural value systems and suggest that open-generation tasks represent a more promising direction for evaluation of cultural values. We interpret the implications of this research for subsequent model development, evaluation, and deployment efforts as they relate to LLMs, AI cultural alignment more broadly, and the influence of AI cultural value systems on human-AI collaboration outcomes.\n  Structured Summary:\n    - Limited research on cultural values embedded in LLM output: While significant effort has been dedicated to minimizing harmful or biased content in AI, research examining the cultural values reflected in large language model outputs remains underdeveloped, despite culture's fundamental role in shaping societal norms and decision-making.\n    - Need for culturally-adept AI systems to prevent cultural encapsulation: As LLMs become increasingly integrated into people's routines and decision-making processes, understanding and aligning AI cultural values with diverse human cultural contexts is imperative to prevent cultural encapsulation, ensure appropriate representation, and facilitate effective cross-cultural deployment.\n    - LLM-GLOBE benchmark based on GLOBE framework: The authors adapt the Global Leadership and Organizational Behavior Effectiveness (GLOBE) framework, which defines nine cultural dimensions validated across 62 societies, to create a comprehensive benchmark for systematically evaluating cultural values embedded in LLM outputs through both closed-ended and open-generation prompts.\n    - LLMs-as-a-Jury automated evaluation pipeline: A novel automated scoring protocol where a panel of leading LLMs (GPT-4, Claude 3 Opus, Ernie 4.0, Qwen-72b) evaluate open-ended responses along cultural dimensions, with their ratings aggregated via least-squares regression trained on human-labeled data to produce final cultural value scores.\n\nPaper 21:\n  Title: Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions\n  Abstract: The deployment of large language models (LLMs) raises concerns regarding their cultural misalignment and potential ramifications on individuals and societies with diverse cultural backgrounds. While the discourse has focused mainly on political and social biases, our research proposes a Cultural Alignment Test (Hoftede's CAT) to quantify cultural alignment using Hofstede's cultural dimension framework, which offers an explanatory cross-cultural comparison through the latent variable analysis. We apply our approach to quantitatively evaluate LLMs, namely Llama 2, GPT-3.5, and GPT-4, against the cultural dimensions of regions like the United States, China, and Arab countries, using different prompting styles and exploring the effects of language-specific fine-tuning on the models' behavioural tendencies and cultural values. Our results quantify the cultural alignment of LLMs and reveal the difference between LLMs in explanatory cultural dimensions. Our study demonstrates that while all LLMs struggle to grasp cultural values, GPT-4 shows a unique capability to adapt to cultural nuances, particularly in Chinese settings. However, it faces challenges with American and Arab cultures. The research also highlights that fine-tuning LLama 2 models with different languages changes their responses to cultural questions, emphasizing the need for culturally diverse development in AI for worldwide acceptance and ethical use. For more details or to contribute to this research, visit our GitHub page this https URL\n  Structured Summary:\n    - Cultural misalignment in large language models: LLMs predominantly reflect Western cultural values while failing to account for cultural variances among diverse user groups, leading to potential misunderstandings and exacerbated cultural tensions.\n    - Need for explanatory cultural alignment assessment: Existing research lacks a comprehensive framework for explaining and quantifying how well LLMs align with different cultural values across diverse regions, necessitating a systematic approach to measure cultural alignment.\n    - Cultural Alignment Test based on Hofstede's framework: The authors propose Hofstede's CAT, which uses the VSM13 questionnaire with 24 Likert-scale questions to quantify LLM alignment across six cultural dimensions (PDI, IDV, MAS, UAI, LTO, IVR) using Kendall Tau correlation coefficients.\n    - Multiple prompting strategies for cultural assessment: The methodology includes model-level comparison (direct questioning in multiple languages), country-level comparison (instructing models to act as specific nationalities), hyperparameter comparison (varying temperature and top-p), and language correlation analysis (comparing models fine-tuned on different languages).\n\nPaper 22:\n  Title: CDEval: A Benchmark for Measuring the Cultural Dimensions of Large Language Models\n  Abstract: As the scaling of Large Language Models (LLMs) has dramatically enhanced their capabilities, there has been a growing focus on the alignment problem to ensure their responsible and ethical use. While existing alignment efforts predominantly concentrate on universal values such as the HHH principle, the aspect of culture, which is inherently pluralistic and diverse, has not received adequate attention. This work introduces a new benchmark, CDEval, aimed at evaluating the cultural dimensions of LLMs. CDEval is constructed by incorporating both GPT-4's automated generation and human verification, covering six cultural dimensions across seven domains. Our comprehensive experiments provide intriguing insights into the culture of mainstream LLMs, highlighting both consistencies and variations across different dimensions and domains. The findings underscore the importance of integrating cultural considerations in LLM development, particularly for applications in diverse cultural settings. Through CDEval, we aim to broaden the horizon of LLM alignment research by including cultural dimensions, thus providing a more holistic framework for the future development and evaluation of LLMs. This benchmark serves as a valuable resource for cultural studies in LLMs, paving the way for more culturally aware and sensitive models.\n  Structured Summary:\n    - Lack of cultural dimension evaluation in LLM alignment: Current LLM alignment research focuses primarily on universal values like helpfulness, honesty, and harmlessness, but neglects the pluralistic and diverse nature of culture. This gap means LLMs may not adequately respect or reflect cultural diversity in their responses.\n    - Importance of cultural diversity in LLM development: The authors are motivated by the recognition that human values are pluralistic and individuals from different cultural backgrounds hold varied viewpoints. LLMs need to respect cultural diversity beyond universal values to serve diverse populations appropriately.\n    - CDEval benchmark construction pipeline: The authors propose a three-step pipeline to construct CDEval: (1) defining a taxonomy based on Hofstede's six cultural dimensions across seven domains with multiple-choice format, (2) generating questions using GPT-4 with zero-shot and few-shot prompting, and (3) manually verifying generated data under specific rules.\n    - Questionnaire-based evaluation using LLMs as respondents: The authors use a questionnaire-based approach where LLMs act as respondents to multiple-choice questions. Each question presents two options representing opposing cultural orientations, allowing measurement of LLMs' cultural tendencies across different dimensions.\n\nPaper 23:\n  Title: How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions\n  Abstract: Large Language Models (LLMs) attempt to imitate human behavior by responding to humans in a way that pleases them, including by adhering to their values. However, humans come from diverse cultures with different values. It is critical to understand whether LLMs showcase different values to the user based on the stereotypical values of a user's known country. We prompt different LLMs with a series of advice requests based on 5 Hofstede Cultural Dimensions -- a quantifiable way of representing the values of a country. Throughout each prompt, we incorporate personas representing 36 different countries and, separately, languages predominantly tied to each country to analyze the consistency in the LLMs' cultural understanding. Through our analysis of the responses, we found that LLMs can differentiate between one side of a value and another, as well as understand that countries have differing values, but will not always uphold the values when giving advice, and fail to understand the need to answer differently based on different cultural values. Rooted in these findings, we present recommendations for training value-aligned and culturally sensitive LLMs. More importantly, the methodology and the framework developed here can help further understand and mitigate culture and language alignment issues with LLMs.\n  Structured Summary:\n    - LLMs fail to consistently uphold cultural values when giving advice: The paper identifies that although large language models can recognize differences in cultural values across countries, they do not reliably reflect these values in their responses when providing advice to users from diverse cultural backgrounds.\n    - Need for culturally appropriate advice to prevent cultural conflicts: The authors are motivated by the concern that LLMs serving diverse global users may provide advice conflicting with users' cultural values, potentially causing community disapproval and lasting ramifications. They aim to ensure LLMs can deliver culturally sensitive advice aligned with users' societal values.\n    - Systematic evaluation using Hofstede cultural dimensions with personas and multilingual prompts: The authors create 50 unique prompts for each of five Hofstede cultural dimensions, testing them with personas declaring nationality and multilingual translations across 36 languages. They use five state-of-the-art LLMs to generate responses and classify whether responses adhere to expected country values.\n    - Framework for understanding cultural alignment issues: The authors develop an empirical framework that analyzes quantifiable cultural values through balanced binary questions, examining whether models adhere to specific values across languages and resource levels, and evaluating justifications to distinguish cultural understanding from stereotypes.\n\nPaper 24:\n  Title: The Utility of Large Language Models and Generative AI for Education Research\n  Abstract: The use of natural language processing (NLP) techniques in engineering education can provide valuable insights into the underlying processes involved in generating text. While accessing these insights can be labor-intensive if done manually, recent advances in NLP and large language models have made it a realistic option for individuals. This study explores and evaluates a combination of clustering, summarization, and prompting techniques to analyze over 1,000 student essays in which students discussed their career interests. The specific assignment prompted students to define and explain their career goals as engineers. Using text embedding representations of student responses, we clustered the responses together to identify thematically similar statements from students. The clustered responses were then summarized to quickly identify career interest themes. We also used a set of a priori codes about career satisfaction and sectors to demonstrate an alternative approach to using these generative text models to analyze student writing. The results of this study demonstrate the feasibility and usefulness of NLP techniques in engineering education research. By automating the initial analysis of student essays, researchers and educators can more efficiently and accurately identify key themes and patterns in student writing. The methods presented in this paper have broader applications for engineering education and research purposes beyond analyzing student essays. By explaining these methods to the engineering education community, readers can utilize them in their own contexts.\n  Structured Summary:\n    - Labor-intensive manual analysis of qualitative text data in engineering education: The paper addresses the challenge that analyzing qualitative data such as student essays, reflections, and interview transcripts in engineering education is time-consuming and resource-intensive, often causing valuable insights to go unobserved due to lack of efficient analysis methods.\n    - Scaling qualitative analysis to extract insights from large text datasets: The authors are motivated by the need to enable educators and researchers to efficiently analyze large volumes of qualitative data (e.g., thousands of student essays or survey responses) to uncover patterns and themes that would otherwise remain hidden, thereby informing teaching strategies and program development.\n    - Inductive clustering and summarization using text embeddings and LLMs: The authors propose an inductive approach that embeds student essay sentences into vector space, clusters semantically similar sentences, and uses a generative language model (GPT-3.5) to summarize clusters at multiple levels of abstraction, ultimately generating a preliminary codebook of themes.\n    - Deductive labeling with a priori codes using LLM prompting: The authors demonstrate a deductive approach where pre-existing coding schemes (O*NET job titles and career satisfaction factors) are applied to student essays via prompts to a generative model, enabling researchers to identify specific topics or themes without manual dictionary creation.\n\nPaper 25:\n  Title: The Ethics of AI in Games\n  Abstract: Video games are one of the richest and most popular forms of human-computer interaction and, hence, their role is critical for our understanding of human behaviour and affect at a large scale. As artificial intelligence (AI) tools are gradually adopted by the game industry a series of ethical concerns arise. Such concerns, however, have so far not been extensively discussed in a video game context. Motivated by the lack of a comprehensive review of the ethics of AI as applied to games, we survey the current state of the art in this area and discuss ethical considerations of these systems from the holistic perspective of the affective loop. Through the components of this loop, we study the ethical challenges that AI faces in video game development. Elicitation highlights the ethical boundaries of artificially induced emotions; sensing showcases the trade-off between privacy and safe gaming spaces; and detection, as utilised during in-game adaptation, poses challenges to transparency and ownership. This paper calls for an open dialogue and action for the games of today and the virtual spaces of the future. By setting an appropriate framework we aim to protect users and to guide developers towards safer and better experiences for their customers.\n  Structured Summary:\n    - Lack of comprehensive review on ethics of AI in games: The paper addresses the absence of a thorough examination of ethical concerns arising from the adoption of AI tools in the video game industry, particularly in the context of player modeling and affective computing systems.\n    - Critical role of games in understanding human behavior at scale: The authors are motivated by the importance of video games as a medium for studying human behavior and emotion, combined with the growing ethical concerns as AI tools become more prevalent in game development and player experience management.\n    - Structuring ethical analysis through the affective game loop: The authors organize their ethical analysis by examining AI systems through the phases of the affective game loop: elicitation (emotion induction), sensing (data capture), detection (affect modeling), and adaptation (content adjustment). This framework allows systematic examination of ethical challenges at each stage of affect-driven game systems.\n    - Need for open dialogue and action on game AI ethics: The paper concludes that addressing ethical concerns in game AI requires active dialogue among all stakeholders—researchers, developers, policymakers, and players—to establish frameworks that protect users while enabling developers to create better, safer gaming experiences.\n\nPaper 26:\n  Title: Towards Accountability in the Use of Artificial Intelligence for Public Administrations\n  Abstract: We argue that the phenomena of distributed responsibility, induced acceptance, and acceptance through ignorance constitute instances of imperfect delegation when tasks are delegated to computationally-driven systems. Imperfect delegation challenges human accountability. We hold that both direct public accountability via public transparency and indirect public accountability via transparency to auditors in public organizations can be both instrumentally ethically valuable and required as a matter of deontology from the principle of democratic self-government. We analyze the regulatory content of 16 guideline documents about the use of AI in the public sector, by mapping their requirements to those of our philosophical account of accountability, and conclude that while some guidelines refer to processes that amount to auditing, it seems that the debate would benefit from more clarity about the nature of the entitlement of auditors and the goals of auditing, also in order to develop ethically meaningful standards with respect to which different forms of auditing can be evaluated and compared.\n  Structured Summary:\n    - Imperfect delegation in AI systems challenges human accountability: The paper identifies that when public administrations delegate tasks to AI systems, three phenomena (distributed responsibility, induced acceptance, and acceptance through ignorance) create imperfect delegation scenarios that undermine the ability to hold human agents accountable for decisions and outcomes.\n    - Democratic self-government requires accountability in public AI use: The authors are motivated by the principle that democratic self-government demands accountability when public administrations use AI. They argue this is not merely instrumentally valuable but a deontological requirement, meaning citizens have a right to control how they are governed regardless of outcomes.\n    - Philosophical framework distinguishing direct and indirect public accountability: The authors develop a conceptual framework that distinguishes two forms of public accountability: direct accountability through public transparency and indirect accountability through auditors. They ground this framework in democratic theory, providing both instrumental and non-instrumental (deontological) justifications.\n    - Analysis of automation as delegation with three accountability challenges: The authors analyze AI automation as a delegation process from human agents to artificial agents, identifying three specific challenges (distributed responsibility, induced acceptance, and acceptance through ignorance) that prevent ideal delegation and threaten accountability elements like responsibility identification and answerability.\n\nPaper 27:\n  Title: Can Science and Technology Capacity be Measured?\n  Abstract: The ability of a nation to participate in the global knowledge economy depends to some extent on its capacities in science and technology. In an effort to assess the capacity of different countries in science and technology, this article updates a classification scheme developed by RAND to measure science and technology capacity for 150 countries of the world.\n  Structured Summary:\n    - Lack of updated and accurate measurement of science and technology capacity across countries: The paper addresses the need to update and improve an existing index that measures science and technology capacity across nations. The original index required more recent data and improved accuracy to better assess countries' abilities to participate in the global knowledge economy.\n    - Supporting policymaking for S&T capacity enhancement and innovation: The authors are motivated by the need to provide policymakers at international institutions with a tool to assess and enhance science and technology capacity. S&T capacity has been shown to correlate positively with economic growth, making it important for development policy.\n    - Three-step construction of composite S&T Capacity Index: The authors construct the index through a systematic three-step process: selecting appropriate variables, combining indicators by standardizing them to a common format and checking consistency, and validating results against alternative evidence. This ensures transparency and reproducibility.\n    - Eight quantitative indicators organized into three domains: The index uses eight indicators grouped into three domains: preconditions (environment for knowledge absorption), resources (direct S&T capacity measures like scientists, institutions, and R&D funding), and output (knowledge production and diffusion). This structure captures different aspects of S&T capacity.\n\nPaper 28:\n  Title: In Oxford Handbook on AI Governance: The Role of Workers in AI Ethics and Governance\n  Abstract:  While the role of states, corporations, and international organizations in AI governance has been extensively theorized, the role of workers has received comparatively little attention. This chapter looks at the role that workers play in identifying and mitigating harms from AI technologies. Harms are the causally assessed impacts of technologies. They arise despite technical reliability and are not a result of technical negligence but rather of normative uncertainty around questions of safety and fairness in complex social systems. There is high consensus in the AI ethics community on the benefits of reducing harms but less consensus on mechanisms for determining or addressing harms. This lack of consensus has resulted in a number of collective actions by workers protesting how harms are identified and addressed in their workplace. We theorize the role of workers within AI governance and construct a model of harm reporting processes in AI workplaces. The harm reporting process involves three steps, identification, the governance decision, and the response. Workers draw upon three types of claims to argue for jurisdiction over questions of AI governance, subjection, control over the product of labor, and proximate knowledge of systems. Examining the past decade of AI related worker activism allows us to understand how different types of workers are positioned within a workplace that produces AI systems, how their position informs their claims, and the place of collective action in staking their claims. This chapter argues that workers occupy a unique role in identifying and mitigating harms caused by AI systems.\n  Structured Summary:\n    - Limited attention to workers' role in AI governance: The paper addresses the gap that while states, corporations, and international organizations have been extensively studied in AI governance, the role of workers in identifying and mitigating AI harms has not been adequately examined.\n    - Understanding worker activism in AI governance: The authors are motivated to understand how workers are positioned in AI workplaces and how they use collective action to stake claims on AI governance questions, particularly when there is disagreement about identifying or treating harms.\n    - Model of harm reporting processes in AI workplaces: The authors construct a three-step model of harm reporting in AI workplaces (identification, governance decision, response) involving three stakeholders (management, AI workers, AI ethicists), drawing on archival data of 25 collective actions from 2010 to 2020.\n    - Typology of worker claims to AI governance jurisdiction: The authors identify three types of claims workers use to assert jurisdiction over AI governance: subjection (being subject to harms), control over the product of one's labor, and proximate knowledge of systems.\n\nPaper 29:\n  Title: The Artificial Intelligence Act: critical overview\n  Abstract: This article provides a critical overview of the recently approved Artificial Intelligence Act. It starts by presenting the main structure, objectives, and approach of Regulation (EU) 2024/1689. A definition of key concepts follows, and then the material and territorial scope, as well as the timing of application, are analyzed. Although the Regulation does not explicitly set out principles, the main ideas of fairness, accountability, transparency, and equity in AI underly a set of rules of the regulation. This is discussed before looking at the ill-defined set of forbidden AI practices (manipulation and e exploitation of vulnerabilities, social scoring, biometric identification and classification, and predictive policing). It is highlighted that those rules deal with behaviors rather than AI systems. The qualification and regulation of high-risk AI systems are tackled, alongside the obligation of transparency for certain systems, the regulation of general-purpose models, and the rules on certification, supervision, and sanctions. The text concludes that even if the overall framework can be deemed adequate and balanced, the approach is so complex that it risks defeating its own purpose of promoting responsible innovation within the European Union and beyond its borders.\n  Structured Summary:\n    - Complexity and ambiguity in the AI Act's regulatory framework: The paper identifies that the AI Act, despite aiming to regulate artificial intelligence responsibly, suffers from excessive complexity, unclear definitions, and overlapping rules that may hinder its effectiveness and the innovation it seeks to promote.\n    - Need for critical assessment of the AI Act's adequacy and balance: The author is motivated to evaluate whether the overall framework of the AI Act is adequate and balanced, given the rapid technological evolution of AI and the numerous legal and ethical challenges it poses across multiple domains of law.\n    - Structured analysis of the AI Act's key components: The paper employs a systematic method of dissecting the AI Act by examining its structure, objectives, definitions, scope, principles, prohibited practices, high-risk system rules, transparency obligations, general-purpose models, and certification/supervision mechanisms.\n    - Adequate but overly complex regulatory framework: The author concludes that while the AI Act's framework is conceptually sound and balanced in addressing AI risks, its excessive complexity and intricate rules may undermine its goal of fostering responsible innovation both within and beyond the EU.\n\nPaper 30:\n  Title: Unpacking the Ethical Value Alignment in Big Models\n  Abstract: Big models have greatly advanced AI's ability to understand, generate, and manipulate information and content, enabling numerous applications. However, as these models become increasingly integrated into everyday life, their inherent ethical values and potential biases pose unforeseen risks to society. This paper provides an overview of the risks and challenges associated with big models, surveys existing AI ethics guidelines, and examines the ethical implications arising from the limitations of these models. Taking a normative ethics perspective, we propose a reassessment of recent normative guidelines, highlighting the importance of collaborative efforts in academia to establish a unified and universal AI ethics framework. Furthermore, we investigate the moral inclinations of current mainstream LLMs using the Moral Foundation theory, analyze existing alignment algorithms, and outline the unique challenges encountered in aligning ethical values within them. To address these challenges, we introduce a novel conceptual paradigm for aligning the ethical values of big models and discuss promising research directions for alignment criteria, evaluation, and method, representing an initial step towards the interdisciplinary construction of the ethically aligned AI\nThis paper is a modified English version of our Chinese paper this https URL, intended to help non-Chinese native speakers better understand our work.\n  Structured Summary:\n    - Ethical risks and value misalignment in big models: The paper addresses the challenge that big models, despite their powerful capabilities, carry inherent ethical values and biases that can lead to societal harms such as discrimination, misinformation, privacy violations, and malicious use, posing unforeseen risks as they integrate into daily life.\n    - Need for ethical governance of powerful AI systems: The authors are motivated by the urgency to ensure that big models, which possess human-level intelligence and unprecedented capabilities, are ethically governed and responsibly employed to prevent societal harms and to benefit humanity's sustainable development.\n    - AI Categorical Imperative framework for reviewing ethical guidelines: The authors propose a normative ethics-based framework, the AI Categorical Imperative, to systematically review and assess existing AI ethical guidelines. This framework uses two formulations to determine which propositions should become universal moral maxims for AI, emphasizing universality, absolute necessity, consensus, and anthropocentrism.\n    - Moral Foundations Theory-based evaluation of LLMs: The authors employ the Moral Foundations Theory, which includes five moral foundations (Care, Fairness, Loyalty, Authority, Sanctity), to empirically assess the ethical value tendencies of mainstream large language models through questionnaire-based evaluation.\n\nPaper 31:\n  Title: DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker\n  Abstract: Artificial intelligence has seen several breakthroughs in recent years, with games often serving as milestones. A common feature of these games is that players have perfect information. Poker is the quintessential game of imperfect information, and a longstanding challenge problem in artificial intelligence. We introduce DeepStack, an algorithm for imperfect information settings. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition that is automatically learned from self-play using deep learning. In a study involving 44,000 hands of poker, DeepStack defeated with statistical significance professional poker players in heads-up no-limit Texas hold'em. The approach is theoretically sound and is shown to produce more difficult to exploit strategies than prior approaches.\n  Structured Summary:\n    - Imperfect information game reasoning in large-scale settings: The paper addresses the challenge of developing AI that can reason effectively in large-scale imperfect information games, where players have asymmetric information about the game state. Prior approaches using abstraction have shown massive flaws and fail to achieve expert-level performance.\n    - Bridging the gap between perfect and imperfect information game AI: The authors are motivated by the need to develop AI that can handle real-world scenarios involving information asymmetry, deception, and recursive reasoning about opponents' beliefs. Mastering imperfect information games like poker represents a fundamental step toward AI applicable to realistic decision-making problems.\n    - Continual re-solving with depth-limited lookahead: DeepStack computes strategies during play only for states that actually arise, using continual re-solving that maintains the agent's range and opponent counterfactual values. It limits lookahead depth by substituting deeper computation with learned value function estimates, avoiding the need to reason about the entire game.\n    - Deep counterfactual value networks as intuition: The algorithm uses deep neural networks trained via self-play on randomly generated poker situations to approximate counterfactual values beyond the lookahead depth. These networks take as input the public state and player ranges, outputting value estimates for each possible hand, serving as a form of learned intuition.\n\nPaper 32:\n  Title: Response to NITRD, NCO, NSF Request for Information on \"Update to the 2016 National Artificial Intelligence Research and Development Strategic Plan\"\n  Abstract: We present a response to the 2018 Request for Information (RFI) from the NITRD, NCO, NSF regarding the \"Update to the 2016 National Artificial Intelligence Research and Development Strategic Plan.\" Through this document, we provide a response to the question of whether and how the National Artificial Intelligence Research and Development Strategic Plan (NAIRDSP) should be updated from the perspective of Fermilab, America's premier national laboratory for High Energy Physics (HEP). We believe the NAIRDSP should be extended in light of the rapid pace of development and innovation in the field of Artificial Intelligence (AI) since 2016, and present our recommendations below. AI has profoundly impacted many areas of human life, promising to dramatically reshape society --- e.g., economy, education, science --- in the coming years. We are still early in this process. It is critical to invest now in this technology to ensure it is safe and deployed ethically. Science and society both have a strong need for accuracy, efficiency, transparency, and accountability in algorithms, making investments in scientific AI particularly valuable. Thus far the US has been a leader in AI technologies, and we believe as a national Laboratory it is crucial to help maintain and extend this leadership. Moreover, investments in AI will be important for maintaining US leadership in the physical sciences.\n  Structured Summary:\n    - Limitations of traditional data analysis for large-scale HEP data: The paper identifies that current traditional algorithms and models cannot adequately handle the enormous scale and complexity of modern high-energy physics datasets, necessitating new approaches for data analysis and discovery.\n    - Maintaining US leadership in AI and physical sciences through HEP applications: The authors are motivated by the need to preserve and extend US leadership in both AI technologies and physical sciences by leveraging HEP's unique capabilities and infrastructure to advance AI research and applications.\n    - Integration of statistical models with AI algorithms for bias reduction: The authors propose combining traditional statistical modeling approaches with AI algorithms to enable bias estimation and removal while producing uncertainty measures that have clear statistical interpretations, addressing interpretability challenges in scientific AI.\n    - Matching neural network symmetries to physical problem symmetries: The paper proposes designing neural network architectures that incorporate the same mathematical symmetries (rotational, spherical, etc.) present in the physical systems being studied, enabling AI to better capture underlying physics from data.\n\nPaper 33:\n  Title: Who Followed the Blueprint? Analyzing the Responses of U.S. Federal Agencies to the Blueprint for an AI Bill of Rights\n  Abstract: This study examines the extent to which U.S. federal agencies responded to and implemented the principles outlined in the White House's October 2022 \"Blueprint for an AI Bill of Rights.\" The Blueprint provided a framework for the ethical governance of artificial intelligence systems, organized around five core principles: safety and effectiveness, protection against algorithmic discrimination, data privacy, notice and explanation about AI systems, and human alternatives and fallback.\nThrough an analysis of publicly available records across 15 federal departments, the authors found limited evidence that the Blueprint directly influenced agency actions after its release. Only five departments explicitly mentioned the Blueprint, while 12 took steps aligned with one or more of its principles. However, much of this work appeared to have precedents predating the Blueprint or motivations disconnected from it, such as compliance with prior executive orders on trustworthy AI. Departments' activities often emphasized priorities like safety, accountability and transparency that overlapped with Blueprint principles, but did not necessarily stem from it.\nThe authors conclude that the non-binding Blueprint seems to have had minimal impact on shaping the U.S. government's approach to ethical AI governance in its first year. Factors like public concerns after high-profile AI releases and obligations to follow direct executive orders likely carried more influence over federal agencies. More rigorous study would be needed to definitively assess the Blueprint's effects within the federal bureaucracy and broader society.\n  Structured Summary:\n    - Unclear impact of the Blueprint for an AI Bill of Rights on federal agencies: The paper addresses the lack of systematic understanding about whether and how U.S. federal agencies responded to the White House's non-binding Blueprint for an AI Bill of Rights after its October 2022 release.\n    - Assessing federal government adherence to ethical AI governance framework: The authors are motivated by the importance of understanding whether federal departments followed the President's vision for ethical AI governance, given the government's significant impact on citizens' rights and access to services.\n    - Analysis of publicly accessible records across 15 federal departments: The authors conducted an exploratory study examining public documents from 15 U.S. federal departments to identify explicit mentions of the Blueprint and actions aligned with its five core principles after October 2022.\n    - Blueprint had minimal impact on federal AI governance: The authors conclude that the non-binding Blueprint did not significantly shape federal agencies' approach to ethical AI governance, with other factors like executive orders and public concerns likely carrying more influence.\n\nPaper 34:\n  Title: LawLuo: A Multi-Agent Collaborative Framework for Multi-Round Chinese Legal Consultation\n  Abstract: Legal Large Language Models (LLMs) have shown promise in providing legal consultations to non-experts. However, most existing Chinese legal consultation models are based on single-agent systems, which differ from real-world legal consultations, where multiple professionals collaborate to offer more tailored responses. To better simulate real consultations, we propose LawLuo, a multi-agent framework for multi-turn Chinese legal consultations. LawLuo includes four agents: the receptionist agent, which assesses user intent and selects a lawyer agent; the lawyer agent, which interacts with the user; the secretary agent, which organizes conversation records and generates consultation reports; and the boss agent, which evaluates the performance of the lawyer and secretary agents to ensure optimal results. These agents' interactions mimic the operations of real law firms. To train them to follow different legal instructions, we developed distinct fine-tuning datasets. We also introduce a case graph-based RAG to help the lawyer agent address vague user inputs. Experimental results show that LawLuo outperforms baselines in generating more personalized and professional responses, handling ambiguous queries, and following legal instructions in multi-turn conversations. Our full code and constructed datasets will be open-sourced upon paper acceptance.\n  Structured Summary:\n    - Limitations of single-agent legal consultation systems: Existing Chinese legal LLMs use single-agent architectures that fail to replicate the collaborative workflows of real law firms, limiting their ability to provide personalized and professional responses to users.\n    - Simulating real law firm operations for better legal advice: The authors aim to replicate the collaborative processes of real law firms, where multiple professionals work together, to deliver more personalized and professional legal consultation services to users.\n    - LawLuo multi-agent framework with four specialized agents: LawLuo comprises four agents: a receptionist that assesses user intent and assigns a specialized lawyer, a lawyer that engages in multi-turn dialogue, a secretary that organizes records and generates reports, and a boss that monitors and optimizes agent performance.\n    - Three distinct fine-tuning datasets for agent training: The authors constructed three specialized datasets: (Inquire, Lawyer description) pairs for the receptionist, a MUlti Rounds LEgal Dialogue (MURLED) dataset for the lawyer, and a Legal Consultation Report Generation (LCRG) dataset for the secretary, to enable each agent to follow domain-specific legal instructions.\n\nPaper 35:\n  Title: Operationalising AI governance through ethics-based auditing: An industry case study\n  Abstract: Ethics based auditing (EBA) is a structured process whereby an entitys past or present behaviour is assessed for consistency with moral principles or norms. Recently, EBA has attracted much attention as a governance mechanism that may bridge the gap between principles and practice in AI ethics. However, important aspects of EBA (such as the feasibility and effectiveness of different auditing procedures) have yet to be substantiated by empirical research. In this article, we address this knowledge gap by providing insights from a longitudinal industry case study. Over 12 months, we observed and analysed the internal activities of AstraZeneca, a biopharmaceutical company, as it prepared for and underwent an ethics-based AI audit. While previous literature concerning EBA has focused on proposing evaluation metrics or visualisation techniques, our findings suggest that the main difficulties large multinational organisations face when conducting EBA mirror classical governance challenges. These include ensuring harmonised standards across decentralised organisations, demarcating the scope of the audit, driving internal communication and change management, and measuring actual outcomes. The case study presented in this article contributes to the existing literature by providing a detailed description of the organisational context in which EBA procedures must be integrated to be feasible and effective.\n  Structured Summary:\n    - Gap between AI ethics principles and practice: The paper addresses the lack of empirical evidence on how ethics-based auditing procedures can be feasibly and effectively implemented in real-world organizational contexts to bridge the gap between abstract AI ethics principles and operational practice.\n    - Need for empirical validation of AI governance mechanisms: The authors are motivated by the observation that while theoretical justifications for ethics-based auditing exist, there is insufficient understanding of how organizations actually implement EBA and what practical challenges they encounter, necessitating empirical case studies.\n    - Longitudinal industry case study using participant observation: The authors conducted a 12-month longitudinal case study of AstraZeneca, employing participant observation and semi-structured interviews to examine how the company prepared for and underwent an ethics-based AI audit, focusing on organizational processes and governance structures.\n    - Qualitative analysis combining participant observation and interviews: The research methodology combined direct participation in organizational meetings and activities with 18 semi-structured interviews across different stakeholder groups, using NVivo for coding and a parallel research design to triangulate findings.\n\nPaper 36:\n  Title: A Machine Learning Approach for Recruitment Prediction in Clinical Trial Design\n  Abstract: Significant advancements have been made in recent years to optimize patient recruitment for clinical trials, however, improved methods for patient recruitment prediction are needed to support trial site selection and to estimate appropriate enrollment timelines in the trial design stage. In this paper, using data from thousands of historical clinical trials, we explore machine learning methods to predict the number of patients enrolled per month at a clinical trial site over the course of a trial's enrollment duration. We show that these methods can reduce the error that is observed with current industry standards and propose opportunities for further improvement.\n  Structured Summary:\n    - Imprecise patient recruitment prediction in clinical trial design: The paper addresses the challenge of accurately predicting patient enrollment in clinical trials during the design stage. Current methods lack precision and granularity, leading to recruitment delays and trial failures despite recent improvements in enrollment optimization.\n    - Reducing trial recruitment delays and failures: The authors are motivated by the need to improve trial success rates by providing more accurate enrollment predictions. While recent efforts have improved enrollment timelines, studies suggest that better prediction methods are still needed to minimize recruitment-related risks.\n    - Data-driven machine learning approach for site-level monthly enrollment prediction: The authors develop a machine learning framework that predicts monthly patient enrollment counts at individual trial sites throughout the enrollment duration. The approach uses historical enrollment records from thousands of trials and incorporates study, country, site, and time-varying features to account for temporal variations and site characteristics.\n    - Feature engineering from historical trial operational data: The authors create approximately 245 features across multiple levels (study, country, site, and site-month) by extracting performance metrics from relevant historical trials. These features include enrollment rates, disease prevalence estimates, and temporal indicators, with hierarchical imputation strategies for missing values.\n\nPaper 37:\n  Title: Analytical modelling of period spacings across the HR diagram\n  Abstract: The characterisation of stellar cores may be accomplished through the modelling of asteroseismic data from stars exhibiting either gravity-mode or mixed-mode pulsations, potentially shedding light on the physical processes responsible for the production, mixing, and segregation of chemical elements. In this work we validate against model data an analytical expression for the period spacing that will facilitate the inference of the properties of stellar cores, including the detection and characterisation of buoyancy glitches (strong chemical gradients). This asymptotically-based analytical expression is tested both in models with and without buoyancy glitches. It does not assume that glitches are small and, consequently, predicts non-sinusoidal glitch-induced period-spacing variations, as often seen in model and real data. We show that the glitch position and width inferred from the fitting of the analytical expression to model data consisting of pure gravity modes are in close agreement (typically better than 7$\\%$ relative difference) with the properties measured directly from the stellar models. In the case of fitting mixed-mode model data, the same expression is shown to reproduce well the numerical results, when the glitch properties are known a priori. In addition, the fits performed to mixed-mode model data reveal a frequency dependence of the coupling coefficient, $q$, for a moderate-luminosity red-giant-branch model star. Finally, we find that fitting the analytical expression to the mixed-mode period spacings may provide a way to infer the frequencies of the pure acoustic dipole modes that would exist if no coupling took place between acoustic and gravity waves.\n  Structured Summary:\n    - Inadequate analytical expressions for period spacing with structural glitches: Previous analytical expressions for period spacing variations caused by buoyancy glitches were limited to small-glitch approximations producing sinusoidal variations, or required interpolation procedures, failing to capture the non-sinusoidal variations seen in models with strong glitches.\n    - Characterizing stellar cores through asteroseismic data: Understanding stellar cores via asteroseismology can reveal physical processes like nuclear burning, diffusion, and mixing. Accurate modeling of period spacings is essential to infer core properties and detect strong chemical gradients (buoyancy glitches).\n    - Asymptotically-based analytical expression for period spacing with arbitrary-strength glitches: The authors derive and validate an analytical expression for period spacing that does not assume small glitches, allowing it to predict non-sinusoidal variations. The expression incorporates both mode coupling and structural glitches modeled by step or Gaussian-like functions.\n    - Modeling glitches with step and Gaussian-like functions: The authors extend previous work by modeling buoyancy glitches using step functions and Gaussian-like functions, which better represent the variety of glitch shapes found in stellar models compared to Dirac delta approximations.\n\nPaper 38:\n  Title: Ethics of generative AI and manipulation: a design-oriented research agenda\n  Abstract: Generative AI enables automated, effective manipulation at scale. Despite the growing general ethical discussion around generative AI, the specific manipulation risks remain inadequately investigated. This article outlines essential inquiries encompassing conceptual, empirical, and design dimensions of manipulation, pivotal for comprehending and curbing manipulation risks. By highlighting these questions, the article underscores the necessity of an appropriate conceptualisation of manipulation to ensure the responsible development of Generative AI technologies.\n  Structured Summary:\n    - Inadequate investigation of manipulation risks in generative AI: The paper identifies a gap in current research: while generative AI enables automated, effective manipulation at scale, the specific ethical risks related to manipulation have not been sufficiently studied or conceptualized.\n    - Manipulation as a fundamental threat to trustworthy AI and democracy: The authors are motivated by the legal and moral imperative to address manipulation, given its identification as a disvalue in AI regulation (e.g., EU AI Act) and its threat to democratic values and trustworthy AI systems.\n    - Design-for-values approach to non-manipulation: The paper proposes a design-for-values framework that systematically addresses manipulation through conceptual, empirical, and design stages, emphasizing that appropriate conceptualization of manipulation is critical for deriving concrete design requirements.\n    - Indifference criterion for identifying manipulation: The paper advocates for the indifference criterion as a more appropriate conceptualization of manipulation than alternatives like hidden influence or bypassing rationality. This criterion identifies manipulation based on the influencer's indifference to revealing reasons, rather than malicious intent.\n\n\nBefore returning, check that:\n- Every paper title appears exactly once.\n- Only leaf nodes have a \"papers\" field.\n- All intermediate nodes have a \"subtopics\" field.\n- The JSON is strictly valid and parsable.\n\n### Output:", "hierarchy_response": "{\n  \"name\": \"Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives\",\n  \"subtopics\": [\n    {\n      \"name\": \"Ethical Frameworks and Principles for AI\",\n      \"subtopics\": [\n        {\n          \"name\": \"Global AI Ethics Guidelines and Convergence\",\n          \"papers\": [\n            \"Artificial Intelligence: the global landscape of ethics guidelines\",\n            \"Ethics of AI: A Systematic Literature Review of Principles and Challenges\",\n            \"Artificial Intelligence Governance and Ethics: Global Perspectives\"\n          ]\n        },\n        {\n          \"name\": \"Conceptual Frameworks for AI Ethics\",\n          \"papers\": [\n            \"Unpacking the Ethical Value Alignment in Big Models\",\n            \"Ethics of generative AI and manipulation: a design-oriented research agenda\"\n          ]\n        },\n        {\n          \"name\": \"Domain-Specific AI Ethics\",\n          \"papers\": [\n            \"The Ethics of AI in Games\",\n            \"Towards Accountability in the Use of Artificial Intelligence for Public Administrations\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Value Alignment Goals and Evaluation\",\n      \"subtopics\": [\n        {\n          \"name\": \"Alignment Goal Taxonomies\",\n          \"papers\": [\n            \"From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models\"\n          ]\n        },\n        {\n          \"name\": \"Value Assessment Benchmarks and Frameworks\",\n          \"papers\": [\n            \"Value Compass Benchmarks: A Platform for Fundamental and Validated Evaluation of LLMs Values\",\n            \"DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life\"\n          ]\n        },\n        {\n          \"name\": \"Safety Alignment Datasets\",\n          \"papers\": [\n            \"BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset\",\n            \"SafeSora: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Cultural Value Alignment\",\n      \"subtopics\": [\n        {\n          \"name\": \"Cultural Bias Detection and Measurement\",\n          \"papers\": [\n            \"Cultural Bias and Cultural Alignment of Large Language Models\",\n            \"Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions\"\n          ]\n        },\n        {\n          \"name\": \"Cross-Cultural Value Evaluation\",\n          \"papers\": [\n            \"CDEval: A Benchmark for Measuring the Cultural Dimensions of Large Language Models\",\n            \"How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions\",\n            \"LLM-GLOBE: A Benchmark Evaluating the Cultural Values Embedded in LLM Output\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Multi-Agent Systems and Cooperation\",\n      \"subtopics\": [\n        {\n          \"name\": \"Multi-Agent Collaboration and Debate\",\n          \"papers\": [\n            \"Improving Factuality and Reasoning in Language Models through Multiagent Debate\",\n            \"LawLuo: A Multi-Agent Collaborative Framework for Multi-Round Chinese Legal Consultation\"\n          ]\n        },\n        {\n          \"name\": \"Fairness and Cooperation in Multi-Agent Settings\",\n          \"papers\": [\n            \"Achieving Fairness in Multi-Agent Markov Decision Processes Using Reinforcement Learning\",\n            \"Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Training Methods for Value Alignment\",\n      \"subtopics\": [\n        {\n          \"name\": \"Social Interaction-Based Training\",\n          \"papers\": [\n            \"Training Socially Aligned Language Models on Simulated Social Interactions\"\n          ]\n        },\n        {\n          \"name\": \"Multi-Objective Alignment\",\n          \"papers\": [\n            \"HydraGAN A Multi-head, Multi-objective Approach to Synthetic Data Generation\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Transparency and Interpretability\",\n      \"subtopics\": [\n        {\n          \"name\": \"Interpretable AI Design Principles\",\n          \"papers\": [\n            \"The five Is: Key principles for interpretable and safe conversational AI\"\n          ]\n        },\n        {\n          \"name\": \"Automated Interpretability Tools\",\n          \"papers\": [\n            \"A Multimodal Automated Interpretability Agent\"\n          ]\n        },\n        {\n          \"name\": \"User-Facing Transparency Systems\",\n          \"papers\": [\n            \"Empowering Users in Digital Privacy Management through Interactive LLM-Based Agents\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"AI Governance and Policy Implementation\",\n      \"subtopics\": [\n        {\n          \"name\": \"Regulatory Frameworks\",\n          \"papers\": [\n            \"The Artificial Intelligence Act: critical overview\"\n          ]\n        },\n        {\n          \"name\": \"Policy Implementation and Compliance\",\n          \"papers\": [\n            \"Who Followed the Blueprint? Analyzing the Responses of U.S. Federal Agencies to the Blueprint for an AI Bill of Rights\",\n            \"Response to NITRD, NCO, NSF Request for Information on \\\"Update to the 2016 National Artificial Intelligence Research and Development Strategic Plan\\\"\"\n          ]\n        },\n        {\n          \"name\": \"Organizational AI Governance\",\n          \"papers\": [\n            \"Operationalising AI governance through ethics-based auditing: An industry case study\",\n            \"The Role of Workers in AI Ethics and Governance\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Domain-Specific Applications and Methods\",\n      \"subtopics\": [\n        {\n          \"name\": \"Imperfect Information Game AI\",\n          \"papers\": [\n            \"DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker\"\n          ]\n        },\n        {\n          \"name\": \"Context-Aware Recommendation Systems\",\n          \"papers\": [\n            \"Situation-Aware Approach to Improve Context-based Recommender System\"\n          ]\n        },\n        {\n          \"name\": \"Scientific Discovery and Research Applications\",\n          \"papers\": [\n            \"BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments\",\n            \"The Utility of Large Language Models and Generative AI for Education Research\",\n            \"A Machine Learning Approach for Recruitment Prediction in Clinical Trial Design\"\n          ]\n        },\n        {\n          \"name\": \"Capacity Measurement and Assessment\",\n          \"papers\": [\n            \"Can Science and Technology Capacity be Measured?\"\n          ]\n        },\n        {\n          \"name\": \"Unrelated Technical Paper\",\n          \"papers\": [\n            \"Analytical modelling of period spacings across the HR diagram\"\n          ]\n        }\n      ]\n    }\n  ]\n}\n", "hierarchy_tree": {"name": "Multi-level Value Alignment in Agentic AI Systems Survey and Perspectives", "subtopics": [{"name": "Ethical Frameworks and Principles for AI", "subtopics": [{"name": "Global AI Ethics Guidelines and Convergence", "papers": ["Artificial Intelligence: the global landscape of ethics guidelines", "Ethics of AI: A Systematic Literature Review of Principles and Challenges", "Artificial Intelligence Governance and Ethics: Global Perspectives"]}, {"name": "Conceptual Frameworks for AI Ethics", "papers": ["Unpacking the Ethical Value Alignment in Big Models", "Ethics of generative AI and manipulation: a design-oriented research agenda"]}, {"name": "Domain-Specific AI Ethics", "papers": ["The Ethics of AI in Games", "Towards Accountability in the Use of Artificial Intelligence for Public Administrations"]}]}, {"name": "Value Alignment Goals and Evaluation", "subtopics": [{"name": "Alignment Goal Taxonomies", "papers": ["From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models"]}, {"name": "Value Assessment Benchmarks and Frameworks", "papers": ["Value Compass Benchmarks: A Platform for Fundamental and Validated Evaluation of LLMs Values", "DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life"]}, {"name": "Safety Alignment Datasets", "papers": ["BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset", "SafeSora: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset"]}]}, {"name": "Cultural Value Alignment", "subtopics": [{"name": "Cultural Bias Detection and Measurement", "papers": ["Cultural Bias and Cultural Alignment of Large Language Models", "Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions"]}, {"name": "Cross-Cultural Value Evaluation", "papers": ["CDEval: A Benchmark for Measuring the Cultural Dimensions of Large Language Models", "How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions", "LLM-GLOBE: A Benchmark Evaluating the Cultural Values Embedded in LLM Output"]}]}, {"name": "Multi-Agent Systems and Cooperation", "subtopics": [{"name": "Multi-Agent Collaboration and Debate", "papers": ["Improving Factuality and Reasoning in Language Models through Multiagent Debate", "LawLuo: A Multi-Agent Collaborative Framework for Multi-Round Chinese Legal Consultation"]}, {"name": "Fairness and Cooperation in Multi-Agent Settings", "papers": ["Achieving Fairness in Multi-Agent Markov Decision Processes Using Reinforcement Learning", "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents"]}]}, {"name": "Training Methods for Value Alignment", "subtopics": [{"name": "Social Interaction-Based Training", "papers": ["Training Socially Aligned Language Models on Simulated Social Interactions"]}, {"name": "Multi-Objective Alignment", "papers": ["HydraGAN A Multi-head, Multi-objective Approach to Synthetic Data Generation"]}]}, {"name": "Transparency and Interpretability", "subtopics": [{"name": "Interpretable AI Design Principles", "papers": ["The five Is: Key principles for interpretable and safe conversational AI"]}, {"name": "Automated Interpretability Tools", "papers": ["A Multimodal Automated Interpretability Agent"]}, {"name": "User-Facing Transparency Systems", "papers": ["Empowering Users in Digital Privacy Management through Interactive LLM-Based Agents"]}]}, {"name": "AI Governance and Policy Implementation", "subtopics": [{"name": "Regulatory Frameworks", "papers": ["The Artificial Intelligence Act: critical overview"]}, {"name": "Policy Implementation and Compliance", "papers": ["Who Followed the Blueprint? Analyzing the Responses of U.S. Federal Agencies to the Blueprint for an AI Bill of Rights", "Response to NITRD, NCO, NSF Request for Information on \"Update to the 2016 National Artificial Intelligence Research and Development Strategic Plan\""]}, {"name": "Organizational AI Governance", "papers": ["Operationalising AI governance through ethics-based auditing: An industry case study", "The Role of Workers in AI Ethics and Governance"]}]}, {"name": "Domain-Specific Applications and Methods", "subtopics": [{"name": "Imperfect Information Game AI", "papers": ["DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker"]}, {"name": "Context-Aware Recommendation Systems", "papers": ["Situation-Aware Approach to Improve Context-based Recommender System"]}, {"name": "Scientific Discovery and Research Applications", "papers": ["BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments", "The Utility of Large Language Models and Generative AI for Education Research", "A Machine Learning Approach for Recruitment Prediction in Clinical Trial Design"]}, {"name": "Capacity Measurement and Assessment", "papers": ["Can Science and Technology Capacity be Measured?"]}, {"name": "Unrelated Technical Paper", "papers": ["Analytical modelling of period spacings across the HR diagram"]}]}]}}
{"id": 28, "survey": "A Survey on Uncertainty Quantification of Large Language", "pdfs": [{"title": "To Believe or Not to Believe Your LLM", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Applications/Chatbot_and_Textual/To Believe or Not to Believe Your LLM.pdf", "summary": {"paper_id": "survey-28_paper-0_To Believe or Not to Believe Your LLM", "core_task": {"text": "uncertainty quantification in large language models", "query_variants": ["uncertainty quantification in large language models", "estimating confidence and reliability in LLMs", "measuring prediction uncertainty for large-scale language models"]}, "contributions": [{"name": "Detecting hallucinations in LLMs with multiple valid answers", "description": "Existing uncertainty quantification methods for LLMs fail when multiple correct responses exist (high aleatoric uncertainty), because they cannot distinguish between epistemic uncertainty (lack of knowledge) and aleatoric uncertainty (multiple valid answers). A perfect predictor and a useless predictor may have the same total uncertainty but different epistemic uncertainty.", "author_claim_text": "a common limitation of these approaches is that they are only meaningful in problems where there exists a single correct response (e.g. label) as they aim for detecting if one response is dominant", "source_hint": "Introduction, paragraph 3"}, {"name": "Importance of decoupling epistemic and aleatoric uncertainty", "description": "The authors are motivated to address the problem of truthfulness in LLMs by distinguishing epistemic uncertainty (arising from lack of knowledge) from aleatoric uncertainty (arising from irreducible randomness like multiple valid answers). When epistemic uncertainty is high, the model's predictions are unreliable and may be hallucinated.", "author_claim_text": "we aim to identify when only the epistemic uncertainty is large, in which case we can suspect that the response is hallucinated", "source_hint": "Introduction, paragraph 4"}, {"name": "Iterative prompting procedure for constructing pseudo joint distributions", "description": "The authors propose an iterative prompting method that constructs a joint distribution over multiple responses by repeatedly querying the LLM with the original query plus previously generated responses. This allows them to measure how responses depend on context, revealing epistemic uncertainty through the chain rule of probability.", "author_claim_text": "first we ask the model to provide a response given a query, then to provide another response given the query and the first response, then a third one given the query and the first two responses", "source_hint": "Introduction, paragraph 5"}, {"name": "Information-theoretic metric based on mutual information", "description": "The authors derive a metric for epistemic uncertainty based on the KL divergence between the LLM-derived pseudo joint distribution and the ground truth. They prove this can be lower-bounded by the mutual information of the LLM-derived distribution, which is computable solely from the model's outputs and is insensitive to aleatoric uncertainty.", "author_claim_text": "we derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large", "source_hint": "Abstract and Section 4"}]}, "abs": "We explore uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large. We simultaneously consider both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers). In particular, we derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable. This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses. Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses. This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected. We conduct a series of experiments which demonstrate the advantage of our formulation. Further, our investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest."}, {"title": "Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Applications/Chatbot_and_Textual/Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus.pdf", "summary": {"paper_id": "survey-28_paper-1_Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus", "core_task": {"text": "Hallucination detection in large language models", "query_variants": ["Hallucination detection in large language models", "Identifying factual errors and false outputs in LLMs", "Detection of unfaithful generation in large-scale language models"]}, "contributions": [{"name": "Hallucination detection in LLMs is costly and inefficient", "description": "Current hallucination detection methods face significant limitations: retrieval-based approaches depend on external knowledge bases that may not be accessible, while sampling-based methods require multiple LLM responses for consistency verification, resulting in high computational costs and inefficiency.", "author_claim_text": "Existing works for detecting hallucinations in LLMs either rely on external knowledge for reference retrieval or require sampling multiple responses from the LLM for consistency verification, making these methods costly and inefficient.", "source_hint": "Abstract"}, {"name": "Need for reliable and trustworthy LLMs in accuracy-critical scenarios", "description": "The prevalence of hallucinations across LLM applications undermines their reliability and trustworthiness, particularly in domains like finance, medicine, and education where factual accuracy is critical. This motivates the need for effective hallucination detection methods.", "author_claim_text": "Hallucinations are a common occurrence in almost all applications, thus undermining the reliability and trustworthiness of LLMs, especially in scenarios where accuracy and veracity are essential.", "source_hint": "Introduction"}, {"name": "Reference-free uncertainty-based hallucination detection with human-like focus", "description": "The authors propose a reference-free, uncertainty-based detection method that mimics human factuality checking by focusing on three key aspects: identifying the most informative keywords, detecting unreliable tokens in historical context that may trigger cascading hallucinations, and considering token properties like type and frequency.", "author_claim_text": "we propose a novel reference-free, uncertainty-based method for detecting hallucinations in LLMs. Our approach imitates human focus in factuality checking from three aspects", "source_hint": "Abstract"}, {"name": "State-of-the-art performance without additional information requirements", "description": "The proposed method achieves state-of-the-art performance across all evaluation metrics on relevant datasets while eliminating the dependency on external knowledge bases or multiple sampled responses, demonstrating its effectiveness and efficiency.", "author_claim_text": "Experimental results on relevant datasets demonstrate the effectiveness of our proposed method, which achieves state-of-the-art performance across all the evaluation metrics and eliminates the need for additional information.", "source_hint": "Abstract"}]}, "abs": "Large Language Models (LLMs) have gained significant popularity for their impressive performance across diverse fields. However, LLMs are prone to hallucinate untruthful or nonsensical outputs that fail to meet user expectations in many real-world applications. Existing works for detecting hallucinations in LLMs either rely on external knowledge for reference retrieval or require sampling multiple responses from the LLM for consistency verification, making these methods costly and inefficient. In this paper, we propose a novel reference-free, uncertainty-based method for detecting hallucinations in LLMs. Our approach imitates human focus in factuality checking from three aspects: 1) focus on the most informative and important keywords in the given text; 2) focus on the unreliable tokens in historical context which may lead to a cascade of hallucinations; and 3) focus on the token properties such as token type and token frequency. Experimental results on relevant datasets demonstrate the effectiveness of our proposed method, which achieves state-of-the-art performance across all the evaluation metrics and eliminates the need for additional information."}, {"title": "Language Models with Conformal Factuality Guarantees", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Applications/Chatbot_and_Textual/Language Models with Conformal Factuality Guarantees.pdf", "summary": {"paper_id": "survey-28_paper-2_Language Models with Conformal Factuality Guarantees", "core_task": {"text": "Ensuring correctness and factuality of language model outputs with probabilistic guarantees", "query_variants": ["Ensuring correctness and factuality of language model outputs with probabilistic guarantees", "Probabilistic guarantees for factual correctness in language model generation", "Verifying accuracy and truthfulness of LM outputs using probabilistic methods"]}, "contributions": [{"name": "Lack of correctness and factuality guarantees for language model outputs", "description": "The paper addresses the fundamental challenge that large language models generate outputs that cannot be fully trusted due to hallucinations and non-factual content, and existing methods lack precise probabilistic guarantees of correctness.", "author_claim_text": "Guaranteeing the correctness and factuality of language model (LM) outputs is a major open problem.", "source_hint": "Abstract, sentence 1"}, {"name": "Need for high-probability correctness guarantees in high-stakes domains", "description": "The authors are motivated by the requirement that in critical applications like healthcare, law, and robotics, language models must provide outputs that are correct with a known, user-specified probability, which current systems cannot guarantee.", "author_claim_text": "In many domains such as health, law, or robotics, safely deploying a language model requires outputs to be correct with at least some known, user-specified probability.", "source_hint": "Introduction, paragraph 2"}, {"name": "Conformal factuality framework connecting entailment sets and conformal prediction", "description": "The authors propose a framework that defines uncertainty sets as the entailment set of an LM's output, establishing that correctness of an output is equivalent to the reference being contained in this entailment set, thereby enabling conformal prediction techniques to provide probabilistic correctness guarantees.", "author_claim_text": "We propose conformal factuality, a framework that can ensure high probability correctness guarantees for LMs by connecting language modeling and conformal prediction.", "source_hint": "Abstract, sentence 2"}, {"name": "Back-off algorithm using progressively less specific outputs", "description": "The method constructs a sequence of increasingly generic outputs by removing uncertain sub-claims, then uses conformal prediction to select a specificity level that is correct with high probability, applying standard split conformal prediction techniques.", "author_claim_text": "We show that conformal prediction in language models corresponds to a back-off algorithm that provides high probability correctness guarantees by progressively making LM outputs less specific.", "source_hint": "Abstract, sentence 3"}]}, "abs": "Guaranteeing the correctness and factuality of language model (LM) outputs is a major open problem. In this work, we propose conformal factuality, a framework that can ensure high probability correctness guarantees for LMs by connecting language modeling and conformal prediction. We observe that the correctness of an LM output is equivalent to an uncertainty quantification problem, where the uncertainty sets are defined as the entailment set of an LM's output. Using this connection, we show that conformal prediction in language models corresponds to a back-off algorithm that provides high probability correctness guarantees by progressively making LM outputs less specific (and expanding the associated uncertainty sets). This approach applies to any black-box LM and requires very few human-annotated samples. Evaluations of our approach on closed book QA (FActScore, NaturalQuestions) and reasoning tasks (MATH) show that our approach can provide 80-90% correctness guarantees while retaining the majority of the LM's original output."}, {"title": "Enhancing Traffic Prediction with Textual Data Using Large Language Models", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Applications/Chatbot_and_Textual/Enhancing Traffic Prediction with Textual Data Using Large Language Models.pdf", "summary": {"paper_id": "survey-28_paper-3_Enhancing Traffic Prediction with Textual Data Using Large Language Models", "core_task": {"text": "Traffic prediction using textual data and large language models", "query_variants": ["Traffic prediction using textual data and large language models", "Forecasting traffic patterns with text-based information and LLMs", "Transportation flow prediction leveraging textual inputs and large language models"]}, "contributions": [{"name": "Inadequate handling of exceptional circumstances and textual contextual information in traffic prediction", "description": "Current traffic prediction models struggle with two key issues: they cannot effectively handle exceptional events (extreme weather, concerts, sports matches) that are underrepresented in training data, and they have difficulty integrating non-numerical contextual information such as detailed weather data that exists in textual formats rather than simple binary embeddings.", "author_claim_text": "Existing researches into short-term traffic prediction, however, face challenges in adequately addressing exceptional circumstances and integrating non-numerical contextual information like weather into models.", "source_hint": "Abstract"}, {"name": "Leveraging LLM world knowledge while avoiding direct prediction drawbacks", "description": "The authors are motivated to exploit the world knowledge embedded in large language models to address exceptional circumstances and textual information, while avoiding the problems of direct LLM-based prediction including high computational costs, non-deterministic outputs, and poor mathematical reasoning capabilities that make LLMs unsuitable for direct numerical forecasting tasks.", "author_claim_text": "Large language models offer a promising solution due to their inherent world knowledge. However, directly using them for traffic prediction presents drawbacks such as high cost, lack of determinism, and limited mathematical capability.", "source_hint": "Abstract and Introduction"}, {"name": "LLM-based textual embedding integration via auxiliary nodes", "description": "The proposed method uses large language models to convert textual contextual information into embedding vectors rather than for direct prediction. These embeddings undergo dimensionality reduction via PCA and are integrated into traditional spatiotemporal models as auxiliary nodes: regional-level information becomes a node connected to all network nodes, while node-level information connects only to corresponding specific nodes.", "author_claim_text": "Instead of directly employing large models for prediction, it utilizes them to process textual information and obtain embeddings. These embeddings are then combined with historical traffic data and inputted into traditional spatiotemporal forecasting models.", "source_hint": "Abstract and Method section 2.2-2.3"}, {"name": "Regional-level and node-level special scenario modeling", "description": "The approach distinguishes between two types of special scenarios: regional-level events (holidays, extreme weather affecting entire cities) are modeled as auxiliary nodes connected to all graph nodes, while node-level events (concerts, sports games at specific locations) are modeled as auxiliary nodes connected only to their corresponding district nodes in the traffic network graph.", "author_claim_text": "For regional-level scenarios, textual information is represented as a node connected to the entire network. For node-level scenarios, embeddings from the large model represent additional nodes connected only to corresponding nodes.", "source_hint": "Abstract and Method section 2.2-2.3"}]}, "abs": "Traffic prediction is pivotal for rational transportation supply scheduling and allocation. Existing researches into short-term traffic prediction, however, face challenges in adequately addressing exceptional circumstances and integrating non-numerical contextual information like weather into models. While, Large language models offer a promising solution due to their inherent world knowledge. However, directly using them for traffic prediction presents drawbacks such as high cost, lack of determinism, and limited mathematical capability. To mitigate these issues, this study proposes a novel approach. Instead of directly employing large models for prediction, it utilizes them to process textual information and obtain embeddings. These embeddings are then combined with historical traffic data and inputted into traditional spatiotemporal forecasting models. The study investigates two types of special scenarios: regional-level and node-level. For regional-level scenarios, textual information is represented as a node connected to the entire network. For node-level scenarios, embeddings from the large model represent additional nodes connected only to corresponding nodes. This approach shows a significant improvement in prediction accuracy according to our experiment of New York Bike dataset."}, {"title": "Leveraging Uncertainty Estimation for Efficient LLM Routing", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Applications/Chatbot_and_Textual/Leveraging Uncertainty Estimation for Efficient LLM Routing.pdf", "summary": {"paper_id": "survey-28_paper-4_Leveraging Uncertainty Estimation for Efficient LLM Routing", "core_task": {"text": "efficient LLM routing in edge-cloud environments using uncertainty estimation", "query_variants": ["efficient LLM routing in edge-cloud environments using uncertainty estimation", "uncertainty-based routing of large language models across edge and cloud infrastructure", "LLM request routing between edge and cloud using uncertainty quantification"]}, "contributions": [{"name": "Inefficient LLM routing in edge-cloud environments", "description": "The paper addresses the challenge of determining when to offload queries from edge devices to cloud-based LLMs. Existing routing methods rely on human preference data or accuracy metrics, which are either subjective and resource-intensive or incomplete as performance indicators, failing to capture response confidence and quality.", "author_claim_text": "Deploying large language models (LLMs) in edge-cloud environments requires an efficient routing strategy to balance cost and response quality. Traditional approaches prioritize either human-preference data or accuracy metrics from benchmark datasets as routing criteria, but these methods suffer from rigidity and subjectivity.", "source_hint": "Abstract"}, {"name": "Balancing cost efficiency and response quality in hybrid deployments", "description": "The authors are motivated by the need to optimize hybrid edge-cloud AI deployments where small models run on-device and larger models remain in the cloud. Effective routing is essential to minimize unnecessary cloud calls and latency while maintaining high response quality for complex queries.", "author_claim_text": "This setup provides a balance between efficiency and performance, allowing low-latency responses for simple queries while reserving cloud-based LLMs for more complex tasks. However, determining when to offload queries to the cloud is a crucial challenge: calling the cloud unnecessarily increases cost and latency, whereas over-relying on local SLMs risks suboptimal response quality.", "source_hint": "Introduction paragraph 1"}, {"name": "Confidence-Driven LLM Router using semantic entropy", "description": "The proposed framework uses semantic entropy as an uncertainty measure to guide routing decisions. It clusters semantically equivalent generated outputs and computes entropy over their aggregated probabilities, enabling the router to offload queries to cloud-based LLMs when higher certainty is needed while keeping confident responses on-device.", "author_claim_text": "we propose the Confidence-Driven LLM Router, a novel framework that leverages uncertainty estimation to optimize routing decisions. Instead of relying on human preferences or accuracy-based thresholds, our system uses semantic entropy to measure model confidence.", "source_hint": "Abstract and Introduction paragraph 4"}, {"name": "Three-phase router training system", "description": "The system design includes three phases: preparing training data from factual datasets, creating preference data by comparing semantic entropy scores across models with a normalized difference metric and threshold, and training router classifiers using vectorized instruction representations from the preference dataset.", "author_claim_text": "The training and deployment of the Confidence-Driven LLM Router consist of three key phases: Phase 1: Router Data Preparation. Phase 2: Constructing Preference Data from Semantic Entropy Scores. Phase 3: Training the Confidence-Driven Router.", "source_hint": "Method Section 2.2"}]}, "abs": "Deploying large language models (LLMs) in edge-cloud environments requires an efficient routing strategy to balance cost and response quality. Traditional approaches prioritize either human-preference data or accuracy metrics from benchmark datasets as routing criteria, but these methods suffer from rigidity and subjectivity. Moreover, existing routing frameworks primarily focus on accuracy and cost, neglecting response quality from a human preference perspective. In this work, we propose the Confidence-Driven LLM Router, a novel framework that leverages uncertainty estimation to optimize routing decisions. To comprehensively assess routing performance, we evaluate both system cost efficiency and response quality. In particular, we introduce the novel use of LLM-as-a-Judge to simulate human rating preferences, providing the first systematic assessment of response quality across different routing strategies. Extensive experiments on MT-Bench, GSM8K, and MMLU demonstrate that our approach outperforms state-of-the-art routing methods, achieving superior response quality while maintaining cost efficiency."}, {"title": "Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Applications/Chatbot_and_Textual/Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models.pdf", "summary": {"paper_id": "survey-28_paper-5_Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models", "core_task": {"text": "jailbreak attacks on large vision-language models", "query_variants": ["jailbreak attacks on large vision-language models", "adversarial jailbreaking methods for vision-language foundation models", "security exploits targeting multimodal large language models"]}, "contributions": [{"name": "Vulnerability of LVLMs to jailbreak attacks despite safety mechanisms", "description": "The paper addresses the critical security threat that current LVLMs can be exploited through jailbreak attacks to generate harmful content, bypassing existing safety constraints. Existing black-box methods are highly detectable and inefficient.", "author_claim_text": "Large Vision-Language Models (LVLMs) demonstrate exceptional performance across multimodal tasks, yet remain vulnerable to jailbreak attacks that bypass built-in safety mechanisms to elicit restricted content generation.", "source_hint": "Abstract, first sentence"}, {"name": "Need for advanced red-teaming to identify LVLM vulnerabilities", "description": "The authors are motivated by the necessity to proactively discover and understand attack vectors in LVLMs through red-teaming, enabling researchers to build stronger defenses before malicious actors can exploit these systems in real-world deployments.", "author_claim_text": "the development of advanced jailbreak attacks is essential for red-teaming LVLM systems—by proactively identifying and understanding potential attack vectors, researchers can develop more robust defences and mitigate vulnerabilities before malicious exploitation occurs.", "source_hint": "Introduction, paragraph 2"}, {"name": "Cross-modal Adversarial Multimodal Obfuscation (CAMO) framework", "description": "CAMO is a black-box jailbreak framework that splits harmful instructions into benign-looking textual and visual components. These fragments are reconstructed by LVLMs through multi-step cross-modal reasoning, evading conventional detection while maintaining attack effectiveness.", "author_claim_text": "we present Cross-modal Adversarial Multimodal Obfuscation (CAMO), a novel black-box jailbreak attack framework that decomposes malicious prompts into semantically benign visual and textual fragments.", "source_hint": "Abstract, second paragraph"}, {"name": "Four-stage attack pipeline with keyword selection and decomposition", "description": "The method operates through four stages: identifying sensitive keywords via POS tagging and dictionaries, decomposing keywords into partially masked text and symbolically encoded visual puzzles, combining them into multimodal prompts, and dynamically adjusting obfuscation difficulty.", "author_claim_text": "CAMO operates in four structured stages: 1) It first identifies candidate sensitive keywords from the input using part-of-speech (POS) tagging and a domain-specific dictionary. 2) It then decomposes these keywords into two components", "source_hint": "Introduction, paragraph 4"}]}, "abs": "Large Vision-Language Models (LVLMs) demonstrate exceptional performance across multimodal tasks, yet remain vulnerable to jailbreak attacks that bypass built-in safety mechanisms to elicit restricted content generation. Existing black-box jailbreak methods primarily rely on adversarial textual prompts or image perturbations, yet these approaches are highly detectable by standard content filtering systems and exhibit low query and computational efficiency. In this work, we present Cross-modal Adversarial Multimodal Obfuscation (CAMO), a novel black-box jailbreak attack framework that decomposes malicious prompts into semantically benign visual and textual fragments. By leveraging LVLMs' cross-modal reasoning abilities, CAMO covertly reconstructs harmful instructions through multi-step reasoning, evading conventional detection mechanisms. Our approach supports adjustable reasoning complexity and requires significantly fewer queries than prior attacks, enabling both stealth and efficiency. Comprehensive evaluations conducted on leading LVLMs validate CAMO's effectiveness, showcasing robust performance and strong cross-model transferability. These results underscore significant vulnerabilities in current built-in safety mechanisms, emphasizing an urgent need for advanced, alignment-aware security and safety solutions in vision-language systems."}, {"title": "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Applications/Chatbot_and_Textual/How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions.pdf", "summary": {"paper_id": "survey-28_paper-6_How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions", "core_task": {"text": "Lie detection in black-box large language models", "query_variants": ["Lie detection in black-box large language models", "Identifying deception in opaque LLMs", "Detecting falsehoods in non-interpretable large language models"]}, "contributions": [{"name": "Detecting lies in black-box LLMs without ground-truth knowledge", "description": "The paper addresses the challenge of detecting when LLMs output false statements despite demonstrably knowing the truth, particularly in black-box settings where model activations are inaccessible and ground-truth knowledge is unavailable.", "author_claim_text": "Large language models (LLMs) can 'lie', which we define as outputting false statements despite 'knowing' the truth in a demonstrable sense. LLMs might 'lie', for example, when instructed to output misinformation.", "source_hint": "Abstract"}, {"name": "Reducing risks from lying LLMs in real-world scenarios", "description": "The authors are motivated by the extensive risks posed by lying LLMs, including malicious uses by bad actors for scams and disinformation, and unintentional lying learned through training. Automated lie detection could mitigate these risks similar to how spam filters work.", "author_claim_text": "Automated lie detection could reduce the risks from lying models, just as automated spam filters have reduced the inconvenience of spam. The risks of lying LLMs are extensive", "source_hint": "Introduction §1"}, {"name": "Black-box lie detector using unrelated elicitation questions", "description": "The method asks a fixed set of yes/no elicitation questions unrelated to the suspected lie, then feeds the LLM's answers into a logistic regression classifier trained on GPT-3.5 examples to detect lies without requiring access to model activations.", "author_claim_text": "The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM's yes/no answers into a logistic regression classifier.", "source_hint": "Abstract"}, {"name": "Question-answering dataset for lie generation and detection", "description": "The authors create a comprehensive dataset of over 20,000 questions covering diverse topics including general knowledge, mathematics, reasoning, translations, self-knowledge, and synthetic facts to generate and test lie detection across various contexts.", "author_claim_text": "We collate a diverse question-answering dataset (>20,000 questions) to test lie detection across a range of topics, encompassing general knowledge, mathematics, common sense reasoning, translations, and self-knowledge.", "source_hint": "Section 4.1"}]}, "abs": "Large language models (LLMs) can \"lie\", which we define as outputting false statements despite \"knowing\" the truth in a demonstrable sense. LLMs might \"lie\", for example, when instructed to output misinformation. Here, we develop a simple lie detector that requires neither access to the LLM's activations (black-box) nor ground-truth knowledge of the fact in question. The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM's yes/no answers into a logistic regression classifier. Despite its simplicity, this lie detector is highly accurate and surprisingly general. When trained on examples from a single setting -- prompting GPT-3.5 to lie about factual questions -- the detector generalises out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie, (3) sycophantic lies, and (4) lies emerging in real-life scenarios such as sales. These results indicate that LLMs have distinctive lie-related behavioural patterns, consistent across architectures and contexts, which could enable general-purpose lie detection."}, {"title": "Probabilistically Correct Language-based Multi-Robot Planning using Conformal Prediction", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Applications/Robotics/Probabilistically Correct Language-based Multi-Robot Planning using Conformal Prediction.pdf", "summary": {"paper_id": "survey-28_paper-7_Probabilistically Correct Language-based Multi-Robot Planning using Conformal Prediction", "core_task": {"text": "language-based multi-robot task planning with probabilistic correctness guarantees", "query_variants": ["language-based multi-robot task planning with probabilistic correctness guarantees", "natural language multi-agent task planning with probabilistic correctness assurances", "linguistic multi-robot mission planning with stochastic correctness guarantees"]}, "contributions": [{"name": "Lack of performance guarantees in LLM-based multi-robot planning", "description": "Existing LLM-based multi-robot planners cannot provide guarantees on mission success rates and tend to generate incorrect outputs with high confidence, lacking mechanisms to reason about their inherent uncertainty.", "author_claim_text": "A major challenge with current LLM-based planners is that they typically lack mission performance and safety guarantees while they often hallucinate, i.e., they confidently generate incorrect and possibly unsafe outputs.", "source_hint": "Introduction, paragraph 3"}, {"name": "Achieving user-specified mission success rates for language-instructed robot teams", "description": "The authors aim to develop planners that can guarantee desired mission completion rates by enabling robots to reason about LLM uncertainties, allowing them to make decisions when confident and seek help otherwise.", "author_claim_text": "Our overarching goal is to design language-based planners capable of computing multi-robot plans (defined as sequences of robot actions) with a user-specified probability of correctness; this ensures desired mission success rates assuming successful plan execution.", "source_hint": "Introduction, paragraph 4"}, {"name": "S-ATLAS: Distributed LLM-based planner with conformal prediction", "description": "The proposed planner uses conformal prediction to construct local prediction sets for each robot in a distributed fashion, enabling robots to determine when they are uncertain about decisions and need to seek assistance from teammates or users.", "author_claim_text": "We propose a new distributed LLM-based planner, called S-ATLAS for Safe plAnning for Teams of Language-instructed AgentS, that is capable of achieving user-defined mission success rates. This is accomplished by leveraging conformal prediction (CP), a distribution-free uncertainty quantification tool in black-box models.", "source_hint": "Abstract and Introduction, paragraph 5"}, {"name": "Multiple-choice question-answering framework for action selection", "description": "The planner frames action selection as MCQA problems where LLMs choose from valid robot skills, mitigating hallucination risks by ensuring only valid actions are considered as choices.", "author_claim_text": "To choose an action for a robot, we present the action selection problem to the corresponding LLM agent as a multiple-choice question-answering (MCQA) scenario. Here, the 'question' corresponds to the textual task description and the history of past decisions, while the set of available 'choices' represents the skills that the selected robot can apply.", "source_hint": "Introduction, paragraph 6"}]}, "abs": "This paper addresses task planning problems for language-instructed robot teams. Tasks are expressed in natural language (NL), requiring the robots to apply their capabilities at various locations and semantic objects. Several recent works have addressed similar planning problems by leveraging pre-trained Large Language Models (LLMs) to design effective multi-robot plans. However, these approaches lack performance guarantees. To address this challenge, we introduce a new distributed LLM-based planner, called S-ATLAS for Safe plAnning for Teams of Language-instructed AgentS, that is capable of achieving user-defined mission success rates. This is accomplished by leveraging conformal prediction (CP), a distribution-free uncertainty quantification tool in black-box models. CP allows the proposed multi-robot planner to reason about its inherent uncertainty in a distributed fashion, enabling robots to make individual decisions when they are sufficiently certain and seek help otherwise. We show, both theoretically and empirically, that the proposed planner can achieve user-specified task success rates, assuming successful plan execution, while minimizing the overall number of help requests. We provide comparative experiments against related works showing that our method is significantly more computational efficient and achieves lower help rates. The advantage of our algorithm over baselines becomes more pronounced with increasing robot team size."}, {"title": "Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Applications/Robotics/Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning.pdf", "summary": {"paper_id": "survey-28_paper-8_Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning", "core_task": {"text": "uncertainty quantification for black-box large language models in decision planning", "query_variants": ["uncertainty quantification for black-box large language models in decision planning", "estimating uncertainty in opaque LLM systems for planning tasks", "quantifying confidence of black-box language models in decision-making scenarios"]}, "contributions": [{"name": "Uncertainty quantification for black-box LLMs in decision planning", "description": "Current uncertainty quantification methods for large language models either require access to internal model components (white-box) or involve high computational costs through multiple inferences, preventing practical use of proprietary black-box LLMs like GPT-4 for AI agent development under budget constraints.", "author_claim_text": "Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets.", "source_hint": "Abstract"}, {"name": "Enabling cost-efficient AI agent development with proprietary LLMs", "description": "The authors aim to enable practical AI agent development using powerful proprietary language models by providing an efficient uncertainty quantification method that works without requiring access to model internals or expensive multiple inferences, thus making black-box LLMs viable for step-by-step decision planning applications.", "author_claim_text": "our uncertainty estimation and decision-making agent design offer a cost-efficient approach for AI agent development.", "source_hint": "Abstract, final sentence"}, {"name": "Non-parametric point-wise dependency neural estimation", "description": "The authors propose using point-wise dependency neural estimation to compute the ratio p(x,y)/(p(x)p(y)) between user prompts/history and actions through a single inference from an auxiliary neural network, without requiring access to token logits. This provides a statistically interpretable measure where values above 1.0 indicate correlation.", "author_claim_text": "The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits.", "source_hint": "Abstract, Introduction"}, {"name": "Decision-making agent with conformal prediction", "description": "The authors design a three-stage system (data collection, model training, deployment) that combines instruction-fine-tuned LLMs with point-wise dependency estimation and conformal prediction to generate actions step-by-step, requesting user input when multiple high-confidence actions exist and stopping when no actions exceed the confidence threshold.", "author_claim_text": "The second contribution outlines a systematic design for a decision-making agent, generating actions like 'turn on the bathroom light' based on user prompts such as 'take a bath'.", "source_hint": "Abstract, Introduction"}]}, "abs": "Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our uncertainty estimation and decision-making agent design offer a cost-efficient approach for AI agent development."}, {"title": "Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Applications/Robotics/Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity.pdf", "summary": {"paper_id": "survey-28_paper-9_Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity", "core_task": {"text": "aligning language model uncertainty with task ambiguity in robot planning", "query_variants": ["aligning language model uncertainty with task ambiguity in robot planning", "matching LM confidence to task uncertainty for robotic planning", "calibrating language model certainty with ambiguous tasks in robotics"]}, "contributions": [{"name": "LLM hallucination and ambiguity in robot planning", "description": "The paper addresses the challenge that large language models used for robot planning can hallucinate and generate unsafe or misaligned plans, while natural language ambiguity introduces uncertainty that models fail to properly calibrate, leading to overconfident or incorrect decisions.", "author_claim_text": "LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or even unsafe in critical scenarios. Additionally, inherent ambiguity in natural language instructions can introduce uncertainty into the LLM's reasoning and planning processes.", "source_hint": "Abstract"}, {"name": "Need for uncertainty alignment with task ambiguity", "description": "The authors are motivated by the need for robots to accurately assess their own uncertainty in ways that align with inherent task ambiguity, enabling them to either execute correct actions or appropriately request clarification from users in safety-critical scenarios.", "author_claim_text": "The robot needs to calibrate its uncertainty quantification and seek further communication with users when ambiguities are identified.", "source_hint": "Introduction paragraph 2"}, {"name": "Introspective planning with knowledge base construction", "description": "The method constructs a knowledge base by prompting LLMs to generate post-hoc reasoning explanations conditioned on human-provided valid action labels. During inference, relevant reasoning examples are retrieved to guide the LLM in explicitly reasoning about task compliance and safety for multiple candidate plans.", "author_claim_text": "We propose introspective planning, a systematic approach that align LLM's uncertainty with the inherent ambiguity of the task. Our approach constructs a knowledge base containing introspective reasoning examples as post-hoc rationalizations of human-selected safe and compliant plans, which are retrieved during deployment.", "source_hint": "Abstract and Section 2"}, {"name": "Integration with conformal prediction", "description": "The authors integrate introspective planning with conformal prediction to provide statistical guarantees that the correct action is included in the prediction set with a specified confidence level, while reducing conservativeness and unnecessary queries compared to prior conformal prediction approaches.", "author_claim_text": "introspective planning, in combination with conformal prediction, achieves tighter confidence bounds, maintaining statistical success guarantees while minimizing unnecessary user clarification requests.", "source_hint": "Abstract and Section 3"}]}, "abs": "Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or even unsafe in critical scenarios. Additionally, inherent ambiguity in natural language instructions can introduce uncertainty into the LLM's reasoning and planning this http URL propose introspective planning, a systematic approach that align LLM's uncertainty with the inherent ambiguity of the task. Our approach constructs a knowledge base containing introspective reasoning examples as post-hoc rationalizations of human-selected safe and compliant plans, which are retrieved during deployment. Evaluations on three tasks, including a newly introduced safe mobile manipulation benchmark, demonstrate that introspection substantially improves both compliance and safety over state-of-the-art LLM-based planning methods. Furthermore, we empirically show that introspective planning, in combination with conformal prediction, achieves tighter confidence bounds, maintaining statistical success guarantees while minimizing unnecessary user clarification requests. The webpage and code are accessible at this https URL."}, {"title": "Knowing When to Ask -- Bridging Large Language Models and Data", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Applications/Robotics/Knowing When to Ask -- Bridging Large Language Models and Data.pdf", "summary": {"paper_id": "survey-28_paper-10_Knowing When to Ask -- Bridging Large Language Models and Data", "core_task": {"text": "Enhancing factual accuracy of large language models with statistical data retrieval", "query_variants": ["Enhancing factual accuracy of large language models with statistical data retrieval", "Improving factual correctness in LLMs through statistical information retrieval", "Boosting factual precision of large language models using statistical data access"]}, "contributions": [{"name": "LLM hallucinations on numerical and statistical facts", "description": "The paper addresses the challenge that LLMs frequently produce factually incorrect statements (hallucinations) when answering queries involving numerical, statistical, or timely data, lacking accurate citations and sufficient factual coverage in training data.", "author_claim_text": "Large Language Models (LLMs) are prone to generating factually incorrect information when responding to queries that involve numerical and statistical data or other timely facts.", "source_hint": "Abstract"}, {"name": "Need for trustworthy LLMs grounded in verifiable data", "description": "The authors aim to improve LLM trustworthiness and reliability by grounding model outputs in verifiable public statistical data, enabling complex factual reasoning with authoritative sources rather than relying solely on parametric knowledge.", "author_claim_text": "Our work represents an early step towards building more trustworthy and reliable LLMs that are grounded in verifiable statistical data and capable of complex factual reasoning.", "source_hint": "Abstract"}, {"name": "Retrieval Interleaved Generation (RIG)", "description": "A tool-inspired approach where the LLM is fine-tuned to generate natural language Data Commons queries alongside statistics. A pipeline converts these queries into structured data queries to retrieve authoritative values from Data Commons, which are presented alongside LLM-generated statistics for fact-checking.", "author_claim_text": "Retrieval Interleaved Generation (RIG), where the LLM is trained to produce natural language queries to retrieve data from Data Commons", "source_hint": "Abstract, Introduction"}, {"name": "Retrieval Augmented Generation (RAG)", "description": "A retrieval approach where a fine-tuned LLM extracts variables from user queries, retrieves relevant tables from Data Commons, and augments the original query with this context before prompting a long-context LLM (Gemini 1.5 Pro) to generate the final answer.", "author_claim_text": "Retrieval Augmented Generation (RAG), where relevant data tables are fetched from Data Commons and used to augment the LLM's prompt", "source_hint": "Abstract, Introduction"}]}, "abs": "Large Language Models (LLMs) are prone to generating factually incorrect information when responding to queries that involve numerical and statistical data or other timely facts. In this paper, we present an approach for enhancing the accuracy of LLMs by integrating them with Data Commons, a vast, open-source repository of public statistics from trusted organizations like the United Nations (UN), Center for Disease Control and Prevention (CDC) and global census bureaus. We explore two primary methods: Retrieval Interleaved Generation (RIG), where the LLM is trained to produce natural language queries to retrieve data from Data Commons, and Retrieval Augmented Generation (RAG), where relevant data tables are fetched from Data Commons and used to augment the LLM's prompt. We evaluate these methods on a diverse set of queries, demonstrating their effectiveness in improving the factual accuracy of LLM outputs. Our work represents an early step towards building more trustworthy and reliable LLMs that are grounded in verifiable statistical data and capable of complex factual reasoning."}, {"title": "Evaluating Uncertainty-based Failure Detection for Closed-Loop LLM Planners", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Applications/Robotics/Evaluating Uncertainty-based Failure Detection for Closed-Loop LLM Planners.pdf", "summary": {"paper_id": "survey-28_paper-11_Evaluating Uncertainty-based Failure Detection for Closed-Loop LLM Planners", "core_task": {"text": "uncertainty-based failure detection for closed-loop LLM planning in robotic manipulation", "query_variants": ["uncertainty-based failure detection for closed-loop LLM planning in robotic manipulation", "uncertainty-driven error identification in iterative LLM-based robot manipulation planning", "failure recognition using uncertainty estimation for feedback-driven language model robotic control"]}, "contributions": [{"name": "Fragility and error-proneness of open-loop LLM-based planning", "description": "Existing LLM-based robot planners operate in open-loop mode, rigidly following initial plans without incorporating environmental feedback. This leads to unaddressed execution errors or plan deficiencies, potentially causing task failures.", "author_claim_text": "the open-loop nature of previous works makes LLM-based planning error-prone and fragile", "source_hint": "Abstract"}, {"name": "Need for trustworthy failure detection without unrealistic assumptions", "description": "Current failure detection methods either rely on task-specific heuristics with limited generalizability or assume that LLM/MLLM predictions are always trustworthy, which is problematic given the hallucination issues in these models. A more general and reliable approach is needed.", "author_claim_text": "failure detection approaches for closed-loop planning are often limited by task-specific heuristics or following an unrealistic assumption that the prediction is trustworthy all the time", "source_hint": "Abstract"}, {"name": "KnowLoop framework with uncertainty-based MLLM failure detector", "description": "The authors propose KnowLoop, a closed-loop planning framework that incorporates an uncertainty-based failure detection module using MLLMs. The framework is model-agnostic and filters out uncertain predictions, actively seeking human help when confidence is low.", "author_claim_text": "we attempt to mitigate these issues by introducing a framework for closed-loop LLM-based planning called KnowLoop, backed by an uncertainty-based MLLMs failure detector, which is agnostic to any used MLLMs or LLMs", "source_hint": "Abstract"}, {"name": "Three uncertainty quantification approaches for MLLMs", "description": "The authors evaluate three methods to quantify MLLM uncertainty: token probability, entropy, and self-explained confidence. These are assessed using three prompting strategies (direct via subgoal state comparison, direct via spatial relationship analysis, and indirect via next action prediction).", "author_claim_text": "we evaluate three different ways for quantifying the uncertainty of MLLMs, namely token probability, entropy, and self-explained confidence as primary metrics based on three carefully designed representative prompting strategies", "source_hint": "Abstract"}]}, "abs": "Recently, Large Language Models (LLMs) have witnessed remarkable performance as zero-shot task planners for robotic manipulation tasks. However, the open-loop nature of previous works makes LLM-based planning error-prone and fragile. On the other hand, failure detection approaches for closed-loop planning are often limited by task-specific heuristics or following an unrealistic assumption that the prediction is trustworthy all the time. As a general-purpose reasoning machine, LLMs or Multimodal Large Language Models (MLLMs) are promising for detecting failures. However, However, the appropriateness of the aforementioned assumption diminishes due to the notorious hullucination problem. In this work, we attempt to mitigate these issues by introducing a framework for closed-loop LLM-based planning called KnowLoop, backed by an uncertainty-based MLLMs failure detector, which is agnostic to any used MLLMs or LLMs. Specifically, we evaluate three different ways for quantifying the uncertainty of MLLMs, namely token probability, entropy, and self-explained confidence as primary metrics based on three carefully designed representative prompting strategies. With a self-collected dataset including various manipulation tasks and an LLM-based robot system, our experiments demonstrate that token probability and entropy are more reflective compared to self-explained confidence. By setting an appropriate threshold to filter out uncertain predictions and seek human help actively, the accuracy of failure detection can be significantly enhanced. This improvement boosts the effectiveness of closed-loop planning and the overall success rate of tasks."}, {"title": "Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Applications/Robotics/Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners.pdf", "summary": {"paper_id": "survey-28_paper-12_Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners", "core_task": {"text": "uncertainty quantification and alignment for large language model based robot planning", "query_variants": ["uncertainty quantification and alignment for large language model based robot planning", "quantifying uncertainty and aligning large language models for robotic planning tasks", "alignment and uncertainty estimation in LLM-driven robot task planning"]}, "contributions": [{"name": "Uncertainty alignment for LLM-based robot planners", "description": "The paper addresses the challenge that large language models used for robot planning tend to confidently hallucinate incorrect outputs, which is problematic when robots must operate reliably in ambiguous real-world environments. The core issue is that LLMs lack proper uncertainty quantification, leading to potentially unsafe or incorrect actions.", "author_claim_text": "One of the major challenges with current LLMs is their tendency to hallucinate, i.e., to confidently generate outputs that are plausible but incorrect and untethered from reality. Such false confidence in incorrect outputs poses a significant challenge to LLM-based planning in robotics.", "source_hint": "Introduction, paragraph 2"}, {"name": "Enabling robots to know when they don't know", "description": "The authors are motivated by the longstanding challenge of enabling robots to accurately model uncertainty so they can operate reliably in unstructured environments. They aim to allow robots to recognize when they are uncertain and appropriately seek human clarification, rather than confidently executing incorrect plans.", "author_claim_text": "How can we endow our robots with the ability to know when they don't know? Accurately modeling and accounting for uncertainty is a longstanding challenge towards robots that operate reliably in unstructured and novel environments.", "source_hint": "Introduction, paragraph 1"}, {"name": "KNOWNO framework using conformal prediction", "description": "The authors propose KNOWNO, a framework that applies conformal prediction theory to LLM-based robot planners. The method generates prediction sets of possible actions and triggers human help when the set is not a singleton, providing statistical guarantees on task success while minimizing the amount of human intervention required.", "author_claim_text": "We present KNOWNO, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed. KNOWNO builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help.", "source_hint": "Abstract"}, {"name": "Planning as multiple-choice question answering", "description": "The authors reformulate LLM planning as a multiple-choice question answering task. The LLM first generates diverse candidate next steps, then evaluates them through MCQA format. This eliminates length bias in probability estimates and aligns with LLM training objectives, enabling better uncertainty quantification through normalized scores.", "author_claim_text": "First, with a few-shot prompt that includes possible next steps in a few scenarios, the LLM generates a set of candidate next steps that are semantically different. Then the task of choosing among them is formatted as multiple-choice Q&A (MCQA).", "source_hint": "Section 2, Planning as multiple-choice Q&A"}]}, "abs": "Large language models (LLMs) exhibit a wide range of promising capabilities -- from step-by-step planning to commonsense reasoning -- that may provide utility for robots, but remain prone to confidently hallucinated predictions. In this work, we present KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed. KnowNo builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help in complex multi-step planning settings. Experiments across a variety of simulated and real robot setups that involve tasks with different modes of ambiguity (e.g., from spatial to numeric uncertainties, from human preferences to Winograd schemas) show that KnowNo performs favorably over modern baselines (which may involve ensembles or extensive prompt tuning) in terms of improving efficiency and autonomy, while providing formal assurances. KnowNo can be used with LLMs out of the box without model-finetuning, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models. Website: this https URL"}, {"title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Datasets_and_Benchmarks/Factuality_Analysis/TruthfulQA: Measuring How Models Mimic Human Falsehoods.pdf", "summary": {"paper_id": "survey-28_paper-13_TruthfulQA: Measuring How Models Mimic Human Falsehoods", "core_task": {"text": "measuring truthfulness of language model question answering", "query_variants": ["measuring truthfulness of language model question answering", "evaluating factual accuracy in LM responses to questions", "assessing veracity of language model answers to queries"]}, "contributions": [{"name": "Language models generate false statements mimicking human falsehoods", "description": "The paper addresses the problem that current language models frequently produce false statements, including subtle inaccuracies and hallucinations, which raises concerns about accidental misuse, blocking positive applications, and potential malicious misuse for deception.", "author_claim_text": "While recent language models are impressively fluent, they have a tendency to generate false statements. These range from subtle inaccuracies to wild hallucinations", "source_hint": "Introduction §1"}, {"name": "Need to quantify and measure model truthfulness to enable safer deployment", "description": "The authors are motivated by the need to systematically measure how truthful language models are across diverse contexts, as better measurement tools will facilitate development of more truthful models and help understand risks of deceptive model deployment.", "author_claim_text": "To address these concerns, it is valuable to quantify how truthful models are. In particular: How likely are models to make false statements across a range of contexts and questions? Better measurement will help in producing more truthful models", "source_hint": "Introduction §1"}, {"name": "TruthfulQA benchmark with 817 adversarially-designed questions", "description": "The authors propose TruthfulQA, a benchmark consisting of 817 questions across 38 categories, specifically designed to elicit 'imitative falsehoods' where models might generate false answers learned from imitating human texts containing misconceptions.", "author_claim_text": "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception", "source_hint": "Abstract"}, {"name": "GPT-judge automated evaluation metric", "description": "The authors developed GPT-judge, a finetuned GPT-3-6.7B model that classifies answers as true or false, achieving 90-96% validation accuracy and providing a cost-effective alternative to human evaluation for measuring truthfulness.", "author_claim_text": "We finetuned GPT-3 on human evaluations of whether an answer is true or false and achieved 90-96% accuracy on held-out models", "source_hint": "Abstract and Section 4.4"}]}, "abs": "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web."}, {"title": "FEVER: a large-scale dataset for Fact Extraction and VERification", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Datasets_and_Benchmarks/Factuality_Analysis/FEVER: a large-scale dataset for Fact Extraction and VERification.pdf", "summary": {"paper_id": "survey-28_paper-14_FEVER: a large-scale dataset for Fact Extraction and VERification", "core_task": {"text": "fact extraction and verification against textual sources", "query_variants": ["fact extraction and verification against textual sources", "extracting facts and verifying them from text documents", "fact mining and validation using textual evidence"]}, "contributions": [{"name": "Lack of large-scale datasets for textual claim verification", "description": "Existing datasets for claim verification are extremely small (e.g., the Fake News Challenge used only 300 claims), which is orders of magnitude smaller than datasets available for related tasks like textual entailment and question answering, limiting progress in developing verification systems.", "author_claim_text": "despite the rising interest in verification and fact checking among researchers, the datasets currently used for this task are limited to a few hundred claims", "source_hint": "Introduction paragraph 3"}, {"name": "Need for verification systems against textual sources", "description": "The growing volume of textual information and ease of web sharing has created urgent demand for verification systems that can check claims against textual sources, with applications in journalism, scientific publications, product reviews, and other domains.", "author_claim_text": "The ever-increasing amounts of textual information available combined with the ease in sharing it through the web has increased the demand for verification", "source_hint": "Introduction paragraph 1"}, {"name": "Two-stage dataset construction process", "description": "The authors developed a two-stage annotation methodology where annotators first extract and mutate claims from Wikipedia sentences, then separately verify these claims by retrieving evidence and classifying them as SUPPORTED, REFUTED, or NOTENOUGHINFO without knowing the original source sentence.", "author_claim_text": "The dataset was constructed in two stages: Claim Generation Extracting information from Wikipedia and generating claims from it. Claim Labeling Classifying whether a claim is supported or refuted by Wikipedia", "source_hint": "Section 3 introduction"}, {"name": "Claim mutation framework with six operators", "description": "To avoid trivially verifiable claims, the authors introduced six mutation types inspired by Natural Logic Inference operators that allow annotators to generate altered versions of claims with varying verifiability, increasing dataset complexity in a controlled manner.", "author_claim_text": "we specified six types of mutation: paraphrasing, negation, substitution of an entity/relation with a similar/dissimilar one, and making the claim more general/specific", "source_hint": "Section 3.1 paragraph 4"}]}, "abs": "In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss $\\kappa$. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources."}, {"title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Datasets_and_Benchmarks/Factuality_Analysis/HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models.pdf", "summary": {"paper_id": "survey-28_paper-15_HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models", "core_task": {"text": "hallucination evaluation in large language models", "query_variants": ["hallucination evaluation in large language models", "assessing factual errors in LLMs", "measuring hallucination in large-scale language models"]}, "contributions": [{"name": "Lack of understanding of hallucination extent and types in LLMs", "description": "The paper addresses the gap in understanding which content types and to what degree large language models are prone to generating hallucinations, i.e., content conflicting with sources or unverifiable by factual knowledge.", "author_claim_text": "it still remains unclear what types of content and to which extent LLMs tend to hallucinate", "source_hint": "Introduction paragraph 2"}, {"name": "Hallucination poses deployment risks for LLMs in real-world applications", "description": "The authors are motivated by the fact that hallucinations in LLM outputs create safety and reliability concerns, hindering their practical deployment despite their impressive language capabilities.", "author_claim_text": "The issue of hallucination makes the deployment of LLMs potentially risky in real-world applications", "source_hint": "Introduction paragraph 2"}, {"name": "Two-stage sampling-then-filtering framework for automatic hallucination generation", "description": "The authors propose a two-step automated approach: first, diverse hallucination sampling using one-pass and conversational instruction schemas with ChatGPT; second, high-quality filtering using ChatGPT with ground-truth-enhanced instructions to select the most plausible and difficult hallucinated samples.", "author_claim_text": "To generate these samples automatically, we propose a two-stage framework, i.e., sampling-then-filtering", "source_hint": "Abstract and Section 2.1"}, {"name": "Human annotation of ChatGPT responses for general user queries", "description": "The authors employ human labelers to annotate whether ChatGPT responses to general user queries contain hallucinated information, using a pre-selection procedure based on response divergence to identify queries most likely to produce hallucinations.", "author_claim_text": "we hire some human labelers to annotate the hallucinations in ChatGPT responses", "source_hint": "Abstract and Section 2.2"}]}, "abs": "Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation benchmark for Large Language Models (HaluEval), a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about $19.5\\%$ responses). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. However, our experiments also prove that providing external knowledge or adding reasoning steps can help LLMs recognize hallucinations. Our benchmark can be accessed at this https URL."}, {"title": "Teaching Models to Express Their Uncertainty in Words", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Datasets_and_Benchmarks/Mathematics/Teaching Models to Express Their Uncertainty in Words.pdf", "summary": {"paper_id": "survey-28_paper-16_Teaching Models to Express Their Uncertainty in Words", "core_task": {"text": "teaching language models to express calibrated uncertainty in natural language", "query_variants": ["teaching language models to express calibrated uncertainty in natural language", "training LMs to communicate well-calibrated confidence through text", "instructing language models to convey properly calibrated uncertainty via natural language"]}, "contributions": [{"name": "Language models produce false statements without expressing uncertainty", "description": "Current state-of-the-art language models frequently generate false statements or hallucinations in long-form text, and users cannot distinguish when the model is truthful versus when it is not, reducing the model's practical value.", "author_claim_text": "Yet when models generate long-form text, they often produce false statements or 'hallucinations'. This reduces their value to human users, as users cannot tell when a model is being truthful or not.", "source_hint": "Introduction paragraph 1"}, {"name": "Calibrated uncertainty enables users to trust model statements appropriately", "description": "Calibration is motivated by the need for truthfulness: if models can express calibrated uncertainty about their statements, users can appropriately gauge trust in those statements, which is important for current hallucination-prone models and for statements where no ground truth exists.", "author_claim_text": "The problem of truthfulness motivates calibration for language models. If models convey calibrated uncertainty about their statements, then users know how much to trust a given statement.", "source_hint": "Introduction paragraph 2"}, {"name": "Finetuning GPT-3 to express verbalized probability in natural language", "description": "The authors finetune GPT-3 using supervised learning to express epistemic uncertainty about its own answers in natural language (called verbalized probability), where the model generates both an answer and a confidence level (e.g. '90% confidence' or 'high confidence') that maps to calibrated probabilities.", "author_claim_text": "In this paper, we finetune models to express epistemic uncertainty using natural language. We call this 'verbalized probability'.", "source_hint": "Introduction paragraph 3 and Abstract"}, {"name": "CalibratedMath test suite for evaluating calibration under distribution shift", "description": "The authors introduce CalibratedMath, a suite of 21 elementary mathematics tasks with programmatically generated questions and answers, designed to test how calibration generalizes under distribution shift by varying question types, difficulty levels, and whether questions have single or multiple correct answers.", "author_claim_text": "For testing calibration, we introduce the CalibratedMath suite of tasks.", "source_hint": "Abstract and Section 2.2"}]}, "abs": "We show that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits. When given a question, the model generates both an answer and a level of confidence (e.g. \"90% confidence\" or \"high confidence\"). These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift, and is sensitive to uncertainty in its own answers, rather than imitating human examples. To our knowledge, this is the first time a model has been shown to express calibrated uncertainty about its own answers in natural language. For testing calibration, we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words (\"verbalized probability\") to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT-3's ability to generalize calibration depends on pre-trained latent representations that correlate with epistemic uncertainty over its answers."}, {"title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Datasets_and_Benchmarks/Multi-Hop_Reasoning/HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.pdf", "summary": {"paper_id": "survey-28_paper-17_HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering", "core_task": {"text": "multi-hop question answering over natural language text", "query_variants": ["multi-hop question answering over natural language text", "multi-step reasoning for question answering on textual documents", "chain reasoning question answering from unstructured text"]}, "contributions": [{"name": "Limitations of existing QA datasets in training complex reasoning and explainability", "description": "Current QA datasets do not adequately test systems' ability to perform multi-hop reasoning across multiple documents, are constrained by knowledge base schemas, and provide only distant supervision without indicating which supporting facts lead to answers, hindering explainability.", "author_claim_text": "Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers.", "source_hint": "Abstract"}, {"name": "Need for diverse, explainable multi-hop reasoning in natural language", "description": "The authors seek to advance machine reasoning over natural language by creating a dataset that tests multi-hop reasoning without KB constraints and provides strong supervision through supporting facts, enabling systems to learn meaningful reasoning processes and make explainable predictions.", "author_claim_text": "we aim at creating a QA dataset that requires reasoning over multiple documents, and does so in natural language, without constraining itself to an existing knowledge base or knowledge schema. We also want it to provide the system with strong supervision about what text the answer is actually derived from", "source_hint": "Introduction paragraph 4"}, {"name": "Wikipedia hyperlink graph-based data collection pipeline", "description": "The authors construct a directed graph from Wikipedia hyperlinks and generate candidate paragraph pairs by sampling edges where the target entity serves as a bridge entity, facilitating the collection of meaningful multi-hop questions that require reasoning across multiple documents.", "author_claim_text": "we carefully design a pipeline to collect text-based multi-hop questions. We use the entire English Wikipedia dump as our corpus. We build a directed graph G, where each edge (a, b) indicates there is a hyperlink from the first paragraph of article a to article b.", "source_hint": "Section 2, Data Collection"}, {"name": "Collection of sentence-level supporting facts", "description": "The dataset includes sentence-level supporting facts annotated by crowd workers, which serve as strong supervision for training explainable QA systems and enable evaluation of model explainability by comparing predicted supporting facts to ground truth.", "author_claim_text": "To enhance the explainability of question answering systems, we want them to output a set of supporting facts necessary to arrive at the answer, when the answer is generated. To this end, we also collect the sentences that determine the answers from crowd workers.", "source_hint": "Section 2, Collecting Supporting Facts"}]}, "abs": "Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems' ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions."}, {"title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Datasets_and_Benchmarks/Multi-Hop_Reasoning/Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies.pdf", "summary": {"paper_id": "survey-28_paper-18_Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies", "core_task": {"text": "question answering with implicit multi-hop reasoning", "query_variants": ["question answering with implicit multi-hop reasoning", "multi-step reasoning for question answering tasks", "QA requiring implicit multi-step inference"]}, "contributions": [{"name": "Explicit reasoning steps in multi-hop QA datasets", "description": "Existing multi-hop reasoning datasets explicitly describe the reasoning process in the question language, whereas real-life questions often require implicit reasoning where the strategy must be inferred by the model.", "author_claim_text": "A key limitation in current datasets for multi-hop reasoning is that the required steps for answering the question are mentioned in it explicitly.", "source_hint": "Abstract, first sentence"}, {"name": "Need for implicit reasoning in QA systems", "description": "The authors are motivated by the gap between explicit multi-hop questions in existing benchmarks and real-world questions that require models to infer implicit reasoning strategies, posing challenges in retrieval and preventing exploitation of language shortcuts.", "author_claim_text": "in real-life questions, reasoning is often implicit. For example, the question 'Did Aristotle use a laptop?' can be answered using the same steps, but the model must infer the strategy for answering the question", "source_hint": "Introduction, paragraph 2"}, {"name": "Term-based priming for creative question generation", "description": "The authors propose priming annotators with single Wikipedia terms rather than full contexts to encourage creative question writing with minimal lexical overlap, reducing biases and shortcuts in the collected data.", "author_claim_text": "we prime crowd workers with random Wikipedia terms that serve as a minimal context to inspire their imagination and increase their creativity", "source_hint": "Introduction, paragraph 5"}, {"name": "Adversarial filtering with continuously updated models", "description": "The method involves deploying pre-trained and continuously fine-tuned models during data collection to reject questions that are too easy or exhibit recurring patterns, ensuring challenging and diverse questions.", "author_claim_text": "we continuously train adversarial models during data collection, slowly increasing the difficulty in question writing and preventing recurring patterns", "source_hint": "Introduction, paragraph 5; Section 3.1"}]}, "abs": "A key limitation in current datasets for multi-hop reasoning is that the required steps for answering the question are mentioned in it explicitly. In this work, we introduce StrategyQA, a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. A fundamental challenge in this setup is how to elicit such creative questions from crowdsourcing workers, while covering a broad range of potential strategies. We propose a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts. Moreover, we annotate each question with (1) a decomposition into reasoning steps for answering it, and (2) Wikipedia paragraphs that contain the answers to each step. Overall, StrategyQA includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Analysis shows that questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies. Empirically, we show that humans perform well (87%) on this task, while our best baseline reaches an accuracy of $\\sim$66%."}, {"title": "CoQA: A Conversational Question Answering Challenge", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Datasets_and_Benchmarks/Reading_Comprehension/CoQA: A Conversational Question Answering Challenge.pdf", "summary": {"paper_id": "survey-28_paper-19_CoQA: A Conversational Question Answering Challenge", "core_task": {"text": "conversational question answering", "query_variants": ["conversational question answering", "dialogue-based question answering systems", "multi-turn question answering in conversations"]}, "contributions": [{"name": "Lack of conversational question answering datasets", "description": "Existing reading comprehension datasets do not contain questions that depend on conversation history, making it difficult to build systems that can answer conversational questions where later questions rely on previous context.", "author_claim_text": "At present, there are no large-scale reading comprehension datasets which contain questions that depend on a conversation history", "source_hint": "Introduction §1"}, {"name": "Enabling machines to assist in information gathering through conversation", "description": "Since humans naturally gather information through conversational exchanges with interconnected questions and answers, machines need the ability to answer conversational questions to effectively assist in information gathering tasks.", "author_claim_text": "Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions.", "source_hint": "Abstract"}, {"name": "CoQA dataset with conversational questions and free-form answers", "description": "The authors introduce CoQA, a dataset containing 127k question-answer pairs from 8k conversations about passages from seven domains. Questions are conversational and answers are free-form text with corresponding evidence spans highlighted in passages.", "author_claim_text": "We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains.", "source_hint": "Abstract"}, {"name": "Data collection interface with questioner-answerer pairing", "description": "The authors employ a two-annotator setup where one acts as questioner and another as answerer, which enables natural dialogue flow, on-the-spot validation, and helps prevent spam while obtaining high agreement data.", "author_claim_text": "For each conversation, we employ two annotators, a questioner and an answerer. This setup has several advantages over using a single annotator to act both as a questioner and an answerer", "source_hint": "Introduction §3"}]}, "abs": "Humans gather information by engaging in conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning. We evaluate strong conversational and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 points behind human performance (88.8%), indicating there is ample room for improvement. We launch CoQA as a challenge to the community at this http URL"}, {"title": "Neural Text Generation from Structured Data with Application to the Biography Domain", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Datasets_and_Benchmarks/Reading_Comprehension/Neural Text Generation from Structured Data with Application to the Biography Domain.pdf", "summary": {"paper_id": "survey-28_paper-20_Neural Text Generation from Structured Data with Application to the Biography Domain", "core_task": {"text": "concept-to-text generation from structured data", "query_variants": ["concept-to-text generation from structured data", "generating natural language text from structured knowledge representations", "text generation from structured semantic inputs"]}, "contributions": [{"name": "Scaling concept-to-text generation to large, rich domains", "description": "Existing concept-to-text generation systems operate on small datasets with limited vocabularies (tens of thousands of records, ~350 words), whereas real-world applications like biography generation require handling much larger and more diverse data (over 700k records, 400k+ vocabulary).", "author_claim_text": "Previous work experimented with datasets that contain only a few tens of thousands of records... while our dataset contains over 700k biographies from Wikipedia. Furthermore, these datasets have a limited vocabulary of only about 350 words each, compared to over 400k words in our dataset.", "source_hint": "Introduction paragraph 2"}, {"name": "Addressing data sparsity in large-scale generation", "description": "The authors are motivated by the difficulty classical count-based models face when estimating probabilities of rare events in diverse, large-scale domains. They propose using embeddings and neural language models to achieve better generalization and handle data sparsity more effectively.", "author_claim_text": "Such diversity makes it difficult for classical count-based models to estimate probabilities of rare events due to data sparsity. We address this issue by parameterizing words and fields as embeddings, along with a neural language model operating on them.", "source_hint": "Introduction paragraph 3"}, {"name": "Table-conditioned neural language model with local and global conditioning", "description": "The authors propose a neural language model that conditions on structured table data through two mechanisms: local conditioning (describing previous words via their table field and position) and global conditioning (summarizing all table fields and words to capture high-level themes).", "author_claim_text": "We introduce a statistical generation model conditioned on a Wikipedia infobox... Our model exploits structured data both globally and locally. Global conditioning summarizes all information about a personality... while as local conditioning describes the previously generated tokens in terms of the their relationship to the infobox.", "source_hint": "Introduction paragraphs 3 and 5"}, {"name": "Copy actions for handling large vocabularies", "description": "The model extends the output vocabulary by introducing copy actions that allow it to emit special field tokens (e.g., name_1, name_2) which are then replaced by the corresponding words from the input table, enabling generation of out-of-vocabulary words.", "author_claim_text": "To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that transfer sample-specific words from the input database to the generated output sentence.", "source_hint": "Abstract"}]}, "abs": "This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with a new dataset of biographies from Wikipedia that is an order of magnitude larger than existing resources with over 700k samples. The dataset is also vastly more diverse with a 400k vocabulary, compared to a few hundred words for Weathergov or Robocup. Our model builds upon recent work on conditional neural language model for text generation. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that transfer sample-specific words from the input database to the generated output sentence. Our neural model significantly out-performs a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU."}, {"title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Datasets_and_Benchmarks/Reading_Comprehension/TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension.pdf", "summary": {"paper_id": "survey-28_paper-21_TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension", "core_task": {"text": "reading comprehension with distant supervision", "query_variants": ["reading comprehension with distant supervision", "distant supervision for reading comprehension tasks", "comprehension of text using distantly supervised methods"]}, "contributions": [{"name": "Limited challenge in existing reading comprehension datasets", "description": "Existing reading comprehension datasets are being solved too quickly by state-of-the-art models, which often reach near-human performance within a short time, creating a need for more challenging benchmarks that better test complex reasoning, lexical variation, and multi-sentence inference.", "author_claim_text": "system performance has improved rapidly as each resource is released. The best models often achieve near-human performance levels within months or a year, fueling a continual need to build ever more difficult datasets.", "source_hint": "Introduction paragraph 2"}, {"name": "Need for datasets testing multiple comprehension challenges simultaneously", "description": "The authors aim to create a dataset that simultaneously tests complex compositional questions, lexical and syntactic variability, and multi-sentence reasoning, addressing limitations of prior datasets that typically focus on only one challenge at a time.", "author_claim_text": "This paper presents TriviaQA, a new reading comprehension dataset designed to simultaneously test all of these challenges.", "source_hint": "Introduction paragraph 1"}, {"name": "Distant supervision through independent question-evidence collection", "description": "The method involves collecting trivia questions authored independently of evidence documents, then retrospectively gathering supporting evidence from Wikipedia and Web search results. This decoupling allows control over potential bias while providing organically generated questions across diverse topics.", "author_claim_text": "TriviaQA is the first dataset where full-sentence questions are authored organically (i.e. independently of an NLP task) and evidence documents are collected retrospectively from Wikipedia and the Web.", "source_hint": "Introduction paragraph 3"}, {"name": "Two-domain evidence collection strategy", "description": "The authors employ different distant supervision strategies for Web and Wikipedia domains: treating each question-answer-document tuple as independent for redundant Web results, while pooling all evidence documents for Wikipedia where facts typically appear once.", "author_claim_text": "we consider two types of distant supervision, depending on the source of our documents. For web search results, we expect the documents that contain the correct answer a to be highly redundant... However, in Wikipedia we generally expect most facts to be stated only once", "source_hint": "Overview section, paragraph 4"}]}, "abs": "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- this http URL"}, {"title": "Distinguishing the Knowable from the Unknowable with Language Models", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Taxonomy/Mechanistic_Interpretability/Distinguishing the Knowable from the Unknowable with Language Models.pdf", "summary": {"paper_id": "survey-28_paper-22_Distinguishing the Knowable from the Unknowable with Language Models", "core_task": {"text": "distinguishing epistemic from aleatoric uncertainty in language model outputs", "query_variants": ["distinguishing epistemic from aleatoric uncertainty in language model outputs", "separating knowledge uncertainty from inherent randomness in LM predictions", "differentiating model uncertainty from data uncertainty in neural language generation"]}, "contributions": [{"name": "Distinguishing epistemic from aleatoric uncertainty in LLM outputs", "description": "The paper addresses the challenge of determining whether uncertainty in language model predictions arises from lack of knowledge (epistemic) or inherent randomness in the data distribution (aleatoric), particularly in unconstrained free-form text generation where ground truth is unavailable.", "author_claim_text": "We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text.", "source_hint": "Abstract"}, {"name": "Reducing hallucinations by identifying epistemic uncertainty", "description": "The authors are motivated by the problem of LLM hallucinations, hypothesizing that many hallucinations occur when models sample tokens under epistemic uncertainty. Identifying such uncertainty could enable interventions to improve model truthfulness or highlight uncertain tokens in user interfaces.", "author_claim_text": "Our work is ultimately motivated by language model hallucinations. To the extent that hallucinations are a consequence of naively sampling tokens in the presence specifically of epistemic uncertainty, tagging instances of primarily epistemic uncertainty could be the first step of a new regimen to improve the truthfulness of LLM generations.", "source_hint": "Introduction §1"}, {"name": "Supervised linear probes on frozen model embeddings", "description": "The authors train small linear classifiers on intermediate embeddings from a smaller language model to predict when a larger model will exhibit high or low entropy. These probes are trained on labeled data generated by comparing small and large model predictions, and they generalize across text domains.", "author_claim_text": "We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others.", "source_hint": "Abstract"}, {"name": "In-Context Learning Test (ICLT) for unsupervised uncertainty classification", "description": "The authors propose ICLT, an unsupervised method that measures how much a model updates its predictions when provided with candidate tokens in context. The hypothesis is that models exhibit different in-context learning behaviors depending on whether their uncertainty is epistemic or aleatoric, being more suggestible under epistemic uncertainty.", "author_claim_text": "Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task.", "source_hint": "Abstract and Introduction §1"}]}, "abs": "We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more informative indicators of model confidence in diverse practical settings."}, {"title": "LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Taxonomy/Self-Verbalized_UQ/LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models.pdf", "summary": {"paper_id": "survey-28_paper-23_LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models", "core_task": {"text": "Calibrating large language models' confidence to align with listener interpretation", "query_variants": ["Calibrating large language models' confidence to align with listener interpretation", "Adjusting large language models' confidence to match listener understanding", "Aligning large language models' confidence with listener interpretation"]}, "contributions": [{"name": "Overconfidence in large language models", "description": "The paper addresses the issue of large language models (LLMs) being overconfident in their responses, which can mislead users and erode trust.", "author_claim_text": "For LLMs to be trustworthy sources of knowledge, the confidence they convey should match their actual expertise on a topic; however, this is currently not the case, with most models tending towards overconfidence.", "source_hint": "Abstract"}, {"name": "Improving trustworthiness of LLMs", "description": "The authors aim to make LLMs more trustworthy by ensuring their expressed confidence aligns with their actual correctness, as users increasingly depend on these models for information.", "author_claim_text": "Overconfidence is particularly troubling given that people are increasingly interacting with LLMs as sources of information; in other words, people rely on LLMs to answer questions they themselves do not know the answer to.", "source_hint": "Introduction §1"}, {"name": "Listener-aware finetuning (LACIE)", "description": "The authors propose LACIE, a finetuning method that optimizes LLMs to express confidence calibrated to how a listener perceives correctness, using a multi-agent speaker-listener framework.", "author_claim_text": "To calibrate both implicit and explicit confidence markers, we introduce a pragmatic, listener-aware finetuning method (LACIE) that directly models the listener, considering not only whether an answer is right, but whether it will be accepted by a listener.", "source_hint": "Abstract"}, {"name": "Multi-agent speaker-listener game", "description": "The method involves a two-agent game where a speaker model generates responses and a listener model evaluates them, creating preference data to optimize calibration.", "author_claim_text": "Specifically, we cast calibration as a preference optimization problem, creating data via a two-agent speaker-listener game, where a speaker model's outputs are judged by a simulated listener.", "source_hint": "Abstract"}]}, "abs": "When answering questions, LLMs can convey not only an answer, but a level of confidence about the answer being correct. This includes explicit confidence markers (e.g. giving a numeric score) as well as implicit markers, like an authoritative tone or elaborating with additional knowledge. For LLMs to be trustworthy knowledge sources, the confidence they convey should match their actual expertise; however, most current models tend towards overconfidence. To calibrate both implicit and explicit confidence markers, we introduce a pragmatic, listener-aware finetuning method (LACIE) that models the listener, considering not only whether an answer is right, but whether it will be accepted by a listener. We cast calibration as preference optimization, creating data via a two-agent game, where a speaker model's outputs are judged by a simulated listener. We then finetune three LLMs (Mistral-7B, Llama3-8B, Llama3-70B) with LACIE, and show that the resulting models are better calibrated w.r.t. a simulated listener. Crucially, these trends transfer to human listeners, helping them correctly predict model correctness: we conduct a human evaluation where annotators accept or reject an LLM's answers, finding that training with LACIE results in 47% fewer incorrect answers being accepted while maintaining the same level of acceptance for correct answers. Furthermore, LACIE generalizes to another dataset, resulting in a large increase in truthfulness on TruthfulQA when trained on TriviaQA. Our analysis indicates that LACIE leads to a better confidence separation between correct and incorrect examples. Qualitatively, we find that a LACIE-trained model hedges more and implicitly signals certainty when it is correct by using an authoritative tone or including details. Finally, LACIE finetuning leads to an emergent increase in model abstention (e.g. saying \"I don't know\") for answers that are likely wrong."}, {"title": "Linguistic Calibration of Long-Form Generations", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Taxonomy/Self-Verbalized_UQ/Linguistic Calibration of Long-Form Generations.pdf", "summary": {"paper_id": "survey-28_paper-24_Linguistic Calibration of Long-Form Generations", "core_task": {"text": "linguistic calibration of long-form language model generations", "query_variants": ["linguistic calibration of long-form language model generations", "calibrating confidence in extended text outputs from language models", "uncertainty estimation for multi-sentence language model predictions"]}, "contributions": [{"name": "Confident hallucinations in long-form LM generations lead to suboptimal user decisions", "description": "The paper addresses the problem that language models often hallucinate incorrect claims with complete confidence in long-form generations, causing users to make poor decisions. Existing calibration methods are restricted to classification or short outputs and cannot handle the multiple claims made in long-form generations.", "author_claim_text": "LMs may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce long-form text with calibrated confidence statements.", "source_hint": "Abstract"}, {"name": "Enabling better user decisions through calibrated confidence statements", "description": "The authors are motivated by the harmful consequences of confident hallucinations, particularly in high-stakes domains like medicine and law. By enabling LMs to convey calibrated confidence levels in natural language, users can make better decisions regardless of the LM's underlying knowledge gaps.", "author_claim_text": "Confident hallucinations are especially harmful. They decrease users' trust in the errant LM and broadly make LMs unsuitable for settings where factuality is paramount such as medicine and law. Perhaps most importantly, they lead the user to confidently make poor decisions.", "source_hint": "Introduction §1"}, {"name": "Definition of linguistic calibration for long-form generations", "description": "The authors define an LM emitting long-form generations to be linguistically calibrated if it enables users to produce calibrated forecasts relevant to their decision-making tasks. This definition reduces the problem of calibrating long-form text to calibrating probabilistic forecasts over a smaller answer space.", "author_claim_text": "Through the lens of decision-making, we define linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions.", "source_hint": "Abstract and Definition 2.1"}, {"name": "Two-step training framework with summary distillation and decision-based RL", "description": "The training framework first uses summary distillation to bootstrap an LM to express confidence statements via supervised finetuning on frequency-based summaries of multiple samples. Then, decision-based RL optimizes a proper scoring rule objective that rewards generations enabling calibrated user forecasts on related questions.", "author_claim_text": "This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as 'I estimate a 30% chance of...' or 'I am certain that...', followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions.", "source_hint": "Abstract and §3"}]}, "abs": "Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce long-form text with calibrated confidence statements. Through the lens of decision-making, we define linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as \"I estimate a 30% chance of...\" or \"I am certain that...\", followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions. We linguistically calibrate Llama 2 7B and find in automated and human evaluations of long-form generations that it is significantly more calibrated than strong finetuned factuality baselines with comparable accuracy. These findings generalize under significant domain shifts to scientific and biomedical questions and to an entirely held-out person biography generation task. Our results demonstrate that long-form generations may be calibrated end-to-end by constructing an objective in the space of the predictions that users make in downstream decision-making."}, {"title": "Reducing conversational agents' overconfidence through linguistic calibration", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Taxonomy/Self-Verbalized_UQ/Reducing conversational agents' overconfidence through linguistic calibration.pdf", "summary": {"paper_id": "survey-28_paper-25_Reducing conversational agents' overconfidence through linguistic calibration", "core_task": {"text": "linguistic calibration of conversational agents", "query_variants": ["linguistic calibration of conversational agents", "language alignment in dialogue systems", "verbal tuning of chatbot models"]}, "contributions": [{"name": "Poor linguistic calibration in neural dialogue agents", "description": "State-of-the-art conversational models express linguistic confidence (e.g., 'Obviously...') that does not match the actual likelihood their answers are factually correct, leading to overconfident incorrect responses that can mislead users.", "author_claim_text": "state-of-the-art chit-chat models are poorly calibrated, yet we show that likelihood of correctness can accurately be predicted", "source_hint": "Abstract"}, {"name": "Transparency about model ignorance in dialogue", "description": "While factual accuracy improvements are widely studied, the authors emphasize the importance of models being transparent about their limitations directly in dialogue, applying Grice's maxim of quality on a metacognitive level to be truthful about what the model knows.", "author_claim_text": "another important aspect of communication, less studied in the setting of neural dialogue, is transparency about ignorance", "source_hint": "Abstract and Introduction §1"}, {"name": "Calibrator for predicting answer correctness", "description": "A trained classifier that predicts the probability an answer is correct using the question, answer, and the vanilla model's internal encoder and decoder representations, without access to gold answers. Trained on 50,000 automatically annotated TriviaQA examples.", "author_claim_text": "we train a calibrator to return the empirical probability that the model's answer is correct (without seeing the gold answer)", "source_hint": "Introduction §1, Figure 1, and §4 'Training a calibrator'"}, {"name": "Two-stage controllable generation fine-tuning", "description": "A two-stage fine-tuning approach: Stage 1 trains a confidence-controllable model using control tokens (<DK>, <LO>, <HI>) on TriviaQA augmented with linguistic confidence labels. Stage 2 adds content control (<SAME>/<DIFF>) to adjust linguistic confidence without changing answer substance.", "author_claim_text": "We achieve this by fine-tuning the generative dialogue model in two steps using controllable conditioned generation techniques", "source_hint": "§4 'Training a controllable generation model'"}]}, "abs": "While improving neural dialogue agents' factual accuracy is the object of much research, another important aspect of communication, less studied in the setting of neural dialogue, is transparency about ignorance. In this work, we analyze to what extent state-of-the-art chit-chat models are linguistically calibrated in the sense that their verbalized expression of doubt (or confidence) matches the likelihood that the model's responses are factually incorrect (or correct). We find that these models are poorly calibrated, yet we show that likelihood of correctness can accurately be predicted. By incorporating such metacognitive features into the training of a controllable generation model, we obtain a dialogue agent with greatly improved linguistic calibration. While improving neural dialogue agents' factual accuracy is the object of much research, another important aspect of communication, less studied in the setting of neural dialogue, is transparency about ignorance. In this work, we analyze to what extent state-of-the-art chit-chat models are linguistically calibrated in the sense that their verbalized expression of doubt (or confidence) matches the likelihood that the model's responses are factually incorrect (or correct). We find that these models are poorly calibrated, yet we show that likelihood of correctness can accurately be predicted. By incorporating such metacognitive features into the training of a controllable generation model, we obtain a dialogue agent with greatly improved linguistic calibration."}, {"title": "When to Trust LLMs: Aligning Confidence with Response Quality", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Taxonomy/Self-Verbalized_UQ/When to Trust LLMs: Aligning Confidence with Response Quality.pdf", "summary": {"paper_id": "survey-28_paper-26_When to Trust LLMs: Aligning Confidence with Response Quality", "core_task": {"text": "aligning confidence with response quality in large language models", "query_variants": ["aligning confidence with response quality in large language models", "calibrating uncertainty estimates to match output accuracy in LLMs", "matching model certainty with generation correctness in large language models"]}, "contributions": [{"name": "Misalignment between LLM confidence and response quality", "description": "Current methods for expressing LLM reliability through confidence levels suffer from misalignment between the stated confidence and actual response quality, primarily due to the absence of objective standards that directly reflect response quality in generative tasks.", "author_claim_text": "existing methods often express reliability by confidence level, however, their effectiveness is limited by the lack of objective guidance", "source_hint": "Abstract"}, {"name": "Determining when to trust LLMs in safety-critical domains", "description": "The authors are motivated by the need to prevent uncritical acceptance of misleading information from LLMs and to guide decisions on when to rely on LLMs versus when to seek external knowledge, particularly in applications where errors could have serious consequences.", "author_claim_text": "This limitation highlights the importance of discerning when to trust LLMs, especially in safety-critical domains", "source_hint": "Abstract and Introduction"}, {"name": "CONQORD reinforcement learning framework with dual-component reward", "description": "CONQORD employs a reinforcement learning framework with a dual-component reward strategy consisting of a quality reward model that rates response quality and an order-preserving alignment reward that encourages ordinal consistency between confidence and quality ratings while penalizing discrepancies.", "author_claim_text": "we propose CONfidence-Quality-ORDer-preserving alignment approach (CONQORD), which leverages reinforcement learning guided by a tailored dual-component reward function", "source_hint": "Abstract and Section 4"}, {"name": "Order-preserving alignment reward function", "description": "The order-preserving reward function is designed to maintain a consistent ordinal relationship between verbalized confidence and response quality by rewarding alignment and penalizing misalignment across sample pairs, promoting careful self-calibration while being robust to outliers.", "author_claim_text": "the order-preserving reward incentivizes the model to verbalize greater confidence for responses of higher quality to align the order of confidence and quality", "source_hint": "Abstract and Section 4.1"}]}, "abs": "Despite the success of large language models (LLMs) in natural language generation, much evidence shows that LLMs may produce incorrect or nonsensical text. This limitation highlights the importance of discerning when to trust LLMs, especially in safety-critical domains. Existing methods often express reliability by confidence level, however, their effectiveness is limited by the lack of objective guidance. To address this, we propose CONfidence-Quality-ORDer-preserving alignment approach (CONQORD), which leverages reinforcement learning guided by a tailored dual-component reward function. This function integrates quality reward and order-preserving alignment reward functions. Specifically, the order-preserving reward incentivizes the model to verbalize greater confidence for responses of higher quality to align the order of confidence and quality. Experiments demonstrate that CONQORD significantly improves the alignment performance between confidence and response accuracy, without causing over-cautious. Furthermore, the aligned confidence provided by CONQORD informs when to trust LLMs, and acts as a determinant for initiating the retrieval process of external knowledge. Aligning confidence with response quality ensures more transparent and reliable responses, providing better trustworthiness."}, {"title": "Can We Trust LLMs? Mitigate Overconfidence Bias in LLMs through Knowledge Transfer", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Taxonomy/Self-Verbalized_UQ/Can We Trust LLMs? Mitigate Overconfidence Bias in LLMs through Knowledge Transfer.pdf", "summary": {"paper_id": "survey-28_paper-27_Can We Trust LLMs? Mitigate Overconfidence Bias in LLMs through Knowledge Transfer", "core_task": {"text": "Mitigating overconfidence bias in large language models", "query_variants": ["Mitigating overconfidence bias in large language models", "Reducing overconfidence in LLM predictions and uncertainty estimation", "Addressing calibration issues and overconfident outputs in large language models"]}, "contributions": [{"name": "Overconfidence bias in large language models", "description": "The paper addresses the problem that large language models exhibit overconfidence bias, producing high confidence levels even when their predictions are incorrect. This issue is particularly pronounced in models like Vicuna and LLaMA 2, as demonstrated through initial tests.", "author_claim_text": "overconfidence bias in LLMs, where models yield overconfidence in their incorrect predictions, is a notable issue", "source_hint": "Introduction paragraph 1"}, {"name": "Improving model reliability and trustworthiness in critical applications", "description": "The authors are motivated by the need to improve model accuracy and trustworthiness in critical sectors like law and finance, where overconfident incorrect predictions could lead to costly errors. Addressing this bias is essential for deploying LLMs reliably in real-world applications.", "author_claim_text": "Mitigating overconfidence in models is vital for accuracy and trust in key sectors such as law and finance, to prevent costly errors and enhance reliability", "source_hint": "Introduction paragraph 2"}, {"name": "Knowledge transfer method using chain of thoughts", "description": "The authors propose a knowledge transfer method where large 'teacher' LLMs like GPT-4 generate detailed chain-of-thought reasoning paths, which are then used to fine-tune smaller 'student' LLMs. This enables smaller models to learn advanced reasoning patterns and produce more accurate predictions with calibrated confidence levels.", "author_claim_text": "we proposed a knowledge transfer (KT) method to address this issue rely on fine-tuning through Chain of Thoughts (CoT)", "source_hint": "Introduction paragraph 2, Methodology section"}, {"name": "Knowledge extraction from big LLMs via CoT generation", "description": "The method involves extracting knowledge from large LLMs by having them generate chain-of-thought reasoning that includes detailed, sequential reasoning pathways showing how the model arrives at correct answers. These CoTs are then used as training data for smaller models.", "author_claim_text": "Big LLMs generate CoTs from questions and prompts as follows. These CoTs include detailed and sequential reasoning pathways, voicing how the model arrives at its correct answers", "source_hint": "Methodology §3.1"}]}, "abs": "The study explores mitigating overconfidence bias in LLMs to improve their reliability. We introduce a knowledge transfer (KT) method utilizing chain of thoughts, where \"big\" LLMs impart knowledge to \"small\" LLMs via detailed, sequential reasoning paths. This method uses advanced reasoning of larger models to fine-tune smaller models, enabling them to produce more accurate predictions with calibrated confidence. Experimental evaluation using multiple-choice questions and sentiment analysis across diverse datasets demonstrated the KT method's superiority over the vanilla and question-answer pair (QA) fine-tuning methods. The most significant improvement in three key metrics, where the KT method outperformed the vanilla and QA methods by an average of 55.3% and 43.1%, respectively. These findings underscore the KT method's potential in enhancing model trustworthiness and accuracy, offering precise outputs with well-matched confidence levels across various contexts."}, {"title": "SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Taxonomy/Self-Verbalized_UQ/SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales.pdf", "summary": {"paper_id": "survey-28_paper-28_SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales", "core_task": {"text": "teaching language models to express confidence with self-reflective rationales", "query_variants": ["teaching language models to express confidence with self-reflective rationales", "training LMs to convey uncertainty through introspective reasoning", "instructing language models to communicate certainty via self-evaluative explanations"]}, "contributions": [{"name": "LLMs fail to express accurate confidence and explain uncertainty", "description": "The paper addresses the challenge that large language models frequently produce hallucinated content and typically do not express their uncertainty when faced with unfamiliar questions, limiting their reliability and trustworthiness in practical applications.", "author_claim_text": "Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications.", "source_hint": "Abstract, first sentence"}, {"name": "Need for trustworthy AI with clear confidence expression", "description": "The authors are motivated by the need to improve trust in AI systems through transparent confidence estimates and explanations. This enables better human-AI interaction, supports proactive learning algorithms, and helps users verify information in LLM responses.", "author_claim_text": "A clear confidence expression with explanations can promote trustworthiness in AI, from the perspective of LLMs' alignment.", "source_hint": "Introduction, paragraph discussing research influence"}, {"name": "Supervised fine-tuning with self-reflective rationales", "description": "The method constructs a model-specific dataset by sampling multiple reasoning chains from an LLM, clustering them based on semantic similarity, and using GPT-4 to summarize inconsistencies as self-reflective rationales. These rationales explain why the model is uncertain and are used for supervised fine-tuning along with confidence estimates derived from response consistency.", "author_claim_text": "We present SaySelf, a novel training framework that teaches LLMs to express more fine-grained confidence estimates... by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language.", "source_hint": "Abstract and Section 3.1"}, {"name": "Reinforcement learning with calibration reward function", "description": "The approach employs PPO-based reinforcement learning with a quadratic reward function that encourages high confidence in correct predictions while penalizing overconfidence in incorrect ones. This further calibrates the confidence estimates beyond what supervised fine-tuning achieves.", "author_claim_text": "We utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs.", "source_hint": "Abstract and Section 3.2"}]}, "abs": "Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work elicits confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present the advanced SaySelf, a training framework that teaches LLMs to express more accurate fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results in both in-distribution and out-of-distribution datasets demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. We show that the generated self-reflective rationales are reasonable and can further contribute to the calibration. The code is made public at this https URL."}, {"title": "CLUE: Concept-Level Uncertainty Estimation for Large Language Models", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Taxonomy/Semantic-Similarity_UQ/CLUE: Concept-Level Uncertainty Estimation for Large Language Models.pdf", "summary": {"paper_id": "survey-28_paper-29_CLUE: Concept-Level Uncertainty Estimation for Large Language Models", "core_task": {"text": "concept-level uncertainty estimation for large language models", "query_variants": ["concept-level uncertainty estimation for large language models", "uncertainty quantification at concept level in LLMs", "estimating conceptual uncertainty for large-scale language models"]}, "contributions": [{"name": "Information entanglement issue in sequence-level uncertainty estimation", "description": "Prior uncertainty estimation methods for LLMs treat entire sequences as single units, failing to distinguish the uncertainty of individual information components within sequences. This limitation prevents nuanced evaluation when sequences contain multiple pieces of information with varying uncertainty levels.", "author_claim_text": "existing approaches to uncertainty estimation mainly focus on sequence-level uncertainty, overlooking individual pieces of information within sequences. These methods fall short in separately assessing the uncertainty of each component in a sequence.", "source_hint": "Abstract"}, {"name": "Need for interpretable uncertainty estimation in LLM outputs", "description": "Understanding and quantifying uncertainty in LLM outputs is essential for task-specific evaluations. High uncertainty may indicate hallucination in question-answering tasks but could be desirable for diversity in story generation, making interpretable uncertainty measurement critical for various applications.", "author_claim_text": "Measuring the uncertainty in LLM generation is important, as it can serve as a crucial indicator, offering insights into the reliability or diversity aspects of specific tasks.", "source_hint": "Introduction §1"}, {"name": "Concept-Level Uncertainty Estimation (CLUE) framework", "description": "CLUE uses LLMs with handcrafted one-shot examples to extract comprehensive concepts from generated sequences. Each concept is treated as an independent unit, evaluated by an NLI-based zero-shot text classifier that assigns entailment scores. Uncertainty is calculated as the average negative logarithm of concept scores across output sequences.", "author_claim_text": "we propose a novel framework for Concept-Level Uncertainty Estimation (CLUE) for LLMs. We leverage LLMs to convert output sequences into concept-level representations, breaking down sequences into individual concepts and measuring the uncertainty of each concept separately.", "source_hint": "Abstract and Introduction §1"}, {"name": "Concept extraction via LLM prompting", "description": "The framework employs LLM prompting with handcrafted one-shot examples to extract high-level concepts from sequences. Extracted concepts from multiple output sequences are combined into a unified concept pool, with similar concepts automatically consolidated using an NLI-based classifier.", "author_claim_text": "To measure the uncertainty at the concept level, we extract concepts from the generated sequences by prompting LLMs. Inspired by Brown et al., 2020, we feed handcrafted one-shot example to guide LLMs in generating concepts consistently", "source_hint": "Methodology §4.1"}]}, "abs": "Large Language Models (LLMs) have demonstrated remarkable proficiency in various natural language generation (NLG) tasks. Previous studies suggest that LLMs' generation process involves uncertainty. However, existing approaches to uncertainty estimation mainly focus on sequence-level uncertainty, overlooking individual pieces of information within sequences. These methods fall short in separately assessing the uncertainty of each component in a sequence. In response, we propose a novel framework for Concept-Level Uncertainty Estimation (CLUE) for LLMs. We leverage LLMs to convert output sequences into concept-level representations, breaking down sequences into individual concepts and measuring the uncertainty of each concept separately. We conduct experiments to demonstrate that CLUE can provide more interpretable uncertainty estimation results compared with sentence-level uncertainty, and could be a useful tool for various tasks such as hallucination detection and story generation."}, {"title": "Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Taxonomy/Semantic-Similarity_UQ/Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness.pdf", "summary": {"paper_id": "survey-28_paper-30_Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness", "core_task": {"text": "Uncertainty quantification for large language model responses", "query_variants": ["Uncertainty quantification for large language model responses", "Estimating confidence and reliability in LLM outputs", "Measuring prediction uncertainty in large language models"]}, "contributions": [{"name": "Unreliability and overconfidence of black-box LLMs", "description": "Large Language Models accessed via black-box APIs produce convincing but often incorrect answers without indicating uncertainty, making them unreliable for high-stakes applications. Traditional uncertainty estimation methods cannot be applied because training data and model internals are inaccessible.", "author_claim_text": "today's best LLMs have been trained to produce convincing-looking responses and thus often appear overconfident... the model cannot be certain about the desired response... yet these models output plausible-sounding yet wildly incorrect answers", "source_hint": "Introduction paragraph 2"}, {"name": "Enabling safe LLM usage in high-value applications", "description": "The authors aim to address the limitation preventing LLMs from being deployed in high-value, high-stakes applications by providing a method to identify when LLM outputs should not be trusted, enabling safer usage through uncertainty quantification.", "author_claim_text": "their usage in high-value applications remains limited by their unreliability... BSD ETECTOR confidence scores allow LLMs to be more safely used in high-stakes applications, since we can know which LLM outputs are not to be trusted", "source_hint": "Introduction paragraphs 1 and 5"}, {"name": "BSDetector uncertainty quantification technique", "description": "BSDetector estimates confidence scores for LLM outputs by calling the API multiple times with varying prompts and temperature values. It combines two factors: Observed Consistency (measuring contradiction among sampled responses) and Self-reflection Certainty (asking the LLM to evaluate its own answer correctness).", "author_claim_text": "We introduce BSD ETECTOR, a method for detecting bad and speculative answers from a pretrained Large Language Model by estimating a numeric confidence score for any output it generated", "source_hint": "Abstract and Introduction paragraph 4"}, {"name": "Observed Consistency measurement", "description": "This component samples multiple diverse responses from the LLM using modified prompts (with Chain-of-Thoughts) and measures semantic similarity between sampled and original answers using natural language inference classification to detect contradictions, combined with an indicator function for closed-form tasks.", "author_claim_text": "Observed Consistency is an extrinsic confidence assessment performed by a user who engages in repeated interactions with LLMs... we extend the indicator function to a particular form of semantic similarity based on contradiction ratings", "source_hint": "Section 3.1"}]}, "abs": "We introduce BSDetector, a method for detecting bad and speculative answers from a pretrained Large Language Model by estimating a numeric confidence score for any output it generated. Our uncertainty quantification technique works for any LLM accessible only via a black-box API, whose training data remains unknown. By expending a bit of extra computation, users of any LLM API can now get the same response as they would ordinarily, as well as a confidence estimate that cautions when not to trust this response. Experiments on both closed and open-form Question-Answer benchmarks reveal that BSDetector more accurately identifies incorrect LLM responses than alternative uncertainty estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple responses from the LLM and considering the one with the highest confidence score, we can additionally obtain more accurate responses from the same LLM, without any extra training steps. In applications involving automated evaluation with LLMs, accounting for our confidence scores leads to more reliable evaluation in both human-in-the-loop and fully-automated settings (across both GPT 3.5 and 4)."}, {"title": "Semantic Density: Uncertainty Quantification for Large Language Models through Confidence Measurement in Semantic Space", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Taxonomy/Semantic-Similarity_UQ/Semantic Density: Uncertainty Quantification for Large Language Models through Confidence Measurement in Semantic Space.pdf", "summary": {"paper_id": "survey-28_paper-31_Semantic Density: Uncertainty Quantification for Large Language Models through Confidence Measurement in Semantic Space", "core_task": {"text": "uncertainty quantification for large language models through semantic confidence measurement", "query_variants": ["uncertainty quantification for large language models through semantic confidence measurement", "measuring semantic confidence to quantify uncertainty in large language models", "confidence-based uncertainty estimation for LLMs using semantic measures"]}, "contributions": [{"name": "Lack of uncertainty quantification for LLM responses", "description": "Current large language models generate responses without quantitative indicators of their uncertainty or confidence, making it challenging to assess the trustworthiness of outputs, especially in safety-critical applications where hallucinations and misinformation are concerns.", "author_claim_text": "Existing LLMs do not have an inherent functionality to provide the users with an uncertainty/confidence metric for each response it generates, making it difficult to evaluate trustworthiness.", "source_hint": "Abstract"}, {"name": "Need for trustworthy LLMs in safety-critical applications", "description": "As LLMs are increasingly deployed in risk-sensitive domains like healthcare and finance, their tendency to hallucinate raises safety concerns. A reliable uncertainty quantification framework is essential to enable users to evaluate response trustworthiness and establish automatic warning systems based on confidence thresholds.", "author_claim_text": "Given their ability for general reasoning and adaptability to new tasks, LLMs are utilized increasingly in safety-critical applications, including healthcare and finance. However, existing LLMs have an unpredictable tendency to hallucinate, leading to misleading information and risky behaviors.", "source_hint": "Introduction paragraph 1"}, {"name": "Semantic density framework", "description": "The paper proposes semantic density, a framework that quantifies response-wise confidence by analyzing output probability distributions in semantic space. It uses kernel density estimation adapted for LLM outputs, incorporating fine-grained semantic differences among responses rather than binary semantic equivalence.", "author_claim_text": "A new framework is proposed in this paper to address these issues. Semantic density extracts uncertainty/confidence information for each response from a probability distribution perspective in semantic space.", "source_hint": "Abstract"}, {"name": "Semantic space definition", "description": "A prompt-specific semantic space is defined as a metric space where normalized embedding vectors represent responses, with distance functions measuring contextual semantic similarity. The space satisfies constraints for semantic equivalence, irrelevance, and contradiction cases.", "author_claim_text": "an oracle semantic space S is assumed to be a Euclidean space where each point is a D-dimensional vector that represents a contextual embedding of response y given prompt x", "source_hint": "Methodology Section 3.2"}]}, "abs": "With the widespread application of Large Language Models (LLMs) to various domains, concerns regarding the trustworthiness of LLMs in safety-critical scenarios have been raised, due to their unpredictable tendency to hallucinate and generate misinformation. Existing LLMs do not have an inherent functionality to provide the users with an uncertainty/confidence metric for each response it generates, making it difficult to evaluate trustworthiness. Although several studies aim to develop uncertainty quantification methods for LLMs, they have fundamental limitations, such as being restricted to classification tasks, requiring additional training and data, considering only lexical instead of semantic information, and being prompt-wise but not response-wise. A new framework is proposed in this paper to address these issues. Semantic density extracts uncertainty/confidence information for each response from a probability distribution perspective in semantic space. It has no restriction on task types and is \"off-the-shelf\" for new models and tasks. Experiments on seven state-of-the-art LLMs, including the latest Llama 3 and Mixtral-8x22B models, on four free-form question-answering benchmarks demonstrate the superior performance and robustness of semantic density compared to prior approaches."}, {"title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Taxonomy/Semantic-Similarity_UQ/Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation.pdf", "summary": {"paper_id": "survey-28_paper-32_Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation", "core_task": {"text": "uncertainty estimation in natural language generation", "query_variants": ["uncertainty estimation in natural language generation", "confidence quantification for text generation models", "measuring prediction uncertainty in NLG systems"]}, "contributions": [{"name": "Measuring uncertainty in natural language generation with semantic equivalence", "description": "The paper addresses the challenge that standard uncertainty measures fail in natural language generation because they treat semantically equivalent outputs (different phrasings of the same meaning) as distinct, leading to inflated uncertainty estimates when models are actually confident about the meaning.", "author_claim_text": "measuring uncertainty in natural language is challenging because of 'semantic equivalence'—different sentences can mean the same thing", "source_hint": "Abstract"}, {"name": "Need for reliable uncertainty in foundation models for trustworthy AI", "description": "The authors are motivated by the need to determine when language model outputs can be trusted, particularly for high-stakes applications like question answering. Without proper uncertainty measures, foundation models cannot be used as reliable information sources, which is critical for building safer AI systems.", "author_claim_text": "For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. Reliable measures of uncertainty have been identified as a key problem in building safer AI systems", "source_hint": "Abstract and Introduction"}, {"name": "Semantic entropy using linguistic invariances", "description": "The authors propose semantic entropy, which computes uncertainty over meanings rather than token sequences. This involves clustering semantically equivalent generations and computing entropy over the resulting meaning-space distribution, thereby accounting for the fact that different phrasings can express the same semantic content.", "author_claim_text": "we introduce semantic entropy—an entropy which incorporates linguistic invariances created by shared meanings", "source_hint": "Abstract and Section 4"}, {"name": "Bidirectional entailment clustering algorithm", "description": "The method uses a natural language inference model to determine semantic equivalence through bidirectional entailment: two sequences are considered semantically equivalent if each entails the other in the context of the question. This operationalizes the semantic equivalence relation needed to compute semantic entropy.", "author_claim_text": "We introduce an algorithm for clustering sequences that mean the same thing based on the principle that two sentences mean the same thing if you can infer each from the other", "source_hint": "Introduction and Section 4"}]}, "abs": "We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of \"semantic equivalence\" -- different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -- an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines."}, {"title": "Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Taxonomy/Semantic-Similarity_UQ/Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs.pdf", "summary": {"paper_id": "survey-28_paper-33_Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs", "core_task": {"text": "hallucination detection in large language models", "query_variants": ["hallucination detection in large language models", "identifying factual errors in LLM outputs", "detecting false information generation by large language models"]}, "contributions": [{"name": "High computational cost of semantic entropy for hallucination detection", "description": "Semantic entropy is effective at detecting hallucinations in LLMs but requires sampling 5-10 model generations per query, resulting in a 5-to-10-fold computational overhead that limits practical deployment.", "author_claim_text": "the 5-to-10-fold increase in computation cost associated with SE computation hinders practical adoption", "source_hint": "Abstract"}, {"name": "Need for cheap and reliable hallucination detection in LLMs", "description": "LLMs are increasingly deployed in high-stakes domains like medicine and law, where hallucinations (plausible but factually incorrect outputs) can cause significant harm. Existing effective detection methods are too expensive for widespread use, motivating the need for cost-efficient alternatives.", "author_claim_text": "Hallucinations present a major challenge to the practical adoption of LLMs. Computationally cheaper methods for reliable hallucination detection in LLMs are needed.", "source_hint": "Abstract and Introduction"}, {"name": "Semantic Entropy Probes (SEPs)", "description": "SEPs are linear logistic regression probes trained on LLM hidden states to predict semantic entropy. They operate on a single model generation rather than requiring multiple samples, reducing computational overhead to nearly zero while retaining hallucination detection performance.", "author_claim_text": "We propose semantic entropy probes (SEPs), a cheap and reliable method for uncertainty quantification in Large Language Models. SEPs directly approximate SE from the hidden states of a single generation.", "source_hint": "Abstract and Section 4"}, {"name": "Training SEPs via binarized semantic entropy supervision", "description": "SEPs are trained by collecting hidden states from single greedy generations paired with semantic entropy computed from 10 high-temperature samples. Semantic entropy scores are binarized using an optimal threshold based on within-cluster variance minimization, then used to supervise logistic regression classifiers.", "author_claim_text": "we compute a training set of hidden state – binarized semantic entropy pairs and use this to train a linear classifier", "source_hint": "Section 4"}]}, "abs": "We propose semantic entropy probes (SEPs), a cheap and reliable method for uncertainty quantification in Large Language Models (LLMs). Hallucinations, which are plausible-sounding but factually incorrect and arbitrary model generations, present a major challenge to the practical adoption of LLMs. Recent work by Farquhar et al. (2024) proposes semantic entropy (SE), which can detect hallucinations by estimating uncertainty in the space semantic meaning for a set of model generations. However, the 5-to-10-fold increase in computation cost associated with SE computation hinders practical adoption. To address this, we propose SEPs, which directly approximate SE from the hidden states of a single generation. SEPs are simple to train and do not require sampling multiple model generations at test time, reducing the overhead of semantic uncertainty quantification to almost zero. We show that SEPs retain high performance for hallucination detection and generalize better to out-of-distribution data than previous probing methods that directly predict model accuracy. Our results across models and tasks suggest that model hidden states capture SE, and our ablation studies give further insights into the token positions and model layers for which this is the case."}, {"title": "Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Taxonomy/Semantic-Similarity_UQ/Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models.pdf", "summary": {"paper_id": "survey-28_paper-34_Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models", "core_task": {"text": "uncertainty quantification for black-box large language models in natural language generation", "query_variants": ["uncertainty quantification for black-box large language models in natural language generation", "estimating confidence for opaque LLMs during text generation tasks", "measuring prediction uncertainty in closed-source language models for NLG"]}, "contributions": [{"name": "Uncertainty quantification for black-box large language models in natural language generation", "description": "The paper addresses the challenge of quantifying uncertainty in natural language generation when only black-box access to large language models is available, without access to internal model outputs like token-level logits or probabilities.", "author_claim_text": "gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification (UQ) for NLG. Furthermore, existing literature typically assumes white-box access to language models, which is becoming unrealistic", "source_hint": "Abstract"}, {"name": "Enabling trustworthy deployment of LLMs through uncertainty management", "description": "The authors are motivated by the need to determine when LLM responses can be trusted, particularly for high-stakes applications like medical or legal question-answering, where selective generation based on uncertainty estimates could improve decision-making processes.", "author_claim_text": "As such advances invariably lead to wider adoption of LLM for language generation tasks, such as question-answering (QA), it is crucial to quantify their uncertainty. A reliable measure of uncertainty is important to decide when to trust a model.", "source_hint": "Introduction §1"}, {"name": "Semantic dispersion measures using multiple generations and similarity scores", "description": "The authors propose generating multiple responses from the LLM for a given input, computing pairwise similarities using Jaccard similarity or NLI classifiers, and then deriving uncertainty and confidence measures from these similarities using graph-based methods.", "author_claim_text": "The main idea entails estimating the uncertainty/confidence from multiple generations from the LLM. We put forward several simple yet effective techniques for estimating uncertainty associated with input data and determining the confidence level of each individual generated response.", "source_hint": "Abstract and Introduction §1"}, {"name": "Graph Laplacian-based uncertainty and confidence measures", "description": "The paper introduces measures based on the graph Laplacian constructed from response similarities, including sum of eigenvalues (UEigV), degree matrix (UDeg, CDeg), and eccentricity (UEcc, CEcc), to quantify both input uncertainty and response-specific confidence.", "author_claim_text": "We first treat each generated response as one node and define the symmetric weighted adjacency matrix... A continuous version of UNumSet could be defined with λ1 < ··· < λm, the eigenvalues of L", "source_hint": "Section 4.2"}]}, "abs": "Large language models (LLMs) specializing in natural language generation (NLG) have recently started exhibiting promising capabilities across a variety of domains. However, gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification (UQ) for NLG. Furthermore, existing literature typically assumes white-box access to language models, which is becoming unrealistic either due to the closed-source nature of the latest LLMs or computational constraints. In this work, we investigate UQ in NLG for *black-box* LLMs. We first differentiate *uncertainty* vs *confidence*: the former refers to the ``dispersion'' of the potential predictions for a fixed input, and the latter refers to the confidence on a particular prediction/generation. We then propose and compare several confidence/uncertainty measures, applying them to *selective NLG* where unreliable results could either be ignored or yielded for further assessment. Experiments were carried out with several popular LLMs on question-answering datasets (for evaluation purposes). Results reveal that a simple measure for the semantic dispersion can be a reliable predictor of the quality of LLM responses, providing valuable insights for practitioners on uncertainty management when adopting LLMs. The code to replicate our experiments is available at this https URL."}, {"title": "CSS: Contrastive Semantic Similarity for Uncertainty Quantification of LLMs", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Taxonomy/Semantic-Similarity_UQ/CSS: Contrastive Semantic Similarity for Uncertainty Quantification of LLMs.pdf", "summary": {"paper_id": "survey-28_paper-35_CSS: Contrastive Semantic Similarity for Uncertainty Quantification of LLMs", "core_task": {"text": "uncertainty quantification of large language model generations", "query_variants": ["uncertainty quantification of large language model generations", "estimating confidence in outputs from large language models", "measuring prediction uncertainty in LLM generated text"]}, "contributions": [{"name": "Uncertainty quantification for large language models", "description": "The paper addresses the challenge of determining when to trust LLM generations, which is critical for deploying LLMs in high-stakes applications like medical or legal question-answering. Existing methods rely on NLI classifier logits that represent class probabilities rather than semantic features needed for accurate clustering.", "author_claim_text": "knowing when to trust their generations remains an open challenge", "source_hint": "Abstract, paragraph 1"}, {"name": "Improving trustworthiness of LLMs through better uncertainty estimation", "description": "The authors are motivated by the need to build safer and more trustworthy AI systems. Without reliable uncertainty measures, LLMs cannot be safely deployed in critical applications, and concerns about trustworthiness have grown alongside their enhanced capabilities.", "author_claim_text": "A reliable measure of uncertainty helps to decide when to trust a model, which is also the key problem in building safer AI systems", "source_hint": "Introduction, paragraph 1"}, {"name": "Contrastive Semantic Similarity (CSS) feature extraction", "description": "The method uses CLIP's text encoder to extract embeddings for text pairs, then applies point-wise product on corresponding embeddings to obtain contrastive semantic similarity features. These features represent semantic relations between text pairs and are used with PCA for dimension reduction before clustering.", "author_claim_text": "we propose Contrastive Semantic Similarity, the CLIP-based feature extraction module to obtain similarity features for measuring uncertainty for text pairs", "source_hint": "Abstract; Methodology §3.2"}, {"name": "Graph Laplacian with CSS features for uncertainty estimation", "description": "The method constructs a symmetric weighted adjacency matrix from CSS features, then applies spectral clustering via Graph Laplacian. Uncertainty is measured using eigenvalues (representing semantic clusters), degree matrix, and eigenvectors (eccentricity) derived from the CSS-based Laplacian.", "author_claim_text": "we then utilize CSS in Graph Laplacian (GL) to estimate uncertainty for LLMs", "source_hint": "Methodology §3.2"}]}, "abs": "Despite the impressive capability of large language models (LLMs), knowing when to trust their generations remains an open challenge. The recent literature on uncertainty quantification of natural language generation (NLG) utilises a conventional natural language inference (NLI) classifier to measure the semantic dispersion of LLMs responses. These studies employ logits of NLI classifier for semantic clustering to estimate uncertainty. However, logits represent the probability of the predicted class and barely contain feature information for potential clustering. Alternatively, CLIP (Contrastive Language-Image Pre-training) performs impressively in extracting image-text pair features and measuring their similarity. To extend its usability, we propose Contrastive Semantic Similarity, the CLIP-based feature extraction module to obtain similarity features for measuring uncertainty for text pairs. We apply this method to selective NLG, which detects and rejects unreliable generations for better trustworthiness of LLMs. We conduct extensive experiments with three LLMs on several benchmark question-answering datasets with comprehensive evaluation metrics. Results show that our proposed method performs better in estimating reliable responses of LLMs than comparable baselines. Results show that our proposed method performs better in estimating reliable responses of LLMs than comparable baselines. The code are available at \\url{this https URL}."}, {"title": "Unconditional Truthfulness: Learning Unconditional Uncertainty of Large Language Models", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Taxonomy/Token-Level_UQ/Unconditional Truthfulness: Learning Unconditional Uncertainty of Large Language Models.pdf", "summary": {"paper_id": "survey-28_paper-36_Unconditional Truthfulness: Learning Unconditional Uncertainty of Large Language Models", "core_task": {"text": "uncertainty quantification for large language model text generation", "query_variants": ["uncertainty quantification for large language model text generation", "estimating confidence in outputs from large language models", "measuring prediction uncertainty in LLM generated text"]}, "contributions": [{"name": "Conditional dependency between generation steps in LLM uncertainty quantification", "description": "The paper addresses the challenge that LLM predictions at each generation step are not conditionally independent. When an LLM generates an error early in a sequence, subsequent tokens condition on this error, leading to high confidence in incorrect outputs despite the initial uncertainty.", "author_claim_text": "During generation, LLMs condition on the previously-generated tokens. Thus, if an LLM has hallucinated and generated an incorrect claim at the beginning or middle of the sequence, all subsequently generated claims might also be incorrect.", "source_hint": "Introduction, paragraph 4"}, {"name": "Need for efficient and practical uncertainty quantification for LLM safety", "description": "The authors are motivated by the need to detect LLM hallucinations and low-quality outputs efficiently without relying on external knowledge sources or additional LLMs, which are either incomplete or computationally expensive. They seek practical UQ methods that leverage the model's internal knowledge.", "author_claim_text": "We argue that models inherently contain information about their own knowledge limitations, and that there should be an efficient way to access this information, which can enable LLM-based applications that are both safe and practical.", "source_hint": "Introduction, paragraph 2"}, {"name": "Trainable Attention-based Dependency (TAD)", "description": "TAD learns conditional dependencies between generation steps using a data-driven approach. A regression model is trained to predict the gap between conditional and unconditional generation confidence, using attention-based features. During inference, this model modulates current step uncertainty based on previous step uncertainty.", "author_claim_text": "We propose to learn this dependency from data. We train a regression model, which target variable is the gap between the conditional and the unconditional generation confidence.", "source_hint": "Abstract and Section 3"}, {"name": "First-order Markov approximation for conditional probability", "description": "The method simplifies the conditional dependency by assuming a first-order Markov process where each token depends only on the immediately preceding token, supported by empirical evidence showing LLMs pay greatest attention to the previous token in 76% of cases.", "author_claim_text": "For the sake of simplicity, we assume that only the uncertainty from the previous tokens is propagated to the current generation step. This assumption leads us to the first-order Markov process, in which the probability for the token ti is conditioned only on the token ti−1.", "source_hint": "Section 3.1"}]}, "abs": "Uncertainty quantification (UQ) has emerged as a promising approach for detecting hallucinations and low-quality output of Large Language Models (LLMs). However, obtaining proper uncertainty scores is complicated by the conditional dependency between the generation steps of an autoregressive LLM because it is hard to model it explicitly. Here, we propose to learn this dependency from attention-based features. In particular, we train a regression model that leverages LLM attention maps, probabilities on the current generation step, and recurrently computed uncertainty scores from previously generated tokens. To incorporate the recurrent features, we also suggest a two-staged training procedure. Our experimental evaluation on ten datasets and three LLMs shows that the proposed method is highly effective for selective generation, achieving substantial improvements over rivaling unsupervised and supervised approaches."}, {"title": "Uncertainty Quantification for In-Context Learning of Large Language Models", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Taxonomy/Token-Level_UQ/Uncertainty Quantification for In-Context Learning of Large Language Models.pdf", "summary": {"paper_id": "survey-28_paper-37_Uncertainty Quantification for In-Context Learning of Large Language Models", "core_task": {"text": "Uncertainty quantification for in-context learning of large language models", "query_variants": ["Uncertainty quantification for in-context learning of large language models", "Estimating confidence and reliability in few-shot prompting with LLMs", "Measuring prediction uncertainty during prompt-based learning in large language models"]}, "contributions": [{"name": "Uncertainty quantification for in-context learning in LLMs", "description": "Existing uncertainty quantification methods for LLMs fail to account for the complex nature of in-context learning and do not decompose uncertainty into its underlying sources, making it difficult to determine whether prediction errors stem from demonstration quality or model configuration.", "author_claim_text": "existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning", "source_hint": "Abstract"}, {"name": "Understanding sources of uncertainty in LLM predictions", "description": "The authors are motivated to decompose uncertainty into aleatoric (from demonstrations) and epistemic (from model configurations) components to better understand and address reliability issues like hallucination in LLM responses, enabling more informed decisions about model selection and adjustments.", "author_claim_text": "when LLM uses in-context learning to predict a wrong answer with high uncertainty, can we indicate if it is caused by the demonstration examples or by the model itself?", "source_hint": "Introduction §1"}, {"name": "Bayesian formulation of in-context learning with latent variables", "description": "The authors formulate in-context learning as a Bayesian Neural Network with latent concept variables z, where the predictive distribution integrates over both latent concepts (from demonstrations) and model parameters, enabling principled uncertainty decomposition.", "author_claim_text": "We formulate the predictive distribution of in-context learning for predicting yT given few-shot demonstrations x1:T−1 and a test case xT", "source_hint": "Section 2.2"}, {"name": "Mutual information-based uncertainty decomposition", "description": "The method decomposes total uncertainty using entropy measures: epistemic uncertainty is quantified as the expected entropy conditioned on model parameters, while aleatoric uncertainty is computed as mutual information between predictions and latent concepts, approximated through beam search sampling.", "author_claim_text": "We propose quantifying both aleatoric and epistemic uncertainty from the mutual information perspective", "source_hint": "Abstract and Section 2.3"}]}, "abs": "In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: this https URL."}, {"title": "Self-Evaluation Improves Selective Generation in Large Language Models", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Taxonomy/Token-Level_UQ/Self-Evaluation Improves Selective Generation in Large Language Models.pdf", "summary": {"paper_id": "survey-28_paper-38_Self-Evaluation Improves Selective Generation in Large Language Models", "core_task": {"text": "selective generation in large language models", "query_variants": ["selective generation in large language models", "conditional text generation in LLMs", "targeted output production in large language models"]}, "contributions": [{"name": "Poor calibration of sequence-level probabilities for assessing LLM generation quality", "description": "Existing sequence-level likelihood scores from large language models do not reliably rank-order their outputs by quality in open-ended generation tasks, making them unsuitable for determining when to abstain or selectively generate content.", "author_claim_text": "recent research has demonstrated the limitations of using sequence-level probability estimates given by LLMs as reliable indicators of generation quality", "source_hint": "Abstract"}, {"name": "Safe deployment of LLMs through reliable quality assessment", "description": "The authors aim to enable safe deployment of large language models by developing reliable methods to assess generated content quality, allowing systems to selectively generate answers based on confidence and abstain when output quality is uncertain.", "author_claim_text": "Having the ability to tell when a language model's output is trustworthy is important for safe deployment of language models", "source_hint": "Introduction §1"}, {"name": "Sample and Select: reformulating generation as multi-choice token-level prediction", "description": "The method samples multiple candidate answers from the LLM and reformulates the task as a multiple-choice question, using token-level probabilities (e.g., A, B, C) to select and score answers. An optional 'None of the above' choice allows the model to express uncertainty explicitly.", "author_claim_text": "we reformulate open-ended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level", "source_hint": "Abstract and Methods §2.1"}, {"name": "Sample and Eval: pointwise true/false evaluation of generated answers", "description": "This approach instructs the LLM to evaluate whether a generated answer is correct using a binary yes/no format, producing a token-level probability score p(Yes|x,y) that is independent of other candidate answers.", "author_claim_text": "We can also evaluate a question and an answer pair using pointwise evaluation format. We ask the model if the candidate answer is correct or not", "source_hint": "Methods §2.2"}]}, "abs": "Safe deployment of large language models (LLMs) may benefit from a reliable method for assessing their generated content to determine when to abstain or to selectively generate. While likelihood-based metrics such as perplexity are widely employed, recent research has demonstrated the limitations of using sequence-level probability estimates given by LLMs as reliable indicators of generation quality. Conversely, LLMs have demonstrated strong calibration at the token level, particularly when it comes to choosing correct answers in multiple-choice questions or evaluating true/false statements. In this work, we reformulate open-ended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level. We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a ``None of the above'' option to express the model's uncertainty explicitly. We benchmark a range of scoring methods based on self-evaluation and evaluate their performance in selective generation using TruthfulQA and TL;DR. Through experiments with PaLM-2 and GPT-3, we demonstrate that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content."}, {"title": "MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Taxonomy/Token-Level_UQ/MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs.pdf", "summary": {"paper_id": "survey-28_paper-39_MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs", "core_task": {"text": "uncertainty estimation in generative large language models", "query_variants": ["uncertainty estimation in generative large language models", "confidence quantification for generative LLMs", "measuring uncertainty in autoregressive language models"]}, "contributions": [{"name": "Inaccurate uncertainty estimation in generative LLMs due to equal token weighting", "description": "Existing probability-based uncertainty estimation methods for generative LLMs use length-normalized scoring that assigns equal importance to all tokens in a generated sequence, ignoring the fact that different tokens contribute differently to the semantic meaning and correctness of the response in the question context.", "author_claim_text": "A downside of these existing UE techniques in the generative LLM literature is treating length-normalized scoring like the class probabilities in classification tasks. However, better ways may exist for estimating uncertainty than directly using the length-normalized score of a sequence, as it treats all tokens equally.", "source_hint": "Introduction, paragraph 5"}, {"name": "Need for reliable uncertainty quantification in high-stakes LLM applications", "description": "The authors are motivated by the critical need to quantify uncertainty in generative LLM outputs, especially in high-stakes domains like medical advice, where incorrect outputs can lead to serious consequences. Reliable uncertainty estimation enables users to assess how much trust to place in LLM responses.", "author_claim_text": "Quantifying the uncertainty of generative LLM responses in such scenarios is not just beneficial but essential for ensuring trustworthy operation. For example, in a medical advice application, accurately assessing the uncertainty of the responses provided by LLMs can prevent the provision of incorrect medical advice.", "source_hint": "Introduction, paragraph 2"}, {"name": "Meaning-Aware Response Scoring (MARS)", "description": "MARS is a novel scoring function that replaces length-normalized scoring by assigning importance weights to each token based on its semantic contribution to the response meaning in the question context. It computes the product of weighted token probabilities, where weights are a convex combination of uniform length normalization and meaning-based importance coefficients.", "author_claim_text": "We propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question.", "source_hint": "Abstract and Section 3.2"}, {"name": "Phrase-level importance function using BERT-like model", "description": "The authors develop a 110M-parameter BERT-like neural network that simultaneously identifies phrases in generated text and assigns importance scores to them in a single forward pass. This model measures semantic impact by evaluating how masking a phrase affects answer correctness, reducing computational overhead compared to iterative masking approaches.", "author_claim_text": "We introduce a BERT-like model, efficiently assigning meaning-aware importance weights to the tokens in a single model pass within MARS calculation.", "source_hint": "Abstract and Section 3.3"}]}, "abs": "Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question. We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book question-answering datasets across five popular pre-trained LLMs. Lastly, we validate the efficacy of MARS on a Medical QA dataset. Code can be found this https URL."}, {"title": "On Hallucination and Predictive Uncertainty in Conditional Language Generation", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Taxonomy/Token-Level_UQ/On Hallucination and Predictive Uncertainty in Conditional Language Generation.pdf", "summary": {"paper_id": "survey-28_paper-40_On Hallucination and Predictive Uncertainty in Conditional Language Generation", "core_task": {"text": "hallucination and predictive uncertainty in conditional language generation", "query_variants": ["hallucination and predictive uncertainty in conditional language generation", "uncertainty estimation and factual errors in conditional text generation", "predictive confidence and hallucinations in conditioned language models"]}, "contributions": [{"name": "Hallucination in conditional language generation", "description": "Neural language generation models generate description tokens that are not supported by the source inputs, a phenomenon called hallucination. This seriously damages the applicability of neural models in practice where information accuracy is vital.", "author_claim_text": "deep neural models are prone to hallucinating facts that are incorrect or nonexistent", "source_hint": "Abstract"}, {"name": "Lack of systematic explanation for hallucination across tasks", "description": "While hallucination has been observed in various conditional NLG tasks and task-specific explanations exist, there is no unified understanding of why hallucinations occur across different tasks. The authors believe predictive uncertainty is a common theme.", "author_claim_text": "Different hypotheses are proposed and examined separately for different tasks, but no systematic explanations are available across these tasks", "source_hint": "Abstract and Introduction"}, {"name": "Uncertainty-aware beam search (UABS)", "description": "A simple extension to beam search that penalizes parts or the entirety of predictive uncertainty during model decoding. It adds a weighted penalty term to balance between log probability and predictive uncertainty of selected candidates.", "author_claim_text": "We propose an uncertainty-aware beam search approach for hallucination reduction to demonstrate that lowering uncertainty can lead to less hallucination", "source_hint": "Abstract and Section 5.1"}, {"name": "Uncertainty decomposition into epistemic and aleatoric components", "description": "The authors decompose total predictive uncertainty into epistemic (model-related) and aleatoric (data-related) components using deep ensembles, and investigate which type is more indicative of hallucination.", "author_claim_text": "We show that uncertainty decomposition helps to achieve better trade-offs between hallucination and performance", "source_hint": "Abstract and Section 2.3"}]}, "abs": "Despite improvements in performances on different natural language generation tasks, deep neural models are prone to hallucinating facts that are incorrect or nonexistent. Different hypotheses are proposed and examined separately for different tasks, but no systematic explanations are available across these tasks. In this study, we draw connections between hallucinations and predictive uncertainty in conditional language generation. We investigate their relationship in both image captioning and data-to-text generation and propose a simple extension to beam search to reduce hallucination. Our analysis shows that higher predictive uncertainty corresponds to a higher chance of hallucination. Epistemic uncertainty is more indicative of hallucination than aleatoric or total uncertainties. It helps to achieve better results of trading performance in standard metric for less hallucination with the proposed beam search variant."}, {"title": "Language Models (Mostly) Know What They Know", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Taxonomy/Token-Level_UQ/Language Models (Mostly) Know What They Know.pdf", "summary": {"paper_id": "survey-28_paper-41_Language Models (Mostly) Know What They Know", "core_task": {"text": "language model calibration and self-knowledge evaluation", "query_variants": ["language model calibration and self-knowledge evaluation", "assessing confidence accuracy and introspective capabilities in LMs", "evaluating uncertainty estimation and metacognitive awareness of language models"]}, "contributions": [{"name": "Language models' inability to evaluate their own knowledge and confidence", "description": "The paper addresses the challenge of determining whether language models can accurately assess the correctness of their own outputs and identify which questions they can answer, a prerequisite for training honest AI systems.", "author_claim_text": "We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly.", "source_hint": "Abstract, first sentence"}, {"name": "Training honest AI systems that recognize their knowledge limits", "description": "The authors are motivated by the goal of developing AI systems capable of honesty, which necessitates that models can recognize what they do and do not know as a foundational capability.", "author_claim_text": "We would eventually like to train AI systems that are honest, which requires that these systems accurately and faithfully evaluate their level of confidence in their own knowledge and reasoning.", "source_hint": "Introduction, paragraph 1"}, {"name": "Self-evaluation via P(True) on model-generated samples", "description": "The authors propose a method where models first generate answers to questions, then evaluate the probability that each generated answer is correct by reformulating the task as a True/False question about the proposed answer.", "author_claim_text": "We can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability 'P(True)' that their answers are correct.", "source_hint": "Abstract, middle section"}, {"name": "Training models to predict P(IK) using a value head", "description": "The authors train language models with an additional classification head to predict whether they can correctly answer a given question, creating a direct measure of model self-knowledge independent of any specific answer.", "author_claim_text": "We investigate whether models can be trained to predict 'P(IK)', the probability that 'I know' the answer to a question, without reference to any particular proposed answer.", "source_hint": "Abstract and Section 5 introduction"}]}, "abs": "We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability \"P(True)\" that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict \"P(IK)\", the probability that \"I know\" the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing."}, {"title": "Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification", "path": "/mnt/data/zrz_survey_data/4_A Survey on Uncertainty Quantification of Large Language/Uncertainty Quantification for LLMs/Taxonomy/Token-Level_UQ/Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification.pdf", "summary": {"paper_id": "survey-28_paper-42_Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification", "core_task": {"text": "fact-checking and hallucination detection in large language model outputs", "query_variants": ["fact-checking and hallucination detection in large language model outputs", "verifying factual accuracy and identifying hallucinations in LLM generations", "detecting false information and hallucinated content in large language model responses"]}, "contributions": [{"name": "Hallucinations in LLM outputs obscure factual inaccuracies", "description": "The paper addresses the challenge that large language models produce hallucinations—factually incorrect claims—that are difficult for users to detect because they are embedded within otherwise coherent and factually correct text.", "author_claim_text": "Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factually correct, making it extremely hard for the users to spot them.", "source_hint": "Abstract"}, {"name": "Need for hallucination detection without external knowledge", "description": "The authors are motivated by the need to detect hallucinations without relying on complex external knowledge sources or fact-checking systems, instead leveraging information already present in the model's outputs through uncertainty quantification.", "author_claim_text": "We argue that information about whether a generation is a hallucination is encapsulated in the model output itself, and can be extracted using uncertainty quantification (UQ)", "source_hint": "Introduction §1"}, {"name": "Fact-checking pipeline based on token-level uncertainty quantification", "description": "The authors propose a pipeline that splits generated text into atomic claims, calculates token-level uncertainty scores for each claim, aggregates them into claim-level scores, and highlights unreliable claims based on a threshold.", "author_claim_text": "we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification", "source_hint": "Abstract"}, {"name": "Claim Conditioned Probability (CCP) for token-level uncertainty", "description": "CCP is a novel token-level uncertainty method that isolates claim uncertainty by filtering out uncertainties related to claim type/order and surface form variations, using NLI to group semantically equivalent token alternatives.", "author_claim_text": "we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what claim to generate on the current step and what surface form to use. Our method Claim Conditioned Probability (CCP) measures only the uncertainty of a particular claim value expressed by the model.", "source_hint": "Abstract"}]}, "abs": "Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factually correct, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what claim to generate on the current step and what surface form to use. Our method Claim Conditioned Probability (CCP) measures only the uncertainty of a particular claim value expressed by the model. Experiments on the task of biography generation demonstrate strong improvements for CCP compared to the baselines for seven LLMs and four languages. Human evaluation reveals that the fact-checking pipeline based on uncertainty quantification is competitive with a fact-checking tool that leverages external knowledge."}], "survey_topic_path": "4_A Survey on Uncertainty Quantification of Large Language", "survey_topic": "A Survey on Uncertainty Quantification of Large Language", "gt_paper_count": 43, "gt": {"name": "4_A Survey on Uncertainty Quantification of Large Language", "subtopics": [{"name": "Uncertainty Quantification for LLMs", "subtopics": [{"name": "Applications", "subtopics": [{"name": "Chatbot_and_Textual", "papers": ["To Believe or Not to Believe Your LLM", "Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus", "Language Models with Conformal Factuality Guarantees", "Enhancing Traffic Prediction with Textual Data Using Large Language Models", "Leveraging Uncertainty Estimation for Efficient LLM Routing", "Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models", "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions"]}, {"name": "Robotics", "papers": ["Probabilistically Correct Language-based Multi-Robot Planning using Conformal Prediction", "Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning", "Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity", "Knowing When to Ask -- Bridging Large Language Models and Data", "Evaluating Uncertainty-based Failure Detection for Closed-Loop LLM Planners", "Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners"]}]}, {"name": "Datasets_and_Benchmarks", "subtopics": [{"name": "Factuality_Analysis", "papers": ["TruthfulQA: Measuring How Models Mimic Human Falsehoods", "FEVER: a large-scale dataset for Fact Extraction and VERification", "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models"]}, {"name": "Mathematics", "papers": ["Teaching Models to Express Their Uncertainty in Words"]}, {"name": "Multi-Hop_Reasoning", "papers": ["HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering", "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies"]}, {"name": "Reading_Comprehension", "papers": ["CoQA: A Conversational Question Answering Challenge", "Neural Text Generation from Structured Data with Application to the Biography Domain", "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"]}]}, {"name": "Taxonomy", "subtopics": [{"name": "Mechanistic_Interpretability", "papers": ["Distinguishing the Knowable from the Unknowable with Language Models"]}, {"name": "Self-Verbalized_UQ", "papers": ["LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models", "Linguistic Calibration of Long-Form Generations", "Reducing conversational agents' overconfidence through linguistic calibration", "When to Trust LLMs: Aligning Confidence with Response Quality", "Can We Trust LLMs? Mitigate Overconfidence Bias in LLMs through Knowledge Transfer", "SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales"]}, {"name": "Semantic-Similarity_UQ", "papers": ["CLUE: Concept-Level Uncertainty Estimation for Large Language Models", "Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness", "Semantic Density: Uncertainty Quantification for Large Language Models through Confidence Measurement in Semantic Space", "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation", "Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs", "Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models", "CSS: Contrastive Semantic Similarity for Uncertainty Quantification of LLMs"]}, {"name": "Token-Level_UQ", "papers": ["Unconditional Truthfulness: Learning Unconditional Uncertainty of Large Language Models", "Uncertainty Quantification for In-Context Learning of Large Language Models", "Self-Evaluation Improves Selective Generation in Large Language Models", "MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs", "On Hallucination and Predictive Uncertainty in Conditional Language Generation", "Language Models (Mostly) Know What They Know", "Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification"]}]}]}]}, "input_paper_count": 43, "input_content": "SYSTEM PROMPT:\nYou are a senior researcher and survey-author with deep experience in structuring\nhigh-quality academic survey papers.\n\nYour task is to organize a set of research papers into a **hierarchical topic tree**\n(bottom-up), given:\n- a survey topic,\n- for each paper: title, abstract, and an structured summary\n  (which may include research problem, motivation, methodology, and findings).\n\nYour goal is NOT just to cluster by surface similarity, but to produce a taxonomy\nthat would be considered **reasonable, informative, and defensible** in a top-tier survey paper.\n\n### Hard Constraints\n1. Output must be **strictly valid JSON**.\n2. Only leaf nodes may contain `\"papers\"`; all internal nodes must contain `\"subtopics\"`.\n3. **Every paper must appear exactly once** in the entire tree.\n4. NO duplicate papers anywhere.\n5. The tree must eventually merge into **one single root node**.\n\n### CLASSIFICATION RULES\n- Group papers by semantic similarity using both title + abstract.\n- Create meaningful names for leaf-level themes.\n\n### ANTI-DUPLICATION PROCEDURE (MANDATORY)\nBefore constructing the tree:\n1. Produce an internal list of all given paper titles.\n2. Assign each paper to exactly one leaf node.\n3. After assignment, verify that:\n   - the number of assigned papers equals the number of input papers,\n   - no paper appears in more than one group.\n\n### Output Format\nUse a JSON structure like this (replace placeholders with actual paper titles) and\nthe output you produce MUST be wrapped inside a fenced code block:\n\n```json\n{\n  \"name\": \"AI Research\",\n  \"subtopics\": [\n    {\n      \"name\": \"NLP\",\n      \"subtopics\": [\n        {\n          \"name\": \"Text Summarization\",\n          \"papers\": [\"<actual paper titles>\"]\n        },\n        {\n          \"name\": \"Machine Translation\",\n          \"papers\": [\"<actual paper titles>\"]\n        }\n      ]\n    }\n  ]\n}```\n\nUSER PROMPT:\nPerform a bottom-up hierarchical clustering of the following 43 papers and produce a JSON research topic tree.\n\nSurvey Topic: A Survey on Uncertainty Quantification of Large Language\n\nPaper List:\nPaper 1:\n  Title: To Believe or Not to Believe Your LLM\n  Abstract: We explore uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large. We simultaneously consider both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers). In particular, we derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable. This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses. Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses. This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected. We conduct a series of experiments which demonstrate the advantage of our formulation. Further, our investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest.\n  Structured Summary:\n    - Detecting hallucinations in LLMs with multiple valid answers: Existing uncertainty quantification methods for LLMs fail when multiple correct responses exist (high aleatoric uncertainty), because they cannot distinguish between epistemic uncertainty (lack of knowledge) and aleatoric uncertainty (multiple valid answers). A perfect predictor and a useless predictor may have the same total uncertainty but different epistemic uncertainty.\n    - Importance of decoupling epistemic and aleatoric uncertainty: The authors are motivated to address the problem of truthfulness in LLMs by distinguishing epistemic uncertainty (arising from lack of knowledge) from aleatoric uncertainty (arising from irreducible randomness like multiple valid answers). When epistemic uncertainty is high, the model's predictions are unreliable and may be hallucinated.\n    - Iterative prompting procedure for constructing pseudo joint distributions: The authors propose an iterative prompting method that constructs a joint distribution over multiple responses by repeatedly querying the LLM with the original query plus previously generated responses. This allows them to measure how responses depend on context, revealing epistemic uncertainty through the chain rule of probability.\n    - Information-theoretic metric based on mutual information: The authors derive a metric for epistemic uncertainty based on the KL divergence between the LLM-derived pseudo joint distribution and the ground truth. They prove this can be lower-bounded by the mutual information of the LLM-derived distribution, which is computable solely from the model's outputs and is insensitive to aleatoric uncertainty.\n\nPaper 2:\n  Title: Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus\n  Abstract: Large Language Models (LLMs) have gained significant popularity for their impressive performance across diverse fields. However, LLMs are prone to hallucinate untruthful or nonsensical outputs that fail to meet user expectations in many real-world applications. Existing works for detecting hallucinations in LLMs either rely on external knowledge for reference retrieval or require sampling multiple responses from the LLM for consistency verification, making these methods costly and inefficient. In this paper, we propose a novel reference-free, uncertainty-based method for detecting hallucinations in LLMs. Our approach imitates human focus in factuality checking from three aspects: 1) focus on the most informative and important keywords in the given text; 2) focus on the unreliable tokens in historical context which may lead to a cascade of hallucinations; and 3) focus on the token properties such as token type and token frequency. Experimental results on relevant datasets demonstrate the effectiveness of our proposed method, which achieves state-of-the-art performance across all the evaluation metrics and eliminates the need for additional information.\n  Structured Summary:\n    - Hallucination detection in LLMs is costly and inefficient: Current hallucination detection methods face significant limitations: retrieval-based approaches depend on external knowledge bases that may not be accessible, while sampling-based methods require multiple LLM responses for consistency verification, resulting in high computational costs and inefficiency.\n    - Need for reliable and trustworthy LLMs in accuracy-critical scenarios: The prevalence of hallucinations across LLM applications undermines their reliability and trustworthiness, particularly in domains like finance, medicine, and education where factual accuracy is critical. This motivates the need for effective hallucination detection methods.\n    - Reference-free uncertainty-based hallucination detection with human-like focus: The authors propose a reference-free, uncertainty-based detection method that mimics human factuality checking by focusing on three key aspects: identifying the most informative keywords, detecting unreliable tokens in historical context that may trigger cascading hallucinations, and considering token properties like type and frequency.\n    - State-of-the-art performance without additional information requirements: The proposed method achieves state-of-the-art performance across all evaluation metrics on relevant datasets while eliminating the dependency on external knowledge bases or multiple sampled responses, demonstrating its effectiveness and efficiency.\n\nPaper 3:\n  Title: Language Models with Conformal Factuality Guarantees\n  Abstract: Guaranteeing the correctness and factuality of language model (LM) outputs is a major open problem. In this work, we propose conformal factuality, a framework that can ensure high probability correctness guarantees for LMs by connecting language modeling and conformal prediction. We observe that the correctness of an LM output is equivalent to an uncertainty quantification problem, where the uncertainty sets are defined as the entailment set of an LM's output. Using this connection, we show that conformal prediction in language models corresponds to a back-off algorithm that provides high probability correctness guarantees by progressively making LM outputs less specific (and expanding the associated uncertainty sets). This approach applies to any black-box LM and requires very few human-annotated samples. Evaluations of our approach on closed book QA (FActScore, NaturalQuestions) and reasoning tasks (MATH) show that our approach can provide 80-90% correctness guarantees while retaining the majority of the LM's original output.\n  Structured Summary:\n    - Lack of correctness and factuality guarantees for language model outputs: The paper addresses the fundamental challenge that large language models generate outputs that cannot be fully trusted due to hallucinations and non-factual content, and existing methods lack precise probabilistic guarantees of correctness.\n    - Need for high-probability correctness guarantees in high-stakes domains: The authors are motivated by the requirement that in critical applications like healthcare, law, and robotics, language models must provide outputs that are correct with a known, user-specified probability, which current systems cannot guarantee.\n    - Conformal factuality framework connecting entailment sets and conformal prediction: The authors propose a framework that defines uncertainty sets as the entailment set of an LM's output, establishing that correctness of an output is equivalent to the reference being contained in this entailment set, thereby enabling conformal prediction techniques to provide probabilistic correctness guarantees.\n    - Back-off algorithm using progressively less specific outputs: The method constructs a sequence of increasingly generic outputs by removing uncertain sub-claims, then uses conformal prediction to select a specificity level that is correct with high probability, applying standard split conformal prediction techniques.\n\nPaper 4:\n  Title: Enhancing Traffic Prediction with Textual Data Using Large Language Models\n  Abstract: Traffic prediction is pivotal for rational transportation supply scheduling and allocation. Existing researches into short-term traffic prediction, however, face challenges in adequately addressing exceptional circumstances and integrating non-numerical contextual information like weather into models. While, Large language models offer a promising solution due to their inherent world knowledge. However, directly using them for traffic prediction presents drawbacks such as high cost, lack of determinism, and limited mathematical capability. To mitigate these issues, this study proposes a novel approach. Instead of directly employing large models for prediction, it utilizes them to process textual information and obtain embeddings. These embeddings are then combined with historical traffic data and inputted into traditional spatiotemporal forecasting models. The study investigates two types of special scenarios: regional-level and node-level. For regional-level scenarios, textual information is represented as a node connected to the entire network. For node-level scenarios, embeddings from the large model represent additional nodes connected only to corresponding nodes. This approach shows a significant improvement in prediction accuracy according to our experiment of New York Bike dataset.\n  Structured Summary:\n    - Inadequate handling of exceptional circumstances and textual contextual information in traffic prediction: Current traffic prediction models struggle with two key issues: they cannot effectively handle exceptional events (extreme weather, concerts, sports matches) that are underrepresented in training data, and they have difficulty integrating non-numerical contextual information such as detailed weather data that exists in textual formats rather than simple binary embeddings.\n    - Leveraging LLM world knowledge while avoiding direct prediction drawbacks: The authors are motivated to exploit the world knowledge embedded in large language models to address exceptional circumstances and textual information, while avoiding the problems of direct LLM-based prediction including high computational costs, non-deterministic outputs, and poor mathematical reasoning capabilities that make LLMs unsuitable for direct numerical forecasting tasks.\n    - LLM-based textual embedding integration via auxiliary nodes: The proposed method uses large language models to convert textual contextual information into embedding vectors rather than for direct prediction. These embeddings undergo dimensionality reduction via PCA and are integrated into traditional spatiotemporal models as auxiliary nodes: regional-level information becomes a node connected to all network nodes, while node-level information connects only to corresponding specific nodes.\n    - Regional-level and node-level special scenario modeling: The approach distinguishes between two types of special scenarios: regional-level events (holidays, extreme weather affecting entire cities) are modeled as auxiliary nodes connected to all graph nodes, while node-level events (concerts, sports games at specific locations) are modeled as auxiliary nodes connected only to their corresponding district nodes in the traffic network graph.\n\nPaper 5:\n  Title: Leveraging Uncertainty Estimation for Efficient LLM Routing\n  Abstract: Deploying large language models (LLMs) in edge-cloud environments requires an efficient routing strategy to balance cost and response quality. Traditional approaches prioritize either human-preference data or accuracy metrics from benchmark datasets as routing criteria, but these methods suffer from rigidity and subjectivity. Moreover, existing routing frameworks primarily focus on accuracy and cost, neglecting response quality from a human preference perspective. In this work, we propose the Confidence-Driven LLM Router, a novel framework that leverages uncertainty estimation to optimize routing decisions. To comprehensively assess routing performance, we evaluate both system cost efficiency and response quality. In particular, we introduce the novel use of LLM-as-a-Judge to simulate human rating preferences, providing the first systematic assessment of response quality across different routing strategies. Extensive experiments on MT-Bench, GSM8K, and MMLU demonstrate that our approach outperforms state-of-the-art routing methods, achieving superior response quality while maintaining cost efficiency.\n  Structured Summary:\n    - Inefficient LLM routing in edge-cloud environments: The paper addresses the challenge of determining when to offload queries from edge devices to cloud-based LLMs. Existing routing methods rely on human preference data or accuracy metrics, which are either subjective and resource-intensive or incomplete as performance indicators, failing to capture response confidence and quality.\n    - Balancing cost efficiency and response quality in hybrid deployments: The authors are motivated by the need to optimize hybrid edge-cloud AI deployments where small models run on-device and larger models remain in the cloud. Effective routing is essential to minimize unnecessary cloud calls and latency while maintaining high response quality for complex queries.\n    - Confidence-Driven LLM Router using semantic entropy: The proposed framework uses semantic entropy as an uncertainty measure to guide routing decisions. It clusters semantically equivalent generated outputs and computes entropy over their aggregated probabilities, enabling the router to offload queries to cloud-based LLMs when higher certainty is needed while keeping confident responses on-device.\n    - Three-phase router training system: The system design includes three phases: preparing training data from factual datasets, creating preference data by comparing semantic entropy scores across models with a normalized difference metric and threshold, and training router classifiers using vectorized instruction representations from the preference dataset.\n\nPaper 6:\n  Title: Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models\n  Abstract: Large Vision-Language Models (LVLMs) demonstrate exceptional performance across multimodal tasks, yet remain vulnerable to jailbreak attacks that bypass built-in safety mechanisms to elicit restricted content generation. Existing black-box jailbreak methods primarily rely on adversarial textual prompts or image perturbations, yet these approaches are highly detectable by standard content filtering systems and exhibit low query and computational efficiency. In this work, we present Cross-modal Adversarial Multimodal Obfuscation (CAMO), a novel black-box jailbreak attack framework that decomposes malicious prompts into semantically benign visual and textual fragments. By leveraging LVLMs' cross-modal reasoning abilities, CAMO covertly reconstructs harmful instructions through multi-step reasoning, evading conventional detection mechanisms. Our approach supports adjustable reasoning complexity and requires significantly fewer queries than prior attacks, enabling both stealth and efficiency. Comprehensive evaluations conducted on leading LVLMs validate CAMO's effectiveness, showcasing robust performance and strong cross-model transferability. These results underscore significant vulnerabilities in current built-in safety mechanisms, emphasizing an urgent need for advanced, alignment-aware security and safety solutions in vision-language systems.\n  Structured Summary:\n    - Vulnerability of LVLMs to jailbreak attacks despite safety mechanisms: The paper addresses the critical security threat that current LVLMs can be exploited through jailbreak attacks to generate harmful content, bypassing existing safety constraints. Existing black-box methods are highly detectable and inefficient.\n    - Need for advanced red-teaming to identify LVLM vulnerabilities: The authors are motivated by the necessity to proactively discover and understand attack vectors in LVLMs through red-teaming, enabling researchers to build stronger defenses before malicious actors can exploit these systems in real-world deployments.\n    - Cross-modal Adversarial Multimodal Obfuscation (CAMO) framework: CAMO is a black-box jailbreak framework that splits harmful instructions into benign-looking textual and visual components. These fragments are reconstructed by LVLMs through multi-step cross-modal reasoning, evading conventional detection while maintaining attack effectiveness.\n    - Four-stage attack pipeline with keyword selection and decomposition: The method operates through four stages: identifying sensitive keywords via POS tagging and dictionaries, decomposing keywords into partially masked text and symbolically encoded visual puzzles, combining them into multimodal prompts, and dynamically adjusting obfuscation difficulty.\n\nPaper 7:\n  Title: How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions\n  Abstract: Large language models (LLMs) can \"lie\", which we define as outputting false statements despite \"knowing\" the truth in a demonstrable sense. LLMs might \"lie\", for example, when instructed to output misinformation. Here, we develop a simple lie detector that requires neither access to the LLM's activations (black-box) nor ground-truth knowledge of the fact in question. The detector works by asking a predefined set of unrelated follow-up questions after a suspected lie, and feeding the LLM's yes/no answers into a logistic regression classifier. Despite its simplicity, this lie detector is highly accurate and surprisingly general. When trained on examples from a single setting -- prompting GPT-3.5 to lie about factual questions -- the detector generalises out-of-distribution to (1) other LLM architectures, (2) LLMs fine-tuned to lie, (3) sycophantic lies, and (4) lies emerging in real-life scenarios such as sales. These results indicate that LLMs have distinctive lie-related behavioural patterns, consistent across architectures and contexts, which could enable general-purpose lie detection.\n  Structured Summary:\n    - Detecting lies in black-box LLMs without ground-truth knowledge: The paper addresses the challenge of detecting when LLMs output false statements despite demonstrably knowing the truth, particularly in black-box settings where model activations are inaccessible and ground-truth knowledge is unavailable.\n    - Reducing risks from lying LLMs in real-world scenarios: The authors are motivated by the extensive risks posed by lying LLMs, including malicious uses by bad actors for scams and disinformation, and unintentional lying learned through training. Automated lie detection could mitigate these risks similar to how spam filters work.\n    - Black-box lie detector using unrelated elicitation questions: The method asks a fixed set of yes/no elicitation questions unrelated to the suspected lie, then feeds the LLM's answers into a logistic regression classifier trained on GPT-3.5 examples to detect lies without requiring access to model activations.\n    - Question-answering dataset for lie generation and detection: The authors create a comprehensive dataset of over 20,000 questions covering diverse topics including general knowledge, mathematics, reasoning, translations, self-knowledge, and synthetic facts to generate and test lie detection across various contexts.\n\nPaper 8:\n  Title: Probabilistically Correct Language-based Multi-Robot Planning using Conformal Prediction\n  Abstract: This paper addresses task planning problems for language-instructed robot teams. Tasks are expressed in natural language (NL), requiring the robots to apply their capabilities at various locations and semantic objects. Several recent works have addressed similar planning problems by leveraging pre-trained Large Language Models (LLMs) to design effective multi-robot plans. However, these approaches lack performance guarantees. To address this challenge, we introduce a new distributed LLM-based planner, called S-ATLAS for Safe plAnning for Teams of Language-instructed AgentS, that is capable of achieving user-defined mission success rates. This is accomplished by leveraging conformal prediction (CP), a distribution-free uncertainty quantification tool in black-box models. CP allows the proposed multi-robot planner to reason about its inherent uncertainty in a distributed fashion, enabling robots to make individual decisions when they are sufficiently certain and seek help otherwise. We show, both theoretically and empirically, that the proposed planner can achieve user-specified task success rates, assuming successful plan execution, while minimizing the overall number of help requests. We provide comparative experiments against related works showing that our method is significantly more computational efficient and achieves lower help rates. The advantage of our algorithm over baselines becomes more pronounced with increasing robot team size.\n  Structured Summary:\n    - Lack of performance guarantees in LLM-based multi-robot planning: Existing LLM-based multi-robot planners cannot provide guarantees on mission success rates and tend to generate incorrect outputs with high confidence, lacking mechanisms to reason about their inherent uncertainty.\n    - Achieving user-specified mission success rates for language-instructed robot teams: The authors aim to develop planners that can guarantee desired mission completion rates by enabling robots to reason about LLM uncertainties, allowing them to make decisions when confident and seek help otherwise.\n    - S-ATLAS: Distributed LLM-based planner with conformal prediction: The proposed planner uses conformal prediction to construct local prediction sets for each robot in a distributed fashion, enabling robots to determine when they are uncertain about decisions and need to seek assistance from teammates or users.\n    - Multiple-choice question-answering framework for action selection: The planner frames action selection as MCQA problems where LLMs choose from valid robot skills, mitigating hallucination risks by ensuring only valid actions are considered as choices.\n\nPaper 9:\n  Title: Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning\n  Abstract: Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our uncertainty estimation and decision-making agent design offer a cost-efficient approach for AI agent development.\n  Structured Summary:\n    - Uncertainty quantification for black-box LLMs in decision planning: Current uncertainty quantification methods for large language models either require access to internal model components (white-box) or involve high computational costs through multiple inferences, preventing practical use of proprietary black-box LLMs like GPT-4 for AI agent development under budget constraints.\n    - Enabling cost-efficient AI agent development with proprietary LLMs: The authors aim to enable practical AI agent development using powerful proprietary language models by providing an efficient uncertainty quantification method that works without requiring access to model internals or expensive multiple inferences, thus making black-box LLMs viable for step-by-step decision planning applications.\n    - Non-parametric point-wise dependency neural estimation: The authors propose using point-wise dependency neural estimation to compute the ratio p(x,y)/(p(x)p(y)) between user prompts/history and actions through a single inference from an auxiliary neural network, without requiring access to token logits. This provides a statistically interpretable measure where values above 1.0 indicate correlation.\n    - Decision-making agent with conformal prediction: The authors design a three-stage system (data collection, model training, deployment) that combines instruction-fine-tuned LLMs with point-wise dependency estimation and conformal prediction to generate actions step-by-step, requesting user input when multiple high-confidence actions exist and stopping when no actions exceed the confidence threshold.\n\nPaper 10:\n  Title: Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity\n  Abstract: Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or even unsafe in critical scenarios. Additionally, inherent ambiguity in natural language instructions can introduce uncertainty into the LLM's reasoning and planning this http URL propose introspective planning, a systematic approach that align LLM's uncertainty with the inherent ambiguity of the task. Our approach constructs a knowledge base containing introspective reasoning examples as post-hoc rationalizations of human-selected safe and compliant plans, which are retrieved during deployment. Evaluations on three tasks, including a newly introduced safe mobile manipulation benchmark, demonstrate that introspection substantially improves both compliance and safety over state-of-the-art LLM-based planning methods. Furthermore, we empirically show that introspective planning, in combination with conformal prediction, achieves tighter confidence bounds, maintaining statistical success guarantees while minimizing unnecessary user clarification requests. The webpage and code are accessible at this https URL.\n  Structured Summary:\n    - LLM hallucination and ambiguity in robot planning: The paper addresses the challenge that large language models used for robot planning can hallucinate and generate unsafe or misaligned plans, while natural language ambiguity introduces uncertainty that models fail to properly calibrate, leading to overconfident or incorrect decisions.\n    - Need for uncertainty alignment with task ambiguity: The authors are motivated by the need for robots to accurately assess their own uncertainty in ways that align with inherent task ambiguity, enabling them to either execute correct actions or appropriately request clarification from users in safety-critical scenarios.\n    - Introspective planning with knowledge base construction: The method constructs a knowledge base by prompting LLMs to generate post-hoc reasoning explanations conditioned on human-provided valid action labels. During inference, relevant reasoning examples are retrieved to guide the LLM in explicitly reasoning about task compliance and safety for multiple candidate plans.\n    - Integration with conformal prediction: The authors integrate introspective planning with conformal prediction to provide statistical guarantees that the correct action is included in the prediction set with a specified confidence level, while reducing conservativeness and unnecessary queries compared to prior conformal prediction approaches.\n\nPaper 11:\n  Title: Knowing When to Ask -- Bridging Large Language Models and Data\n  Abstract: Large Language Models (LLMs) are prone to generating factually incorrect information when responding to queries that involve numerical and statistical data or other timely facts. In this paper, we present an approach for enhancing the accuracy of LLMs by integrating them with Data Commons, a vast, open-source repository of public statistics from trusted organizations like the United Nations (UN), Center for Disease Control and Prevention (CDC) and global census bureaus. We explore two primary methods: Retrieval Interleaved Generation (RIG), where the LLM is trained to produce natural language queries to retrieve data from Data Commons, and Retrieval Augmented Generation (RAG), where relevant data tables are fetched from Data Commons and used to augment the LLM's prompt. We evaluate these methods on a diverse set of queries, demonstrating their effectiveness in improving the factual accuracy of LLM outputs. Our work represents an early step towards building more trustworthy and reliable LLMs that are grounded in verifiable statistical data and capable of complex factual reasoning.\n  Structured Summary:\n    - LLM hallucinations on numerical and statistical facts: The paper addresses the challenge that LLMs frequently produce factually incorrect statements (hallucinations) when answering queries involving numerical, statistical, or timely data, lacking accurate citations and sufficient factual coverage in training data.\n    - Need for trustworthy LLMs grounded in verifiable data: The authors aim to improve LLM trustworthiness and reliability by grounding model outputs in verifiable public statistical data, enabling complex factual reasoning with authoritative sources rather than relying solely on parametric knowledge.\n    - Retrieval Interleaved Generation (RIG): A tool-inspired approach where the LLM is fine-tuned to generate natural language Data Commons queries alongside statistics. A pipeline converts these queries into structured data queries to retrieve authoritative values from Data Commons, which are presented alongside LLM-generated statistics for fact-checking.\n    - Retrieval Augmented Generation (RAG): A retrieval approach where a fine-tuned LLM extracts variables from user queries, retrieves relevant tables from Data Commons, and augments the original query with this context before prompting a long-context LLM (Gemini 1.5 Pro) to generate the final answer.\n\nPaper 12:\n  Title: Evaluating Uncertainty-based Failure Detection for Closed-Loop LLM Planners\n  Abstract: Recently, Large Language Models (LLMs) have witnessed remarkable performance as zero-shot task planners for robotic manipulation tasks. However, the open-loop nature of previous works makes LLM-based planning error-prone and fragile. On the other hand, failure detection approaches for closed-loop planning are often limited by task-specific heuristics or following an unrealistic assumption that the prediction is trustworthy all the time. As a general-purpose reasoning machine, LLMs or Multimodal Large Language Models (MLLMs) are promising for detecting failures. However, However, the appropriateness of the aforementioned assumption diminishes due to the notorious hullucination problem. In this work, we attempt to mitigate these issues by introducing a framework for closed-loop LLM-based planning called KnowLoop, backed by an uncertainty-based MLLMs failure detector, which is agnostic to any used MLLMs or LLMs. Specifically, we evaluate three different ways for quantifying the uncertainty of MLLMs, namely token probability, entropy, and self-explained confidence as primary metrics based on three carefully designed representative prompting strategies. With a self-collected dataset including various manipulation tasks and an LLM-based robot system, our experiments demonstrate that token probability and entropy are more reflective compared to self-explained confidence. By setting an appropriate threshold to filter out uncertain predictions and seek human help actively, the accuracy of failure detection can be significantly enhanced. This improvement boosts the effectiveness of closed-loop planning and the overall success rate of tasks.\n  Structured Summary:\n    - Fragility and error-proneness of open-loop LLM-based planning: Existing LLM-based robot planners operate in open-loop mode, rigidly following initial plans without incorporating environmental feedback. This leads to unaddressed execution errors or plan deficiencies, potentially causing task failures.\n    - Need for trustworthy failure detection without unrealistic assumptions: Current failure detection methods either rely on task-specific heuristics with limited generalizability or assume that LLM/MLLM predictions are always trustworthy, which is problematic given the hallucination issues in these models. A more general and reliable approach is needed.\n    - KnowLoop framework with uncertainty-based MLLM failure detector: The authors propose KnowLoop, a closed-loop planning framework that incorporates an uncertainty-based failure detection module using MLLMs. The framework is model-agnostic and filters out uncertain predictions, actively seeking human help when confidence is low.\n    - Three uncertainty quantification approaches for MLLMs: The authors evaluate three methods to quantify MLLM uncertainty: token probability, entropy, and self-explained confidence. These are assessed using three prompting strategies (direct via subgoal state comparison, direct via spatial relationship analysis, and indirect via next action prediction).\n\nPaper 13:\n  Title: Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners\n  Abstract: Large language models (LLMs) exhibit a wide range of promising capabilities -- from step-by-step planning to commonsense reasoning -- that may provide utility for robots, but remain prone to confidently hallucinated predictions. In this work, we present KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed. KnowNo builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help in complex multi-step planning settings. Experiments across a variety of simulated and real robot setups that involve tasks with different modes of ambiguity (e.g., from spatial to numeric uncertainties, from human preferences to Winograd schemas) show that KnowNo performs favorably over modern baselines (which may involve ensembles or extensive prompt tuning) in terms of improving efficiency and autonomy, while providing formal assurances. KnowNo can be used with LLMs out of the box without model-finetuning, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models. Website: this https URL\n  Structured Summary:\n    - Uncertainty alignment for LLM-based robot planners: The paper addresses the challenge that large language models used for robot planning tend to confidently hallucinate incorrect outputs, which is problematic when robots must operate reliably in ambiguous real-world environments. The core issue is that LLMs lack proper uncertainty quantification, leading to potentially unsafe or incorrect actions.\n    - Enabling robots to know when they don't know: The authors are motivated by the longstanding challenge of enabling robots to accurately model uncertainty so they can operate reliably in unstructured environments. They aim to allow robots to recognize when they are uncertain and appropriately seek human clarification, rather than confidently executing incorrect plans.\n    - KNOWNO framework using conformal prediction: The authors propose KNOWNO, a framework that applies conformal prediction theory to LLM-based robot planners. The method generates prediction sets of possible actions and triggers human help when the set is not a singleton, providing statistical guarantees on task success while minimizing the amount of human intervention required.\n    - Planning as multiple-choice question answering: The authors reformulate LLM planning as a multiple-choice question answering task. The LLM first generates diverse candidate next steps, then evaluates them through MCQA format. This eliminates length bias in probability estimates and aligns with LLM training objectives, enabling better uncertainty quantification through normalized scores.\n\nPaper 14:\n  Title: TruthfulQA: Measuring How Models Mimic Human Falsehoods\n  Abstract: We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.\n  Structured Summary:\n    - Language models generate false statements mimicking human falsehoods: The paper addresses the problem that current language models frequently produce false statements, including subtle inaccuracies and hallucinations, which raises concerns about accidental misuse, blocking positive applications, and potential malicious misuse for deception.\n    - Need to quantify and measure model truthfulness to enable safer deployment: The authors are motivated by the need to systematically measure how truthful language models are across diverse contexts, as better measurement tools will facilitate development of more truthful models and help understand risks of deceptive model deployment.\n    - TruthfulQA benchmark with 817 adversarially-designed questions: The authors propose TruthfulQA, a benchmark consisting of 817 questions across 38 categories, specifically designed to elicit 'imitative falsehoods' where models might generate false answers learned from imitating human texts containing misconceptions.\n    - GPT-judge automated evaluation metric: The authors developed GPT-judge, a finetuned GPT-3-6.7B model that classifies answers as true or false, achieving 90-96% validation accuracy and providing a cost-effective alternative to human evaluation for measuring truthfulness.\n\nPaper 15:\n  Title: FEVER: a large-scale dataset for Fact Extraction and VERification\n  Abstract: In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss $\\kappa$. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.\n  Structured Summary:\n    - Lack of large-scale datasets for textual claim verification: Existing datasets for claim verification are extremely small (e.g., the Fake News Challenge used only 300 claims), which is orders of magnitude smaller than datasets available for related tasks like textual entailment and question answering, limiting progress in developing verification systems.\n    - Need for verification systems against textual sources: The growing volume of textual information and ease of web sharing has created urgent demand for verification systems that can check claims against textual sources, with applications in journalism, scientific publications, product reviews, and other domains.\n    - Two-stage dataset construction process: The authors developed a two-stage annotation methodology where annotators first extract and mutate claims from Wikipedia sentences, then separately verify these claims by retrieving evidence and classifying them as SUPPORTED, REFUTED, or NOTENOUGHINFO without knowing the original source sentence.\n    - Claim mutation framework with six operators: To avoid trivially verifiable claims, the authors introduced six mutation types inspired by Natural Logic Inference operators that allow annotators to generate altered versions of claims with varying verifiability, increasing dataset complexity in a controlled manner.\n\nPaper 16:\n  Title: HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models\n  Abstract: Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation benchmark for Large Language Models (HaluEval), a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about $19.5\\%$ responses). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. However, our experiments also prove that providing external knowledge or adding reasoning steps can help LLMs recognize hallucinations. Our benchmark can be accessed at this https URL.\n  Structured Summary:\n    - Lack of understanding of hallucination extent and types in LLMs: The paper addresses the gap in understanding which content types and to what degree large language models are prone to generating hallucinations, i.e., content conflicting with sources or unverifiable by factual knowledge.\n    - Hallucination poses deployment risks for LLMs in real-world applications: The authors are motivated by the fact that hallucinations in LLM outputs create safety and reliability concerns, hindering their practical deployment despite their impressive language capabilities.\n    - Two-stage sampling-then-filtering framework for automatic hallucination generation: The authors propose a two-step automated approach: first, diverse hallucination sampling using one-pass and conversational instruction schemas with ChatGPT; second, high-quality filtering using ChatGPT with ground-truth-enhanced instructions to select the most plausible and difficult hallucinated samples.\n    - Human annotation of ChatGPT responses for general user queries: The authors employ human labelers to annotate whether ChatGPT responses to general user queries contain hallucinated information, using a pre-selection procedure based on response divergence to identify queries most likely to produce hallucinations.\n\nPaper 17:\n  Title: Teaching Models to Express Their Uncertainty in Words\n  Abstract: We show that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits. When given a question, the model generates both an answer and a level of confidence (e.g. \"90% confidence\" or \"high confidence\"). These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift, and is sensitive to uncertainty in its own answers, rather than imitating human examples. To our knowledge, this is the first time a model has been shown to express calibrated uncertainty about its own answers in natural language. For testing calibration, we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words (\"verbalized probability\") to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT-3's ability to generalize calibration depends on pre-trained latent representations that correlate with epistemic uncertainty over its answers.\n  Structured Summary:\n    - Language models produce false statements without expressing uncertainty: Current state-of-the-art language models frequently generate false statements or hallucinations in long-form text, and users cannot distinguish when the model is truthful versus when it is not, reducing the model's practical value.\n    - Calibrated uncertainty enables users to trust model statements appropriately: Calibration is motivated by the need for truthfulness: if models can express calibrated uncertainty about their statements, users can appropriately gauge trust in those statements, which is important for current hallucination-prone models and for statements where no ground truth exists.\n    - Finetuning GPT-3 to express verbalized probability in natural language: The authors finetune GPT-3 using supervised learning to express epistemic uncertainty about its own answers in natural language (called verbalized probability), where the model generates both an answer and a confidence level (e.g. '90% confidence' or 'high confidence') that maps to calibrated probabilities.\n    - CalibratedMath test suite for evaluating calibration under distribution shift: The authors introduce CalibratedMath, a suite of 21 elementary mathematics tasks with programmatically generated questions and answers, designed to test how calibration generalizes under distribution shift by varying question types, difficulty levels, and whether questions have single or multiple correct answers.\n\nPaper 18:\n  Title: HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\n  Abstract: Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems' ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.\n  Structured Summary:\n    - Limitations of existing QA datasets in training complex reasoning and explainability: Current QA datasets do not adequately test systems' ability to perform multi-hop reasoning across multiple documents, are constrained by knowledge base schemas, and provide only distant supervision without indicating which supporting facts lead to answers, hindering explainability.\n    - Need for diverse, explainable multi-hop reasoning in natural language: The authors seek to advance machine reasoning over natural language by creating a dataset that tests multi-hop reasoning without KB constraints and provides strong supervision through supporting facts, enabling systems to learn meaningful reasoning processes and make explainable predictions.\n    - Wikipedia hyperlink graph-based data collection pipeline: The authors construct a directed graph from Wikipedia hyperlinks and generate candidate paragraph pairs by sampling edges where the target entity serves as a bridge entity, facilitating the collection of meaningful multi-hop questions that require reasoning across multiple documents.\n    - Collection of sentence-level supporting facts: The dataset includes sentence-level supporting facts annotated by crowd workers, which serve as strong supervision for training explainable QA systems and enable evaluation of model explainability by comparing predicted supporting facts to ground truth.\n\nPaper 19:\n  Title: Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies\n  Abstract: A key limitation in current datasets for multi-hop reasoning is that the required steps for answering the question are mentioned in it explicitly. In this work, we introduce StrategyQA, a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. A fundamental challenge in this setup is how to elicit such creative questions from crowdsourcing workers, while covering a broad range of potential strategies. We propose a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts. Moreover, we annotate each question with (1) a decomposition into reasoning steps for answering it, and (2) Wikipedia paragraphs that contain the answers to each step. Overall, StrategyQA includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Analysis shows that questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies. Empirically, we show that humans perform well (87%) on this task, while our best baseline reaches an accuracy of $\\sim$66%.\n  Structured Summary:\n    - Explicit reasoning steps in multi-hop QA datasets: Existing multi-hop reasoning datasets explicitly describe the reasoning process in the question language, whereas real-life questions often require implicit reasoning where the strategy must be inferred by the model.\n    - Need for implicit reasoning in QA systems: The authors are motivated by the gap between explicit multi-hop questions in existing benchmarks and real-world questions that require models to infer implicit reasoning strategies, posing challenges in retrieval and preventing exploitation of language shortcuts.\n    - Term-based priming for creative question generation: The authors propose priming annotators with single Wikipedia terms rather than full contexts to encourage creative question writing with minimal lexical overlap, reducing biases and shortcuts in the collected data.\n    - Adversarial filtering with continuously updated models: The method involves deploying pre-trained and continuously fine-tuned models during data collection to reject questions that are too easy or exhibit recurring patterns, ensuring challenging and diverse questions.\n\nPaper 20:\n  Title: CoQA: A Conversational Question Answering Challenge\n  Abstract: Humans gather information by engaging in conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning. We evaluate strong conversational and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 points behind human performance (88.8%), indicating there is ample room for improvement. We launch CoQA as a challenge to the community at this http URL\n  Structured Summary:\n    - Lack of conversational question answering datasets: Existing reading comprehension datasets do not contain questions that depend on conversation history, making it difficult to build systems that can answer conversational questions where later questions rely on previous context.\n    - Enabling machines to assist in information gathering through conversation: Since humans naturally gather information through conversational exchanges with interconnected questions and answers, machines need the ability to answer conversational questions to effectively assist in information gathering tasks.\n    - CoQA dataset with conversational questions and free-form answers: The authors introduce CoQA, a dataset containing 127k question-answer pairs from 8k conversations about passages from seven domains. Questions are conversational and answers are free-form text with corresponding evidence spans highlighted in passages.\n    - Data collection interface with questioner-answerer pairing: The authors employ a two-annotator setup where one acts as questioner and another as answerer, which enables natural dialogue flow, on-the-spot validation, and helps prevent spam while obtaining high agreement data.\n\nPaper 21:\n  Title: Neural Text Generation from Structured Data with Application to the Biography Domain\n  Abstract: This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with a new dataset of biographies from Wikipedia that is an order of magnitude larger than existing resources with over 700k samples. The dataset is also vastly more diverse with a 400k vocabulary, compared to a few hundred words for Weathergov or Robocup. Our model builds upon recent work on conditional neural language model for text generation. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that transfer sample-specific words from the input database to the generated output sentence. Our neural model significantly out-performs a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU.\n  Structured Summary:\n    - Scaling concept-to-text generation to large, rich domains: Existing concept-to-text generation systems operate on small datasets with limited vocabularies (tens of thousands of records, ~350 words), whereas real-world applications like biography generation require handling much larger and more diverse data (over 700k records, 400k+ vocabulary).\n    - Addressing data sparsity in large-scale generation: The authors are motivated by the difficulty classical count-based models face when estimating probabilities of rare events in diverse, large-scale domains. They propose using embeddings and neural language models to achieve better generalization and handle data sparsity more effectively.\n    - Table-conditioned neural language model with local and global conditioning: The authors propose a neural language model that conditions on structured table data through two mechanisms: local conditioning (describing previous words via their table field and position) and global conditioning (summarizing all table fields and words to capture high-level themes).\n    - Copy actions for handling large vocabularies: The model extends the output vocabulary by introducing copy actions that allow it to emit special field tokens (e.g., name_1, name_2) which are then replaced by the corresponding words from the input table, enabling generation of out-of-vocabulary words.\n\nPaper 22:\n  Title: TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension\n  Abstract: We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- this http URL\n  Structured Summary:\n    - Limited challenge in existing reading comprehension datasets: Existing reading comprehension datasets are being solved too quickly by state-of-the-art models, which often reach near-human performance within a short time, creating a need for more challenging benchmarks that better test complex reasoning, lexical variation, and multi-sentence inference.\n    - Need for datasets testing multiple comprehension challenges simultaneously: The authors aim to create a dataset that simultaneously tests complex compositional questions, lexical and syntactic variability, and multi-sentence reasoning, addressing limitations of prior datasets that typically focus on only one challenge at a time.\n    - Distant supervision through independent question-evidence collection: The method involves collecting trivia questions authored independently of evidence documents, then retrospectively gathering supporting evidence from Wikipedia and Web search results. This decoupling allows control over potential bias while providing organically generated questions across diverse topics.\n    - Two-domain evidence collection strategy: The authors employ different distant supervision strategies for Web and Wikipedia domains: treating each question-answer-document tuple as independent for redundant Web results, while pooling all evidence documents for Wikipedia where facts typically appear once.\n\nPaper 23:\n  Title: Distinguishing the Knowable from the Unknowable with Language Models\n  Abstract: We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more informative indicators of model confidence in diverse practical settings.\n  Structured Summary:\n    - Distinguishing epistemic from aleatoric uncertainty in LLM outputs: The paper addresses the challenge of determining whether uncertainty in language model predictions arises from lack of knowledge (epistemic) or inherent randomness in the data distribution (aleatoric), particularly in unconstrained free-form text generation where ground truth is unavailable.\n    - Reducing hallucinations by identifying epistemic uncertainty: The authors are motivated by the problem of LLM hallucinations, hypothesizing that many hallucinations occur when models sample tokens under epistemic uncertainty. Identifying such uncertainty could enable interventions to improve model truthfulness or highlight uncertain tokens in user interfaces.\n    - Supervised linear probes on frozen model embeddings: The authors train small linear classifiers on intermediate embeddings from a smaller language model to predict when a larger model will exhibit high or low entropy. These probes are trained on labeled data generated by comparing small and large model predictions, and they generalize across text domains.\n    - In-Context Learning Test (ICLT) for unsupervised uncertainty classification: The authors propose ICLT, an unsupervised method that measures how much a model updates its predictions when provided with candidate tokens in context. The hypothesis is that models exhibit different in-context learning behaviors depending on whether their uncertainty is epistemic or aleatoric, being more suggestible under epistemic uncertainty.\n\nPaper 24:\n  Title: LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models\n  Abstract: When answering questions, LLMs can convey not only an answer, but a level of confidence about the answer being correct. This includes explicit confidence markers (e.g. giving a numeric score) as well as implicit markers, like an authoritative tone or elaborating with additional knowledge. For LLMs to be trustworthy knowledge sources, the confidence they convey should match their actual expertise; however, most current models tend towards overconfidence. To calibrate both implicit and explicit confidence markers, we introduce a pragmatic, listener-aware finetuning method (LACIE) that models the listener, considering not only whether an answer is right, but whether it will be accepted by a listener. We cast calibration as preference optimization, creating data via a two-agent game, where a speaker model's outputs are judged by a simulated listener. We then finetune three LLMs (Mistral-7B, Llama3-8B, Llama3-70B) with LACIE, and show that the resulting models are better calibrated w.r.t. a simulated listener. Crucially, these trends transfer to human listeners, helping them correctly predict model correctness: we conduct a human evaluation where annotators accept or reject an LLM's answers, finding that training with LACIE results in 47% fewer incorrect answers being accepted while maintaining the same level of acceptance for correct answers. Furthermore, LACIE generalizes to another dataset, resulting in a large increase in truthfulness on TruthfulQA when trained on TriviaQA. Our analysis indicates that LACIE leads to a better confidence separation between correct and incorrect examples. Qualitatively, we find that a LACIE-trained model hedges more and implicitly signals certainty when it is correct by using an authoritative tone or including details. Finally, LACIE finetuning leads to an emergent increase in model abstention (e.g. saying \"I don't know\") for answers that are likely wrong.\n  Structured Summary:\n    - Overconfidence in large language models: The paper addresses the issue of large language models (LLMs) being overconfident in their responses, which can mislead users and erode trust.\n    - Improving trustworthiness of LLMs: The authors aim to make LLMs more trustworthy by ensuring their expressed confidence aligns with their actual correctness, as users increasingly depend on these models for information.\n    - Listener-aware finetuning (LACIE): The authors propose LACIE, a finetuning method that optimizes LLMs to express confidence calibrated to how a listener perceives correctness, using a multi-agent speaker-listener framework.\n    - Multi-agent speaker-listener game: The method involves a two-agent game where a speaker model generates responses and a listener model evaluates them, creating preference data to optimize calibration.\n\nPaper 25:\n  Title: Linguistic Calibration of Long-Form Generations\n  Abstract: Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce long-form text with calibrated confidence statements. Through the lens of decision-making, we define linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as \"I estimate a 30% chance of...\" or \"I am certain that...\", followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions. We linguistically calibrate Llama 2 7B and find in automated and human evaluations of long-form generations that it is significantly more calibrated than strong finetuned factuality baselines with comparable accuracy. These findings generalize under significant domain shifts to scientific and biomedical questions and to an entirely held-out person biography generation task. Our results demonstrate that long-form generations may be calibrated end-to-end by constructing an objective in the space of the predictions that users make in downstream decision-making.\n  Structured Summary:\n    - Confident hallucinations in long-form LM generations lead to suboptimal user decisions: The paper addresses the problem that language models often hallucinate incorrect claims with complete confidence in long-form generations, causing users to make poor decisions. Existing calibration methods are restricted to classification or short outputs and cannot handle the multiple claims made in long-form generations.\n    - Enabling better user decisions through calibrated confidence statements: The authors are motivated by the harmful consequences of confident hallucinations, particularly in high-stakes domains like medicine and law. By enabling LMs to convey calibrated confidence levels in natural language, users can make better decisions regardless of the LM's underlying knowledge gaps.\n    - Definition of linguistic calibration for long-form generations: The authors define an LM emitting long-form generations to be linguistically calibrated if it enables users to produce calibrated forecasts relevant to their decision-making tasks. This definition reduces the problem of calibrating long-form text to calibrating probabilistic forecasts over a smaller answer space.\n    - Two-step training framework with summary distillation and decision-based RL: The training framework first uses summary distillation to bootstrap an LM to express confidence statements via supervised finetuning on frequency-based summaries of multiple samples. Then, decision-based RL optimizes a proper scoring rule objective that rewards generations enabling calibrated user forecasts on related questions.\n\nPaper 26:\n  Title: Reducing conversational agents' overconfidence through linguistic calibration\n  Abstract: While improving neural dialogue agents' factual accuracy is the object of much research, another important aspect of communication, less studied in the setting of neural dialogue, is transparency about ignorance. In this work, we analyze to what extent state-of-the-art chit-chat models are linguistically calibrated in the sense that their verbalized expression of doubt (or confidence) matches the likelihood that the model's responses are factually incorrect (or correct). We find that these models are poorly calibrated, yet we show that likelihood of correctness can accurately be predicted. By incorporating such metacognitive features into the training of a controllable generation model, we obtain a dialogue agent with greatly improved linguistic calibration. While improving neural dialogue agents' factual accuracy is the object of much research, another important aspect of communication, less studied in the setting of neural dialogue, is transparency about ignorance. In this work, we analyze to what extent state-of-the-art chit-chat models are linguistically calibrated in the sense that their verbalized expression of doubt (or confidence) matches the likelihood that the model's responses are factually incorrect (or correct). We find that these models are poorly calibrated, yet we show that likelihood of correctness can accurately be predicted. By incorporating such metacognitive features into the training of a controllable generation model, we obtain a dialogue agent with greatly improved linguistic calibration.\n  Structured Summary:\n    - Poor linguistic calibration in neural dialogue agents: State-of-the-art conversational models express linguistic confidence (e.g., 'Obviously...') that does not match the actual likelihood their answers are factually correct, leading to overconfident incorrect responses that can mislead users.\n    - Transparency about model ignorance in dialogue: While factual accuracy improvements are widely studied, the authors emphasize the importance of models being transparent about their limitations directly in dialogue, applying Grice's maxim of quality on a metacognitive level to be truthful about what the model knows.\n    - Calibrator for predicting answer correctness: A trained classifier that predicts the probability an answer is correct using the question, answer, and the vanilla model's internal encoder and decoder representations, without access to gold answers. Trained on 50,000 automatically annotated TriviaQA examples.\n    - Two-stage controllable generation fine-tuning: A two-stage fine-tuning approach: Stage 1 trains a confidence-controllable model using control tokens (<DK>, <LO>, <HI>) on TriviaQA augmented with linguistic confidence labels. Stage 2 adds content control (<SAME>/<DIFF>) to adjust linguistic confidence without changing answer substance.\n\nPaper 27:\n  Title: When to Trust LLMs: Aligning Confidence with Response Quality\n  Abstract: Despite the success of large language models (LLMs) in natural language generation, much evidence shows that LLMs may produce incorrect or nonsensical text. This limitation highlights the importance of discerning when to trust LLMs, especially in safety-critical domains. Existing methods often express reliability by confidence level, however, their effectiveness is limited by the lack of objective guidance. To address this, we propose CONfidence-Quality-ORDer-preserving alignment approach (CONQORD), which leverages reinforcement learning guided by a tailored dual-component reward function. This function integrates quality reward and order-preserving alignment reward functions. Specifically, the order-preserving reward incentivizes the model to verbalize greater confidence for responses of higher quality to align the order of confidence and quality. Experiments demonstrate that CONQORD significantly improves the alignment performance between confidence and response accuracy, without causing over-cautious. Furthermore, the aligned confidence provided by CONQORD informs when to trust LLMs, and acts as a determinant for initiating the retrieval process of external knowledge. Aligning confidence with response quality ensures more transparent and reliable responses, providing better trustworthiness.\n  Structured Summary:\n    - Misalignment between LLM confidence and response quality: Current methods for expressing LLM reliability through confidence levels suffer from misalignment between the stated confidence and actual response quality, primarily due to the absence of objective standards that directly reflect response quality in generative tasks.\n    - Determining when to trust LLMs in safety-critical domains: The authors are motivated by the need to prevent uncritical acceptance of misleading information from LLMs and to guide decisions on when to rely on LLMs versus when to seek external knowledge, particularly in applications where errors could have serious consequences.\n    - CONQORD reinforcement learning framework with dual-component reward: CONQORD employs a reinforcement learning framework with a dual-component reward strategy consisting of a quality reward model that rates response quality and an order-preserving alignment reward that encourages ordinal consistency between confidence and quality ratings while penalizing discrepancies.\n    - Order-preserving alignment reward function: The order-preserving reward function is designed to maintain a consistent ordinal relationship between verbalized confidence and response quality by rewarding alignment and penalizing misalignment across sample pairs, promoting careful self-calibration while being robust to outliers.\n\nPaper 28:\n  Title: Can We Trust LLMs? Mitigate Overconfidence Bias in LLMs through Knowledge Transfer\n  Abstract: The study explores mitigating overconfidence bias in LLMs to improve their reliability. We introduce a knowledge transfer (KT) method utilizing chain of thoughts, where \"big\" LLMs impart knowledge to \"small\" LLMs via detailed, sequential reasoning paths. This method uses advanced reasoning of larger models to fine-tune smaller models, enabling them to produce more accurate predictions with calibrated confidence. Experimental evaluation using multiple-choice questions and sentiment analysis across diverse datasets demonstrated the KT method's superiority over the vanilla and question-answer pair (QA) fine-tuning methods. The most significant improvement in three key metrics, where the KT method outperformed the vanilla and QA methods by an average of 55.3% and 43.1%, respectively. These findings underscore the KT method's potential in enhancing model trustworthiness and accuracy, offering precise outputs with well-matched confidence levels across various contexts.\n  Structured Summary:\n    - Overconfidence bias in large language models: The paper addresses the problem that large language models exhibit overconfidence bias, producing high confidence levels even when their predictions are incorrect. This issue is particularly pronounced in models like Vicuna and LLaMA 2, as demonstrated through initial tests.\n    - Improving model reliability and trustworthiness in critical applications: The authors are motivated by the need to improve model accuracy and trustworthiness in critical sectors like law and finance, where overconfident incorrect predictions could lead to costly errors. Addressing this bias is essential for deploying LLMs reliably in real-world applications.\n    - Knowledge transfer method using chain of thoughts: The authors propose a knowledge transfer method where large 'teacher' LLMs like GPT-4 generate detailed chain-of-thought reasoning paths, which are then used to fine-tune smaller 'student' LLMs. This enables smaller models to learn advanced reasoning patterns and produce more accurate predictions with calibrated confidence levels.\n    - Knowledge extraction from big LLMs via CoT generation: The method involves extracting knowledge from large LLMs by having them generate chain-of-thought reasoning that includes detailed, sequential reasoning pathways showing how the model arrives at correct answers. These CoTs are then used as training data for smaller models.\n\nPaper 29:\n  Title: SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales\n  Abstract: Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work elicits confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present the advanced SaySelf, a training framework that teaches LLMs to express more accurate fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results in both in-distribution and out-of-distribution datasets demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. We show that the generated self-reflective rationales are reasonable and can further contribute to the calibration. The code is made public at this https URL.\n  Structured Summary:\n    - LLMs fail to express accurate confidence and explain uncertainty: The paper addresses the challenge that large language models frequently produce hallucinated content and typically do not express their uncertainty when faced with unfamiliar questions, limiting their reliability and trustworthiness in practical applications.\n    - Need for trustworthy AI with clear confidence expression: The authors are motivated by the need to improve trust in AI systems through transparent confidence estimates and explanations. This enables better human-AI interaction, supports proactive learning algorithms, and helps users verify information in LLM responses.\n    - Supervised fine-tuning with self-reflective rationales: The method constructs a model-specific dataset by sampling multiple reasoning chains from an LLM, clustering them based on semantic similarity, and using GPT-4 to summarize inconsistencies as self-reflective rationales. These rationales explain why the model is uncertain and are used for supervised fine-tuning along with confidence estimates derived from response consistency.\n    - Reinforcement learning with calibration reward function: The approach employs PPO-based reinforcement learning with a quadratic reward function that encourages high confidence in correct predictions while penalizing overconfidence in incorrect ones. This further calibrates the confidence estimates beyond what supervised fine-tuning achieves.\n\nPaper 30:\n  Title: CLUE: Concept-Level Uncertainty Estimation for Large Language Models\n  Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in various natural language generation (NLG) tasks. Previous studies suggest that LLMs' generation process involves uncertainty. However, existing approaches to uncertainty estimation mainly focus on sequence-level uncertainty, overlooking individual pieces of information within sequences. These methods fall short in separately assessing the uncertainty of each component in a sequence. In response, we propose a novel framework for Concept-Level Uncertainty Estimation (CLUE) for LLMs. We leverage LLMs to convert output sequences into concept-level representations, breaking down sequences into individual concepts and measuring the uncertainty of each concept separately. We conduct experiments to demonstrate that CLUE can provide more interpretable uncertainty estimation results compared with sentence-level uncertainty, and could be a useful tool for various tasks such as hallucination detection and story generation.\n  Structured Summary:\n    - Information entanglement issue in sequence-level uncertainty estimation: Prior uncertainty estimation methods for LLMs treat entire sequences as single units, failing to distinguish the uncertainty of individual information components within sequences. This limitation prevents nuanced evaluation when sequences contain multiple pieces of information with varying uncertainty levels.\n    - Need for interpretable uncertainty estimation in LLM outputs: Understanding and quantifying uncertainty in LLM outputs is essential for task-specific evaluations. High uncertainty may indicate hallucination in question-answering tasks but could be desirable for diversity in story generation, making interpretable uncertainty measurement critical for various applications.\n    - Concept-Level Uncertainty Estimation (CLUE) framework: CLUE uses LLMs with handcrafted one-shot examples to extract comprehensive concepts from generated sequences. Each concept is treated as an independent unit, evaluated by an NLI-based zero-shot text classifier that assigns entailment scores. Uncertainty is calculated as the average negative logarithm of concept scores across output sequences.\n    - Concept extraction via LLM prompting: The framework employs LLM prompting with handcrafted one-shot examples to extract high-level concepts from sequences. Extracted concepts from multiple output sequences are combined into a unified concept pool, with similar concepts automatically consolidated using an NLI-based classifier.\n\nPaper 31:\n  Title: Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness\n  Abstract: We introduce BSDetector, a method for detecting bad and speculative answers from a pretrained Large Language Model by estimating a numeric confidence score for any output it generated. Our uncertainty quantification technique works for any LLM accessible only via a black-box API, whose training data remains unknown. By expending a bit of extra computation, users of any LLM API can now get the same response as they would ordinarily, as well as a confidence estimate that cautions when not to trust this response. Experiments on both closed and open-form Question-Answer benchmarks reveal that BSDetector more accurately identifies incorrect LLM responses than alternative uncertainty estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple responses from the LLM and considering the one with the highest confidence score, we can additionally obtain more accurate responses from the same LLM, without any extra training steps. In applications involving automated evaluation with LLMs, accounting for our confidence scores leads to more reliable evaluation in both human-in-the-loop and fully-automated settings (across both GPT 3.5 and 4).\n  Structured Summary:\n    - Unreliability and overconfidence of black-box LLMs: Large Language Models accessed via black-box APIs produce convincing but often incorrect answers without indicating uncertainty, making them unreliable for high-stakes applications. Traditional uncertainty estimation methods cannot be applied because training data and model internals are inaccessible.\n    - Enabling safe LLM usage in high-value applications: The authors aim to address the limitation preventing LLMs from being deployed in high-value, high-stakes applications by providing a method to identify when LLM outputs should not be trusted, enabling safer usage through uncertainty quantification.\n    - BSDetector uncertainty quantification technique: BSDetector estimates confidence scores for LLM outputs by calling the API multiple times with varying prompts and temperature values. It combines two factors: Observed Consistency (measuring contradiction among sampled responses) and Self-reflection Certainty (asking the LLM to evaluate its own answer correctness).\n    - Observed Consistency measurement: This component samples multiple diverse responses from the LLM using modified prompts (with Chain-of-Thoughts) and measures semantic similarity between sampled and original answers using natural language inference classification to detect contradictions, combined with an indicator function for closed-form tasks.\n\nPaper 32:\n  Title: Semantic Density: Uncertainty Quantification for Large Language Models through Confidence Measurement in Semantic Space\n  Abstract: With the widespread application of Large Language Models (LLMs) to various domains, concerns regarding the trustworthiness of LLMs in safety-critical scenarios have been raised, due to their unpredictable tendency to hallucinate and generate misinformation. Existing LLMs do not have an inherent functionality to provide the users with an uncertainty/confidence metric for each response it generates, making it difficult to evaluate trustworthiness. Although several studies aim to develop uncertainty quantification methods for LLMs, they have fundamental limitations, such as being restricted to classification tasks, requiring additional training and data, considering only lexical instead of semantic information, and being prompt-wise but not response-wise. A new framework is proposed in this paper to address these issues. Semantic density extracts uncertainty/confidence information for each response from a probability distribution perspective in semantic space. It has no restriction on task types and is \"off-the-shelf\" for new models and tasks. Experiments on seven state-of-the-art LLMs, including the latest Llama 3 and Mixtral-8x22B models, on four free-form question-answering benchmarks demonstrate the superior performance and robustness of semantic density compared to prior approaches.\n  Structured Summary:\n    - Lack of uncertainty quantification for LLM responses: Current large language models generate responses without quantitative indicators of their uncertainty or confidence, making it challenging to assess the trustworthiness of outputs, especially in safety-critical applications where hallucinations and misinformation are concerns.\n    - Need for trustworthy LLMs in safety-critical applications: As LLMs are increasingly deployed in risk-sensitive domains like healthcare and finance, their tendency to hallucinate raises safety concerns. A reliable uncertainty quantification framework is essential to enable users to evaluate response trustworthiness and establish automatic warning systems based on confidence thresholds.\n    - Semantic density framework: The paper proposes semantic density, a framework that quantifies response-wise confidence by analyzing output probability distributions in semantic space. It uses kernel density estimation adapted for LLM outputs, incorporating fine-grained semantic differences among responses rather than binary semantic equivalence.\n    - Semantic space definition: A prompt-specific semantic space is defined as a metric space where normalized embedding vectors represent responses, with distance functions measuring contextual semantic similarity. The space satisfies constraints for semantic equivalence, irrelevance, and contradiction cases.\n\nPaper 33:\n  Title: Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation\n  Abstract: We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of \"semantic equivalence\" -- different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -- an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.\n  Structured Summary:\n    - Measuring uncertainty in natural language generation with semantic equivalence: The paper addresses the challenge that standard uncertainty measures fail in natural language generation because they treat semantically equivalent outputs (different phrasings of the same meaning) as distinct, leading to inflated uncertainty estimates when models are actually confident about the meaning.\n    - Need for reliable uncertainty in foundation models for trustworthy AI: The authors are motivated by the need to determine when language model outputs can be trusted, particularly for high-stakes applications like question answering. Without proper uncertainty measures, foundation models cannot be used as reliable information sources, which is critical for building safer AI systems.\n    - Semantic entropy using linguistic invariances: The authors propose semantic entropy, which computes uncertainty over meanings rather than token sequences. This involves clustering semantically equivalent generations and computing entropy over the resulting meaning-space distribution, thereby accounting for the fact that different phrasings can express the same semantic content.\n    - Bidirectional entailment clustering algorithm: The method uses a natural language inference model to determine semantic equivalence through bidirectional entailment: two sequences are considered semantically equivalent if each entails the other in the context of the question. This operationalizes the semantic equivalence relation needed to compute semantic entropy.\n\nPaper 34:\n  Title: Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs\n  Abstract: We propose semantic entropy probes (SEPs), a cheap and reliable method for uncertainty quantification in Large Language Models (LLMs). Hallucinations, which are plausible-sounding but factually incorrect and arbitrary model generations, present a major challenge to the practical adoption of LLMs. Recent work by Farquhar et al. (2024) proposes semantic entropy (SE), which can detect hallucinations by estimating uncertainty in the space semantic meaning for a set of model generations. However, the 5-to-10-fold increase in computation cost associated with SE computation hinders practical adoption. To address this, we propose SEPs, which directly approximate SE from the hidden states of a single generation. SEPs are simple to train and do not require sampling multiple model generations at test time, reducing the overhead of semantic uncertainty quantification to almost zero. We show that SEPs retain high performance for hallucination detection and generalize better to out-of-distribution data than previous probing methods that directly predict model accuracy. Our results across models and tasks suggest that model hidden states capture SE, and our ablation studies give further insights into the token positions and model layers for which this is the case.\n  Structured Summary:\n    - High computational cost of semantic entropy for hallucination detection: Semantic entropy is effective at detecting hallucinations in LLMs but requires sampling 5-10 model generations per query, resulting in a 5-to-10-fold computational overhead that limits practical deployment.\n    - Need for cheap and reliable hallucination detection in LLMs: LLMs are increasingly deployed in high-stakes domains like medicine and law, where hallucinations (plausible but factually incorrect outputs) can cause significant harm. Existing effective detection methods are too expensive for widespread use, motivating the need for cost-efficient alternatives.\n    - Semantic Entropy Probes (SEPs): SEPs are linear logistic regression probes trained on LLM hidden states to predict semantic entropy. They operate on a single model generation rather than requiring multiple samples, reducing computational overhead to nearly zero while retaining hallucination detection performance.\n    - Training SEPs via binarized semantic entropy supervision: SEPs are trained by collecting hidden states from single greedy generations paired with semantic entropy computed from 10 high-temperature samples. Semantic entropy scores are binarized using an optimal threshold based on within-cluster variance minimization, then used to supervise logistic regression classifiers.\n\nPaper 35:\n  Title: Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models\n  Abstract: Large language models (LLMs) specializing in natural language generation (NLG) have recently started exhibiting promising capabilities across a variety of domains. However, gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification (UQ) for NLG. Furthermore, existing literature typically assumes white-box access to language models, which is becoming unrealistic either due to the closed-source nature of the latest LLMs or computational constraints. In this work, we investigate UQ in NLG for *black-box* LLMs. We first differentiate *uncertainty* vs *confidence*: the former refers to the ``dispersion'' of the potential predictions for a fixed input, and the latter refers to the confidence on a particular prediction/generation. We then propose and compare several confidence/uncertainty measures, applying them to *selective NLG* where unreliable results could either be ignored or yielded for further assessment. Experiments were carried out with several popular LLMs on question-answering datasets (for evaluation purposes). Results reveal that a simple measure for the semantic dispersion can be a reliable predictor of the quality of LLM responses, providing valuable insights for practitioners on uncertainty management when adopting LLMs. The code to replicate our experiments is available at this https URL.\n  Structured Summary:\n    - Uncertainty quantification for black-box large language models in natural language generation: The paper addresses the challenge of quantifying uncertainty in natural language generation when only black-box access to large language models is available, without access to internal model outputs like token-level logits or probabilities.\n    - Enabling trustworthy deployment of LLMs through uncertainty management: The authors are motivated by the need to determine when LLM responses can be trusted, particularly for high-stakes applications like medical or legal question-answering, where selective generation based on uncertainty estimates could improve decision-making processes.\n    - Semantic dispersion measures using multiple generations and similarity scores: The authors propose generating multiple responses from the LLM for a given input, computing pairwise similarities using Jaccard similarity or NLI classifiers, and then deriving uncertainty and confidence measures from these similarities using graph-based methods.\n    - Graph Laplacian-based uncertainty and confidence measures: The paper introduces measures based on the graph Laplacian constructed from response similarities, including sum of eigenvalues (UEigV), degree matrix (UDeg, CDeg), and eccentricity (UEcc, CEcc), to quantify both input uncertainty and response-specific confidence.\n\nPaper 36:\n  Title: CSS: Contrastive Semantic Similarity for Uncertainty Quantification of LLMs\n  Abstract: Despite the impressive capability of large language models (LLMs), knowing when to trust their generations remains an open challenge. The recent literature on uncertainty quantification of natural language generation (NLG) utilises a conventional natural language inference (NLI) classifier to measure the semantic dispersion of LLMs responses. These studies employ logits of NLI classifier for semantic clustering to estimate uncertainty. However, logits represent the probability of the predicted class and barely contain feature information for potential clustering. Alternatively, CLIP (Contrastive Language-Image Pre-training) performs impressively in extracting image-text pair features and measuring their similarity. To extend its usability, we propose Contrastive Semantic Similarity, the CLIP-based feature extraction module to obtain similarity features for measuring uncertainty for text pairs. We apply this method to selective NLG, which detects and rejects unreliable generations for better trustworthiness of LLMs. We conduct extensive experiments with three LLMs on several benchmark question-answering datasets with comprehensive evaluation metrics. Results show that our proposed method performs better in estimating reliable responses of LLMs than comparable baselines. Results show that our proposed method performs better in estimating reliable responses of LLMs than comparable baselines. The code are available at \\url{this https URL}.\n  Structured Summary:\n    - Uncertainty quantification for large language models: The paper addresses the challenge of determining when to trust LLM generations, which is critical for deploying LLMs in high-stakes applications like medical or legal question-answering. Existing methods rely on NLI classifier logits that represent class probabilities rather than semantic features needed for accurate clustering.\n    - Improving trustworthiness of LLMs through better uncertainty estimation: The authors are motivated by the need to build safer and more trustworthy AI systems. Without reliable uncertainty measures, LLMs cannot be safely deployed in critical applications, and concerns about trustworthiness have grown alongside their enhanced capabilities.\n    - Contrastive Semantic Similarity (CSS) feature extraction: The method uses CLIP's text encoder to extract embeddings for text pairs, then applies point-wise product on corresponding embeddings to obtain contrastive semantic similarity features. These features represent semantic relations between text pairs and are used with PCA for dimension reduction before clustering.\n    - Graph Laplacian with CSS features for uncertainty estimation: The method constructs a symmetric weighted adjacency matrix from CSS features, then applies spectral clustering via Graph Laplacian. Uncertainty is measured using eigenvalues (representing semantic clusters), degree matrix, and eigenvectors (eccentricity) derived from the CSS-based Laplacian.\n\nPaper 37:\n  Title: Unconditional Truthfulness: Learning Unconditional Uncertainty of Large Language Models\n  Abstract: Uncertainty quantification (UQ) has emerged as a promising approach for detecting hallucinations and low-quality output of Large Language Models (LLMs). However, obtaining proper uncertainty scores is complicated by the conditional dependency between the generation steps of an autoregressive LLM because it is hard to model it explicitly. Here, we propose to learn this dependency from attention-based features. In particular, we train a regression model that leverages LLM attention maps, probabilities on the current generation step, and recurrently computed uncertainty scores from previously generated tokens. To incorporate the recurrent features, we also suggest a two-staged training procedure. Our experimental evaluation on ten datasets and three LLMs shows that the proposed method is highly effective for selective generation, achieving substantial improvements over rivaling unsupervised and supervised approaches.\n  Structured Summary:\n    - Conditional dependency between generation steps in LLM uncertainty quantification: The paper addresses the challenge that LLM predictions at each generation step are not conditionally independent. When an LLM generates an error early in a sequence, subsequent tokens condition on this error, leading to high confidence in incorrect outputs despite the initial uncertainty.\n    - Need for efficient and practical uncertainty quantification for LLM safety: The authors are motivated by the need to detect LLM hallucinations and low-quality outputs efficiently without relying on external knowledge sources or additional LLMs, which are either incomplete or computationally expensive. They seek practical UQ methods that leverage the model's internal knowledge.\n    - Trainable Attention-based Dependency (TAD): TAD learns conditional dependencies between generation steps using a data-driven approach. A regression model is trained to predict the gap between conditional and unconditional generation confidence, using attention-based features. During inference, this model modulates current step uncertainty based on previous step uncertainty.\n    - First-order Markov approximation for conditional probability: The method simplifies the conditional dependency by assuming a first-order Markov process where each token depends only on the immediately preceding token, supported by empirical evidence showing LLMs pay greatest attention to the previous token in 76% of cases.\n\nPaper 38:\n  Title: Uncertainty Quantification for In-Context Learning of Large Language Models\n  Abstract: In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: this https URL.\n  Structured Summary:\n    - Uncertainty quantification for in-context learning in LLMs: Existing uncertainty quantification methods for LLMs fail to account for the complex nature of in-context learning and do not decompose uncertainty into its underlying sources, making it difficult to determine whether prediction errors stem from demonstration quality or model configuration.\n    - Understanding sources of uncertainty in LLM predictions: The authors are motivated to decompose uncertainty into aleatoric (from demonstrations) and epistemic (from model configurations) components to better understand and address reliability issues like hallucination in LLM responses, enabling more informed decisions about model selection and adjustments.\n    - Bayesian formulation of in-context learning with latent variables: The authors formulate in-context learning as a Bayesian Neural Network with latent concept variables z, where the predictive distribution integrates over both latent concepts (from demonstrations) and model parameters, enabling principled uncertainty decomposition.\n    - Mutual information-based uncertainty decomposition: The method decomposes total uncertainty using entropy measures: epistemic uncertainty is quantified as the expected entropy conditioned on model parameters, while aleatoric uncertainty is computed as mutual information between predictions and latent concepts, approximated through beam search sampling.\n\nPaper 39:\n  Title: Self-Evaluation Improves Selective Generation in Large Language Models\n  Abstract: Safe deployment of large language models (LLMs) may benefit from a reliable method for assessing their generated content to determine when to abstain or to selectively generate. While likelihood-based metrics such as perplexity are widely employed, recent research has demonstrated the limitations of using sequence-level probability estimates given by LLMs as reliable indicators of generation quality. Conversely, LLMs have demonstrated strong calibration at the token level, particularly when it comes to choosing correct answers in multiple-choice questions or evaluating true/false statements. In this work, we reformulate open-ended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level. We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a ``None of the above'' option to express the model's uncertainty explicitly. We benchmark a range of scoring methods based on self-evaluation and evaluate their performance in selective generation using TruthfulQA and TL;DR. Through experiments with PaLM-2 and GPT-3, we demonstrate that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content.\n  Structured Summary:\n    - Poor calibration of sequence-level probabilities for assessing LLM generation quality: Existing sequence-level likelihood scores from large language models do not reliably rank-order their outputs by quality in open-ended generation tasks, making them unsuitable for determining when to abstain or selectively generate content.\n    - Safe deployment of LLMs through reliable quality assessment: The authors aim to enable safe deployment of large language models by developing reliable methods to assess generated content quality, allowing systems to selectively generate answers based on confidence and abstain when output quality is uncertain.\n    - Sample and Select: reformulating generation as multi-choice token-level prediction: The method samples multiple candidate answers from the LLM and reformulates the task as a multiple-choice question, using token-level probabilities (e.g., A, B, C) to select and score answers. An optional 'None of the above' choice allows the model to express uncertainty explicitly.\n    - Sample and Eval: pointwise true/false evaluation of generated answers: This approach instructs the LLM to evaluate whether a generated answer is correct using a binary yes/no format, producing a token-level probability score p(Yes|x,y) that is independent of other candidate answers.\n\nPaper 40:\n  Title: MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs\n  Abstract: Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question. We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book question-answering datasets across five popular pre-trained LLMs. Lastly, we validate the efficacy of MARS on a Medical QA dataset. Code can be found this https URL.\n  Structured Summary:\n    - Inaccurate uncertainty estimation in generative LLMs due to equal token weighting: Existing probability-based uncertainty estimation methods for generative LLMs use length-normalized scoring that assigns equal importance to all tokens in a generated sequence, ignoring the fact that different tokens contribute differently to the semantic meaning and correctness of the response in the question context.\n    - Need for reliable uncertainty quantification in high-stakes LLM applications: The authors are motivated by the critical need to quantify uncertainty in generative LLM outputs, especially in high-stakes domains like medical advice, where incorrect outputs can lead to serious consequences. Reliable uncertainty estimation enables users to assess how much trust to place in LLM responses.\n    - Meaning-Aware Response Scoring (MARS): MARS is a novel scoring function that replaces length-normalized scoring by assigning importance weights to each token based on its semantic contribution to the response meaning in the question context. It computes the product of weighted token probabilities, where weights are a convex combination of uniform length normalization and meaning-based importance coefficients.\n    - Phrase-level importance function using BERT-like model: The authors develop a 110M-parameter BERT-like neural network that simultaneously identifies phrases in generated text and assigns importance scores to them in a single forward pass. This model measures semantic impact by evaluating how masking a phrase affects answer correctness, reducing computational overhead compared to iterative masking approaches.\n\nPaper 41:\n  Title: On Hallucination and Predictive Uncertainty in Conditional Language Generation\n  Abstract: Despite improvements in performances on different natural language generation tasks, deep neural models are prone to hallucinating facts that are incorrect or nonexistent. Different hypotheses are proposed and examined separately for different tasks, but no systematic explanations are available across these tasks. In this study, we draw connections between hallucinations and predictive uncertainty in conditional language generation. We investigate their relationship in both image captioning and data-to-text generation and propose a simple extension to beam search to reduce hallucination. Our analysis shows that higher predictive uncertainty corresponds to a higher chance of hallucination. Epistemic uncertainty is more indicative of hallucination than aleatoric or total uncertainties. It helps to achieve better results of trading performance in standard metric for less hallucination with the proposed beam search variant.\n  Structured Summary:\n    - Hallucination in conditional language generation: Neural language generation models generate description tokens that are not supported by the source inputs, a phenomenon called hallucination. This seriously damages the applicability of neural models in practice where information accuracy is vital.\n    - Lack of systematic explanation for hallucination across tasks: While hallucination has been observed in various conditional NLG tasks and task-specific explanations exist, there is no unified understanding of why hallucinations occur across different tasks. The authors believe predictive uncertainty is a common theme.\n    - Uncertainty-aware beam search (UABS): A simple extension to beam search that penalizes parts or the entirety of predictive uncertainty during model decoding. It adds a weighted penalty term to balance between log probability and predictive uncertainty of selected candidates.\n    - Uncertainty decomposition into epistemic and aleatoric components: The authors decompose total predictive uncertainty into epistemic (model-related) and aleatoric (data-related) components using deep ensembles, and investigate which type is more indicative of hallucination.\n\nPaper 42:\n  Title: Language Models (Mostly) Know What They Know\n  Abstract: We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability \"P(True)\" that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict \"P(IK)\", the probability that \"I know\" the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.\n  Structured Summary:\n    - Language models' inability to evaluate their own knowledge and confidence: The paper addresses the challenge of determining whether language models can accurately assess the correctness of their own outputs and identify which questions they can answer, a prerequisite for training honest AI systems.\n    - Training honest AI systems that recognize their knowledge limits: The authors are motivated by the goal of developing AI systems capable of honesty, which necessitates that models can recognize what they do and do not know as a foundational capability.\n    - Self-evaluation via P(True) on model-generated samples: The authors propose a method where models first generate answers to questions, then evaluate the probability that each generated answer is correct by reformulating the task as a True/False question about the proposed answer.\n    - Training models to predict P(IK) using a value head: The authors train language models with an additional classification head to predict whether they can correctly answer a given question, creating a direct measure of model self-knowledge independent of any specific answer.\n\nPaper 43:\n  Title: Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification\n  Abstract: Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factually correct, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what claim to generate on the current step and what surface form to use. Our method Claim Conditioned Probability (CCP) measures only the uncertainty of a particular claim value expressed by the model. Experiments on the task of biography generation demonstrate strong improvements for CCP compared to the baselines for seven LLMs and four languages. Human evaluation reveals that the fact-checking pipeline based on uncertainty quantification is competitive with a fact-checking tool that leverages external knowledge.\n  Structured Summary:\n    - Hallucinations in LLM outputs obscure factual inaccuracies: The paper addresses the challenge that large language models produce hallucinations—factually incorrect claims—that are difficult for users to detect because they are embedded within otherwise coherent and factually correct text.\n    - Need for hallucination detection without external knowledge: The authors are motivated by the need to detect hallucinations without relying on complex external knowledge sources or fact-checking systems, instead leveraging information already present in the model's outputs through uncertainty quantification.\n    - Fact-checking pipeline based on token-level uncertainty quantification: The authors propose a pipeline that splits generated text into atomic claims, calculates token-level uncertainty scores for each claim, aggregates them into claim-level scores, and highlights unreliable claims based on a threshold.\n    - Claim Conditioned Probability (CCP) for token-level uncertainty: CCP is a novel token-level uncertainty method that isolates claim uncertainty by filtering out uncertainties related to claim type/order and surface form variations, using NLI to group semantically equivalent token alternatives.\n\n\nBefore returning, check that:\n- Every paper title appears exactly once.\n- Only leaf nodes have a \"papers\" field.\n- All intermediate nodes have a \"subtopics\" field.\n- The JSON is strictly valid and parsable.\n\n### Output:", "hierarchy_response": "{\n  \"name\": \"Uncertainty Quantification of Large Language Models\",\n  \"subtopics\": [\n    {\n      \"name\": \"Theoretical Foundations and Frameworks\",\n      \"subtopics\": [\n        {\n          \"name\": \"Epistemic vs Aleatoric Uncertainty Decomposition\",\n          \"papers\": [\n            \"To Believe or Not to Believe Your LLM\",\n            \"Distinguishing the Knowable from the Unknowable with Language Models\",\n            \"Uncertainty Quantification for In-Context Learning of Large Language Models\",\n            \"On Hallucination and Predictive Uncertainty in Conditional Language Generation\"\n          ]\n        },\n        {\n          \"name\": \"Conformal Prediction for LLMs\",\n          \"papers\": [\n            \"Language Models with Conformal Factuality Guarantees\",\n            \"Probabilistically Correct Language-based Multi-Robot Planning using Conformal Prediction\",\n            \"Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity\",\n            \"Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners\"\n          ]\n        },\n        {\n          \"name\": \"Semantic Uncertainty Quantification\",\n          \"papers\": [\n            \"Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation\",\n            \"Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs\",\n            \"Semantic Density: Uncertainty Quantification for Large Language Models through Confidence Measurement in Semantic Space\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Uncertainty Estimation Methods\",\n      \"subtopics\": [\n        {\n          \"name\": \"Black-Box Uncertainty Quantification\",\n          \"papers\": [\n            \"Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning\",\n            \"Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness\",\n            \"Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models\",\n            \"How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions\"\n          ]\n        },\n        {\n          \"name\": \"Token-Level and Response-Level Uncertainty\",\n          \"papers\": [\n            \"CLUE: Concept-Level Uncertainty Estimation for Large Language Models\",\n            \"Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification\",\n            \"MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs\",\n            \"Unconditional Truthfulness: Learning Unconditional Uncertainty of Large Language Models\"\n          ]\n        },\n        {\n          \"name\": \"Self-Evaluation and Introspection\",\n          \"papers\": [\n            \"Self-Evaluation Improves Selective Generation in Large Language Models\",\n            \"Language Models (Mostly) Know What They Know\",\n            \"SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales\"\n          ]\n        },\n        {\n          \"name\": \"Contrastive and Similarity-Based Methods\",\n          \"papers\": [\n            \"CSS: Contrastive Semantic Similarity for Uncertainty Quantification of LLMs\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Hallucination Detection and Mitigation\",\n      \"subtopics\": [\n        {\n          \"name\": \"Uncertainty-Based Hallucination Detection\",\n          \"papers\": [\n            \"Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus\",\n            \"Evaluating Uncertainty-based Failure Detection for Closed-Loop LLM Planners\"\n          ]\n        },\n        {\n          \"name\": \"Hallucination Benchmarks and Evaluation\",\n          \"papers\": [\n            \"HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models\",\n            \"TruthfulQA: Measuring How Models Mimic Human Falsehoods\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Confidence Calibration and Expression\",\n      \"subtopics\": [\n        {\n          \"name\": \"Verbalized Confidence and Linguistic Calibration\",\n          \"papers\": [\n            \"Teaching Models to Express Their Uncertainty in Words\",\n            \"LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models\",\n            \"Linguistic Calibration of Long-Form Generations\",\n            \"Reducing conversational agents' overconfidence through linguistic calibration\"\n          ]\n        },\n        {\n          \"name\": \"Confidence Alignment and Calibration Training\",\n          \"papers\": [\n            \"When to Trust LLMs: Aligning Confidence with Response Quality\",\n            \"Can We Trust LLMs? Mitigate Overconfidence Bias in LLMs through Knowledge Transfer\",\n            \"CONQORD\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Applications and Task-Specific Methods\",\n      \"subtopics\": [\n        {\n          \"name\": \"Question Answering and Fact Verification\",\n          \"papers\": [\n            \"Knowing When to Ask -- Bridging Large Language Models and Data\",\n            \"FEVER: a large-scale dataset for Fact Extraction and VERification\"\n          ]\n        },\n        {\n          \"name\": \"Multi-Hop and Complex Reasoning\",\n          \"papers\": [\n            \"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\",\n            \"Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies\",\n            \"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension\"\n          ]\n        },\n        {\n          \"name\": \"Conversational and Interactive Systems\",\n          \"papers\": [\n            \"CoQA: A Conversational Question Answering Challenge\"\n          ]\n        },\n        {\n          \"name\": \"Robot Planning and Decision Making\",\n          \"papers\": [\n            \"Leveraging Uncertainty Estimation for Efficient LLM Routing\"\n          ]\n        },\n        {\n          \"name\": \"Specialized Domain Applications\",\n          \"papers\": [\n            \"Enhancing Traffic Prediction with Textual Data Using Large Language Models\",\n            \"Neural Text Generation from Structured Data with Application to the Biography Domain\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Security and Adversarial Aspects\",\n      \"subtopics\": [\n        {\n          \"name\": \"Jailbreak Attacks and Safety\",\n          \"papers\": [\n            \"Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models\"\n          ]\n        }\n      ]\n    }\n  ]\n}\n", "hierarchy_tree": {"name": "Uncertainty Quantification of Large Language Models", "subtopics": [{"name": "Theoretical Foundations and Frameworks", "subtopics": [{"name": "Epistemic vs Aleatoric Uncertainty Decomposition", "papers": ["To Believe or Not to Believe Your LLM", "Distinguishing the Knowable from the Unknowable with Language Models", "Uncertainty Quantification for In-Context Learning of Large Language Models", "On Hallucination and Predictive Uncertainty in Conditional Language Generation"]}, {"name": "Conformal Prediction for LLMs", "papers": ["Language Models with Conformal Factuality Guarantees", "Probabilistically Correct Language-based Multi-Robot Planning using Conformal Prediction", "Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity", "Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners"]}, {"name": "Semantic Uncertainty Quantification", "papers": ["Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation", "Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs", "Semantic Density: Uncertainty Quantification for Large Language Models through Confidence Measurement in Semantic Space"]}]}, {"name": "Uncertainty Estimation Methods", "subtopics": [{"name": "Black-Box Uncertainty Quantification", "papers": ["Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning", "Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness", "Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models", "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions"]}, {"name": "Token-Level and Response-Level Uncertainty", "papers": ["CLUE: Concept-Level Uncertainty Estimation for Large Language Models", "Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification", "MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs", "Unconditional Truthfulness: Learning Unconditional Uncertainty of Large Language Models"]}, {"name": "Self-Evaluation and Introspection", "papers": ["Self-Evaluation Improves Selective Generation in Large Language Models", "Language Models (Mostly) Know What They Know", "SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales"]}, {"name": "Contrastive and Similarity-Based Methods", "papers": ["CSS: Contrastive Semantic Similarity for Uncertainty Quantification of LLMs"]}]}, {"name": "Hallucination Detection and Mitigation", "subtopics": [{"name": "Uncertainty-Based Hallucination Detection", "papers": ["Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus", "Evaluating Uncertainty-based Failure Detection for Closed-Loop LLM Planners"]}, {"name": "Hallucination Benchmarks and Evaluation", "papers": ["HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models", "TruthfulQA: Measuring How Models Mimic Human Falsehoods"]}]}, {"name": "Confidence Calibration and Expression", "subtopics": [{"name": "Verbalized Confidence and Linguistic Calibration", "papers": ["Teaching Models to Express Their Uncertainty in Words", "LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models", "Linguistic Calibration of Long-Form Generations", "Reducing conversational agents' overconfidence through linguistic calibration"]}, {"name": "Confidence Alignment and Calibration Training", "papers": ["When to Trust LLMs: Aligning Confidence with Response Quality", "Can We Trust LLMs? Mitigate Overconfidence Bias in LLMs through Knowledge Transfer", "CONQORD"]}]}, {"name": "Applications and Task-Specific Methods", "subtopics": [{"name": "Question Answering and Fact Verification", "papers": ["Knowing When to Ask -- Bridging Large Language Models and Data", "FEVER: a large-scale dataset for Fact Extraction and VERification"]}, {"name": "Multi-Hop and Complex Reasoning", "papers": ["HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering", "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies", "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"]}, {"name": "Conversational and Interactive Systems", "papers": ["CoQA: A Conversational Question Answering Challenge"]}, {"name": "Robot Planning and Decision Making", "papers": ["Leveraging Uncertainty Estimation for Efficient LLM Routing"]}, {"name": "Specialized Domain Applications", "papers": ["Enhancing Traffic Prediction with Textual Data Using Large Language Models", "Neural Text Generation from Structured Data with Application to the Biography Domain"]}]}, {"name": "Security and Adversarial Aspects", "subtopics": [{"name": "Jailbreak Attacks and Safety", "papers": ["Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models"]}]}]}}
{"id": 34, "survey": "A Survey on Detection of LLMs-Generated Content", "pdfs": [{"title": "RefGPT: Dialogue Generation of GPT, by GPT, and for GPT", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/1_Training-based Methods/Black-box/Known Source/Mixed decoding/RefGPT: Dialogue Generation of GPT, by GPT, and for GPT.pdf", "summary": {"paper_id": "survey-34_paper-0_RefGPT: Dialogue Generation of GPT, by GPT, and for GPT", "core_task": {"text": "Truthful multi-turn dialogue generation using large language models", "query_variants": ["Truthful multi-turn dialogue generation using large language models", "Factual multi-step conversation synthesis with large language models", "Accurate long-term dialogue production leveraging large language models"]}, "contributions": [{"name": "Untruthful dialogue generation due to LLM hallucination", "description": "Previous methods for automatically generating dialogues using LLMs risk producing untruthful and misleading content because models recite their own knowledge rather than relying on factual information, leading to hallucination.", "author_claim_text": "they all suffer from generating untruthful dialogues because of the model hallucination", "source_hint": "Abstract"}, {"name": "High cost of collecting human-written multi-turn dialogues", "description": "High-quality instruction datasets, particularly multi-turn dialogues in vertical domains, require extensive crowdsourcing with professional knowledge, making the cost unaffordable for most researchers and practitioners.", "author_claim_text": "collecting human-written data of high quality, especially multi-turn dialogues, is expensive and unattainable for most people", "source_hint": "Abstract and Introduction"}, {"name": "RefGPT dialogue generation method", "description": "RefGPT generates dialogues by providing LLMs with plain text or documents as references, prompting the models to leverage these references instead of reciting their own knowledge, thereby resolving hallucination issues while enabling detailed control over structure, style, and content.", "author_claim_text": "we propose a method called RefGPT to generate enormous truthful and customized dialogues without worrying about factual errors caused by the model hallucination", "source_hint": "Abstract and Introduction"}, {"name": "RefGPT-Fact dataset", "description": "A dataset containing 100,000 multi-turn dialogues generated by GPT-4 using online encyclopedia websites as references, ensuring authenticity through reliable source material.", "author_claim_text": "RefGPT-Fact is a dataset with 100k multi-turn dialogues based on factual knowledge", "source_hint": "Abstract"}]}, "abs": "Large Language Models (LLMs) have attained the impressive capability to resolve a wide range of NLP tasks by fine-tuning high-quality instruction data. However, collecting human-written data of high quality, especially multi-turn dialogues, is expensive and unattainable for most people. Though previous studies have used powerful LLMs to generate the dialogues automatically, they all suffer from generating untruthful dialogues because of the model hallucination. Therefore, we propose a method called RefGPT to generate enormous truthful and customized dialogues without worrying about factual errors caused by the model hallucination. RefGPT solves the model hallucination in dialogue generation by restricting the LLMs to leverage the given reference instead of reciting their own knowledge to generate dialogues. Additionally, RefGPT adds detailed controls on every utterance to enable high customization capability, which previous studies have ignored. On the basis of RefGPT, we also propose two high-quality dialogue datasets generated by GPT-4, namely RefGPT-Fact and RefGPT-Code. RefGPT-Fact is a dataset with 100k multi-turn dialogues based on factual knowledge and RefGPT-Code has 76k multi-turn dialogues covering a wide range of coding scenarios. Our code and datasets are released in this https URL."}, {"title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/1_Training-based Methods/Black-box/Known Source/Mixed decoding/Automatic Detection of Generated Text is Easiest when Humans are Fooled.pdf", "summary": {"paper_id": "survey-34_paper-1_Automatic Detection of Generated Text is Easiest when Humans are Fooled", "core_task": {"text": "Detecting machine-generated text from language models", "query_variants": ["Detecting machine-generated text from language models", "Identifying synthetic text produced by neural language models", "Recognition of automatically generated content from LLMs"]}, "contributions": [{"name": "Difficulty in detecting machine-generated text by both humans and automatic systems", "description": "The paper addresses the challenge that as neural language models become more capable of generating human-sounding text, both humans and automatic systems struggle to distinguish machine-generated content from human-written text, with each relying on fundamentally different detection cues.", "author_claim_text": "The capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research interest, but humans and machines rely on different cues to make their decisions.", "source_hint": "Abstract"}, {"name": "Public concern over deceptive machine-generated text and its societal impacts", "description": "The authors are motivated by the growing societal risks posed by machine-generated text, including its rapid spread, influence on political processes, and erosion of public trust, making detection increasingly imperative as generative technology matures.", "author_claim_text": "Deceptive text, whether human- or machine-generated, has entered the sphere of public concern. It propogates quickly, sets political agendas, influences elections, and undermines user trust.", "source_hint": "Introduction paragraph 2"}, {"name": "Benchmarking three sampling-based decoding strategies with BERT-based discriminators", "description": "The authors systematically evaluate three decoding strategies (top-k, nucleus sampling, and untruncated random sampling) by training BERT-based binary classifiers on balanced datasets of machine-generated and human-written text, varying excerpt length and priming conditions.", "author_claim_text": "Here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategies—top-k, nucleus sampling, and untruncated random sampling", "source_hint": "Abstract"}, {"name": "Human evaluation study with expert raters on multi-length excerpts", "description": "The authors conduct human evaluation experiments where expert raters (university students given training examples) progressively view excerpts of increasing length (from 16 to 192 tokens) and classify them as human-written or machine-generated, comparing their performance to automatic discriminators.", "author_claim_text": "We ask the raters to decide whether a passage of text was written by a human or by a computer algorithm. Raters are first shown an excerpt of length 16 WordPiece tokens. After they make a guess, the length of the excerpt is doubled", "source_hint": "Section 6"}]}, "abs": "Recent advancements in neural language modelling make it possible to rapidly generate vast amounts of human-sounding text. The capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research interest, but humans and machines rely on different cues to make their decisions. Here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategies---top-$k$, nucleus sampling, and untruncated random sampling---and show that improvements in decoding methods have primarily optimized for fooling humans. This comes at the expense of introducing statistical abnormalities that make detection easy for automatic systems. We also show that though both human and automatic detector performance improve with longer excerpt length, even multi-sentence excerpts can fool expert human raters over 30% of the time. Our findings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems."}, {"title": "Technical Report on the Pangram AI-Generated Text Classifier", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/1_Training-based Methods/Black-box/Known Source/Mixed sources/Technical Report on the Pangram AI-Generated Text Classifier.pdf", "summary": {"paper_id": "survey-34_paper-2_Technical Report on the Pangram AI-Generated Text Classifier", "core_task": {"text": "AI-generated text detection", "query_variants": ["AI-generated text detection", "identifying machine-generated text", "detection of artificially generated textual content"]}, "contributions": [{"name": "High false positive rates in existing AI text detection methods", "description": "Existing AI detection methods, including commercial tools and zero-shot approaches, suffer from unacceptably high false positive rates that prevent their adoption in production settings such as academic integrity enforcement. Additionally, these methods show bias against nonnative English speakers.", "author_claim_text": "Commercial AI detectors such as TurnItIn have demonstrated inadequate accuracy for the context of academic plagarism - primarily due to high false positive rates", "source_hint": "Introduction paragraph 2"}, {"name": "Need for robust detection with extremely low false positive rates", "description": "The authors are motivated by the practical requirement for AI detection tools that can be reliably deployed in real-world applications, particularly where false accusations of AI usage would be harmful, such as in academic contexts.", "author_claim_text": "Our technical approach is motivated by the need for robust detection methods with extremely low false positive rates that can be used in production settings", "source_hint": "Introduction paragraph 3"}, {"name": "Hard negative mining with synthetic mirrors training algorithm", "description": "The authors propose a training algorithm that alternates between training a classifier and identifying false positives (hard negatives) from a large pool of human text, then generating synthetic 'mirror' examples using LLMs that closely match the content and style of these false positives to add to the training set.", "author_claim_text": "We detail a novel algorithm for training AI-text generation models that scales to web data by efficiently selecting and generating novel AI examples based on offline inference results from a partially trained classifier", "source_hint": "Introduction paragraph 4"}, {"name": "Mirror prompting for synthetic data generation", "description": "A prompting strategy where LLMs are instructed to generate text that closely resembles human examples in topic, length, style, and tone, preventing the classifier from overfitting to superficial features like document length or topic choice.", "author_claim_text": "For each human example, we generate an AI-generated example that matches the original document on as many axes as possible, to ensure that our model learns to classify documents solely based on specific characteristics of LLM writing", "source_hint": "Section 4.2"}]}, "abs": "We present Pangram Text, a transformer-based neural network trained to distinguish text written by large language models from text written by humans. Pangram Text outperforms zero-shot methods such as DetectGPT as well as leading commercial AI detection tools with over 38 times lower error rates on a comprehensive benchmark comprised of 10 text domains (student writing, creative writing, scientific writing, books, encyclopedias, news, email, scientific papers, short-form Q&A) and 8 open- and closed-source large language models. We propose a training algorithm, hard negative mining with synthetic mirrors, that enables our classifier to achieve orders of magnitude lower false positive rates on high-data domains such as reviews. Finally, we show that Pangram Text is not biased against nonnative English speakers and generalizes to domains and models unseen during training."}, {"title": "GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/1_Training-based Methods/Black-box/Known Source/Mixed sources/GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content.pdf", "summary": {"paper_id": "survey-34_paper-3_GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content", "core_task": {"text": "Distinguishing human-written and ChatGPT-generated text", "query_variants": ["Distinguishing human-written and ChatGPT-generated text", "Differentiating between human-authored and ChatGPT-produced content", "Classification of human versus ChatGPT text generation"]}, "contributions": [{"name": "Distinguishing ChatGPT-generated text from human-written text", "description": "The paper addresses the challenge of detecting text generated by ChatGPT versus human-written content, which has become critical for verifying information authenticity in domains like legal proceedings and journalism. Existing detection methods face significant challenges with newer large language models like ChatGPT.", "author_claim_text": "The development of an algorithm that can accurately distinguish between machine-generated text and human-generated text has become crucial in contexts where verifying the authenticity of information is essential, such as in legal proceedings and news reporting.", "source_hint": "Introduction paragraph 1"}, {"name": "Need for novel detection algorithms due to ChatGPT emergence", "description": "The authors are motivated by the fact that ChatGPT and similar large language models have rendered traditional detection methods inadequate. The widespread availability of ChatGPT makes developing effective detection algorithms an urgent necessity for maintaining information authenticity.", "author_claim_text": "the emergence of Large Language Models (LLMs) like InstructGPT and the availability of its free deployment, ChatGPT, has presented significant challenges to existing detection methods. As a result, the need to develop novel algorithms that can accurately distinguish between machine and human-generated text has become more pressing than ever before.", "source_hint": "Introduction paragraph 1"}, {"name": "OpenGPTText dataset collection and cleaning", "description": "The authors created OpenGPTText, a dataset of 29,395 text samples rephrased by ChatGPT from the OpenWebText corpus. They implemented a cleaning procedure to remove stylistic artifacts like Unicode quotation marks and excessive newlines, producing OpenGPTText-Final to enhance classifier robustness.", "author_claim_text": "We first collected and released a pre-processed dataset named OpenGPTText, which consists of rephrased content generated using ChatGPT.", "source_hint": "Abstract; Section 3"}, {"name": "RoBERTa-Sentinel model with frozen encoder and MLP classifier", "description": "The authors developed RoBERTa-Sentinel by freezing a pretrained RoBERTa model to preserve linguistic knowledge and adding an MLP classifier with GELU activation. The model extracts features from the [CLS] token's hidden state and classifies text through the MLP module.", "author_claim_text": "We then designed, implemented, and trained two different models for text classification, using Robustly Optimized BERT Pretraining Approach (RoBERTa)", "source_hint": "Abstract; Section 4.1"}]}, "abs": "This paper presents a novel approach for detecting ChatGPT-generated vs. human-written text using language models. To this end, we first collected and released a pre-processed dataset named OpenGPTText, which consists of rephrased content generated using ChatGPT. We then designed, implemented, and trained two different models for text classification, using Robustly Optimized BERT Pretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5), respectively. Our models achieved remarkable results, with an accuracy of over 97% on the test dataset, as evaluated through various metrics. Furthermore, we conducted an interpretability study to showcase our model's ability to extract and differentiate key features between human-written and ChatGPT-generated text. Our findings provide important insights into the effective use of language models to detect generated text."}, {"title": "Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/1_Training-based Methods/Black-box/Known Source/Mixed sources/Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays.pdf", "summary": {"paper_id": "survey-34_paper-4_Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays", "core_task": {"text": "Evaluating AI detection tool accuracy in identifying AI-generated versus human-written text", "query_variants": ["Evaluating AI detection tool accuracy in identifying AI-generated versus human-written text", "Assessing performance of AI detectors distinguishing machine-generated from human-authored content", "Measuring detection system reliability for separating synthetic text from human-produced writing"]}, "contributions": [{"name": "Uncertainty in reliability of AI detection tools for student essays", "description": "The paper addresses the challenge that while AI detection tools are increasingly used by educators to identify AI-generated student work, their reliability and accuracy in distinguishing between human-written and AI-generated texts remains unclear, particularly regarding false positives.", "author_claim_text": "As the use of AI tools by students has become more prevalent, instructors have started using AI detection tools like GPTZero and QuillBot to detect AI written text. However, the reliability of these detectors remains uncertain.", "source_hint": "Abstract"}, {"name": "Ensuring fair assessment of students and proper identification of AI use", "description": "The authors are motivated by the need to fairly evaluate students, ensuring that those who use AI are correctly identified while preventing false accusations against students who write their own work, which requires understanding the accuracy of AI detection tools.", "author_claim_text": "The results from this study could be used to assess the accuracy of AI detectors. Therefore, we can ensure that students who are using AI in their work can be properly identified, and the students who are not, are not wrongly punished for using it.", "source_hint": "Introduction"}, {"name": "Comparative testing of GPTZero across essay length categories", "description": "The authors tested GPTZero by categorizing essays into three length groups (short, medium, long) and submitting both human-written and AI-generated essays to measure detection accuracy and false positive rates across different text lengths.", "author_claim_text": "We focused mostly on the success rate of GPTZero, the most-used AI detector, in identifying AI-generated texts based on different lengths of randomly submitted essays: short (40-100 word count), medium (100-350 word count), and long (350-800 word count).", "source_hint": "Abstract"}, {"name": "Dataset construction with human and AI-generated essays", "description": "The methodology involved collecting fifty human-written student essays and twenty-eight AI-generated essays from ChatGPT, then systematically inputting each into GPTZero to record the tool's assessment of AI generation likelihood.", "author_claim_text": "We gathered a data set consisting of twenty-eight AI-generated papers and fifty human-written papers. With this randomized essay data, papers were individually plugged into GPTZero and measured for percentage of AI generation and confidence.", "source_hint": "Abstract and Methodology"}]}, "abs": "As the use of AI tools by students has become more prevalent, instructors have started using AI detection tools like GPTZero and QuillBot to detect AI written text. However, the reliability of these detectors remains uncertain. In our study, we focused mostly on the success rate of GPTZero, the most-used AI detector, in identifying AI-generated texts based on different lengths of randomly submitted essays: short (40-100 word count), medium (100-350 word count), and long (350-800 word count). We gathered a data set consisting of twenty-eight AI-generated papers and fifty human-written papers. With this randomized essay data, papers were individually plugged into GPTZero and measured for percentage of AI generation and confidence. A vast majority of the AI-generated papers were detected accurately (ranging from 91-100% AI believed generation), while the human generated essays fluctuated; there were a handful of false positives. These findings suggest that although GPTZero is effective at detecting purely AI-generated content, its reliability in distinguishing human-authored texts is limited. Educators should therefore exercise caution when relying solely on AI detection tools."}, {"title": "RADAR: Robust AI-Text Detection via Adversarial Learning", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/1_Training-based Methods/Black-box/Known Source/Mixed strategies/Adversarial training/RADAR: Robust AI-Text Detection via Adversarial Learning.pdf", "summary": {"paper_id": "survey-34_paper-5_RADAR: Robust AI-Text Detection via Adversarial Learning", "core_task": {"text": "robust AI-text detection via adversarial learning", "query_variants": ["robust AI-text detection via adversarial learning", "adversarial training for reliable machine-generated text identification", "detecting AI-generated text using adversarial robustness methods"]}, "contributions": [{"name": "Vulnerability of AI-text detectors to LLM-based paraphrasing", "description": "Current AI-text detection methods fail to maintain reliable performance when AI-generated texts are paraphrased using large language models, creating challenges for distinguishing machine-generated content from human-written text.", "author_claim_text": "existing works show that current AI-text detectors are not robust to LLM-based paraphrasing", "source_hint": "Abstract"}, {"name": "Addressing misuse risks and fairness concerns in AI-text detection", "description": "The inability to reliably detect AI-generated text creates serious socio-technological risks including fake content generation, academic plagiarism using LLMs, and potential false accusations against innocent human writers, motivating the need for robust detection methods.", "author_claim_text": "the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusations of innocent writers", "source_hint": "Abstract"}, {"name": "RADAR adversarial training framework with paraphraser and detector", "description": "RADAR introduces an adversarial learning framework where a paraphraser (tunable LLM) attempts to rewrite AI-text to evade detection, while a detector (separate tunable LLM) learns to identify both original and paraphrased AI-text. The two models update iteratively using feedback from each other.", "author_claim_text": "this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a robust AI-text detector via adversarial learning. RADAR is based on adversarial training of a paraphraser and a detector", "source_hint": "Abstract and Introduction"}, {"name": "Clipped PPO with entropy penalty for paraphraser training", "description": "The paraphraser is optimized using Proximal Policy Optimization with clipping and an entropy penalty term to balance advantage maximization with generation diversity, treating text generation as a decision-making process with detector feedback as rewards.", "author_claim_text": "We propose Clipped PPO with Entropy Penalty (cppo-ep) in RADAR to optimize Gσ", "source_hint": "Section 3.1"}]}, "abs": "Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusations of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a robust AI-text detector via adversarial learning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic content to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5-Turbo."}, {"title": "CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Data Limitation With Contrastive Learning", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/1_Training-based Methods/Black-box/Known Source/Mixed strategies/Graph structure and contrastive learning/CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Data Limitation With Contrastive Learning.pdf", "summary": {"paper_id": "survey-34_paper-6_CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Data Limitation With Contrastive Learning", "core_task": {"text": "machine-generated text detection under low resource", "query_variants": ["machine-generated text detection under low resource", "identifying synthetic text with limited data", "detecting automatically generated content in resource-constrained settings"]}, "contributions": [{"name": "MGT detection under low-resource scenarios with linguistic structure ignored", "description": "Existing MGT detectors treat texts as flat token sequences and rely on token-level distributions, ignoring high-level linguistic structure. They also suffer from limited annotated data, leading to poor accuracy in low-resource settings where training examples are scarce.", "author_claim_text": "these methods fail to consider the linguistic structure of texts. Moreover, they lack the ability to handle the low-resource problem, which could often happen in practice considering the enormous amount of textual data online.", "source_hint": "Abstract"}, {"name": "Preventing misuse of text generative models", "description": "The authors are motivated by the need to prevent malicious uses of advanced text generation models (e.g., fake news, review forging, spamming), which produce human-like texts that are difficult for humans to distinguish at scale.", "author_claim_text": "Machine-Generated Text (MGT) detection, a task that discriminates MGT from Human-Written Text (HWT), plays a crucial role in preventing misuse of text generative models", "source_hint": "Abstract"}, {"name": "Coherence graph construction with entity consistency", "description": "The authors construct an entity coherence graph to model sentence-level text structure based on Centering Theory. Entities are nodes, with edges representing entity co-occurrence within sentences (inner relation) and entity repetition across sentences (inter relation), capturing coherence differences between MGT and HWT.", "author_claim_text": "we propose to construct an undirected graph with entities as nodes. Specifically, we first implement the ELMo-based NER model TagLM to extract the entities from document. A relation <inter> is constructed between the same entities in different sentences and nodes within the same sentences are connected by relation <inner>", "source_hint": "Section 3.1"}, {"name": "Coherence encoder module with relation-aware GCN and attention LSTM", "description": "The authors design a coherence encoder module that uses relation-aware GCN to process the coherence graph separately for each relation type, then aggregates node representations into sentence-level features, and applies attention LSTM to capture sentence-level coherence and generate a coherence-enhanced document representation.", "author_claim_text": "we introduce the structure of graph neural network structure, an innovative coherence encoder module(CEM), which is utilized to integrate coherence information into a semantic representation of text by propagating and aggregating information from different granularity.", "source_hint": "Section 3.2.3"}]}, "abs": "Machine-Generated Text (MGT) detection, a task that discriminates MGT from Human-Written Text (HWT), plays a crucial role in preventing misuse of text generative models, which excel in mimicking human writing style recently. Latest proposed detectors usually take coarse text sequences as input and fine-tune pretrained models with standard cross-entropy loss. However, these methods fail to consider the linguistic structure of texts. Moreover, they lack the ability to handle the low-resource problem which could often happen in practice considering the enormous amount of textual data online. In this paper, we present a coherence-based contrastive learning model named CoCo to detect the possible MGT under low-resource scenario. To exploit the linguistic feature, we encode coherence information in form of graph into text representation. To tackle the challenges of low data resource, we employ a contrastive learning framework and propose an improved contrastive loss for preventing performance degradation brought by simple samples. The experiment results on two public datasets and two self-constructed datasets prove our approach outperforms the state-of-art methods significantly. Also, we surprisingly find that MGTs originated from up-to-date language models could be easier to detect than these from previous models, in our experiments. And we propose some preliminary explanations for this counter-intuitive phenomena. All the codes and datasets are open-sourced."}, {"title": "Multiscale Positive-Unlabeled Detection of AI-Generated Texts", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/1_Training-based Methods/Black-box/Known Source/Mixed strategies/Positive unlabeled training/Multiscale Positive-Unlabeled Detection of AI-Generated Texts.pdf", "summary": {"paper_id": "survey-34_paper-7_Multiscale Positive-Unlabeled Detection of AI-Generated Texts", "core_task": {"text": "detecting AI-generated texts across multiple length scales", "query_variants": ["detecting AI-generated texts across multiple length scales", "identification of machine-generated text at various document lengths", "recognizing synthetic text across different text size ranges"]}, "contributions": [{"name": "Difficulty of detecting short AI-generated texts", "description": "Existing AI-generated text detection methods perform poorly on short texts such as SMS messages, tweets, and reviews, despite their effectiveness on longer texts. This creates a significant gap in detection capabilities for fragmented mobile media content.", "author_claim_text": "mainstream detectors always fail on short texts, like SMSes, Tweets, and reviews", "source_hint": "Introduction paragraph 2"}, {"name": "Threat of fake short texts in mobile media era", "description": "The authors are motivated by the vulnerability of smartphone users to LLM-generated fake short texts in daily life contexts. Current detectors lack comprehensive capability to handle both short and long texts, leaving users defenseless against such threats.", "author_claim_text": "In the era of smartphones where people rely heavily on fragmented mobile media, fake short articles like SMSes, Tweets, and reviews generated by LLMs could pose huge threats to one's daily life", "source_hint": "Introduction paragraph 2"}, {"name": "Multiscale Positive-Unlabeled (MPU) training framework", "description": "The authors propose an MPU framework that treats short machine-generated texts as partially 'unlabeled' rather than clearly labeled as AI. This reframes AI text detection as a partial Positive-Unlabeled problem, enabling better handling of texts across different lengths.", "author_claim_text": "a Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the difficulty of short-text detection without sacrificing long-texts", "source_hint": "Abstract"}, {"name": "Length-sensitive Multiscale PU Loss", "description": "A novel loss function that adjusts the prior probability of a text being positive based on text length. An abstract recurrent model estimates length-variant priors, recognizing that distribution shifts gradually from long to short texts.", "author_claim_text": "we propose the length-sensitive Multiscale PU Loss, where a recurrent model in abstraction is used to estimate positive priors of scale-variant corpora", "source_hint": "Abstract and Section 3.3"}]}, "abs": "Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may impact the authenticity of texts. Previous works proposed methods to detect these AI-generated texts, including simple ML classifiers, pretrained-model-based zero-shot methods, and finetuned language classification models. However, mainstream detectors always fail on short texts, like SMSes, Tweets, and reviews. In this paper, a Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the difficulty of short-text detection without sacrificing long-texts. Firstly, we acknowledge the human-resemblance property of short machine texts, and rephrase AI text detection as a partial Positive-Unlabeled (PU) problem by regarding these short machine texts as partially ``unlabeled\". Then in this PU context, we propose the length-sensitive Multiscale PU Loss, where a recurrent model in abstraction is used to estimate positive priors of scale-variant corpora. Additionally, we introduce a Text Multiscaling module to enrich training corpora. Experiments show that our MPU method augments detection performance on long AI-generated texts, and significantly improves short-text detection of language model detectors. Language Models trained with MPU could outcompete existing detectors on various short-text and long-text detection benchmarks. The codes are available at this https URL and this https URL."}, {"title": "LLMDet: A Third Party Large Language Models Generated Text Detection Tool", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/1_Training-based Methods/Black-box/Known Source/Mixed strategies/Proxy perplexity/LLMDet: A Third Party Large Language Models Generated Text Detection Tool.pdf", "summary": {"paper_id": "survey-34_paper-8_LLMDet: A Third Party Large Language Models Generated Text Detection Tool", "core_task": {"text": "detecting and attributing text generated by specific large language models", "query_variants": ["detecting and attributing text generated by specific large language models", "identifying and tracing text produced by particular LLMs", "attribution and detection of content from specific generative language models"]}, "contributions": [{"name": "Inability to trace text to specific LLMs and lack of secure, efficient detection", "description": "Current detection methods cannot identify which specific large language model generated a given text, and they require model access which compromises security and efficiency. They only perform binary classification between human and machine text.", "author_claim_text": "existing detection tools typically rely on access to LLMs and can only differentiate between machine-generated and human-authored text, failing to meet the requirements of fine-grained tracing, intermediary judgment, and rapid detection", "source_hint": "Abstract"}, {"name": "Need for practical detection with specificity, safety, efficiency, and extendibility", "description": "The authors are motivated by the need for a detection tool that can trace text to specific models for copyright protection, avoid accessing model parameters for security, operate with low resource requirements for widespread use, and adapt to new emerging models.", "author_claim_text": "a practical LLM detection tool should possess the following capabilities: Specificity, Safety, Efficiency, Extendibility", "source_hint": "Introduction §3"}, {"name": "Dictionary construction using n-gram next-token probabilities", "description": "The method constructs dictionaries offline by sampling generated text from each LLM, extracting frequent n-grams, and recording their next-token probability distributions. These dictionaries serve as prior information for computing proxy perplexity without accessing the models during detection.", "author_claim_text": "we record the next-token probabilities of salient n-gram as features to calculate proxy perplexity for each LLM", "source_hint": "Abstract and §4.1"}, {"name": "Proxy perplexity estimation for text detection", "description": "During detection, the tool computes proxy perplexity for input text across multiple LLMs using the pre-built dictionaries, then feeds these perplexity features into a trained classifier to identify the source model. This approach eliminates the need for model access at detection time.", "author_claim_text": "By jointly analyzing the proxy perplexities of LLMs, we can determine the source of the generated text", "source_hint": "Abstract and §4.2"}]}, "abs": "Generated texts from large language models (LLMs) are remarkably close to high-quality human-authored text, raising concerns about their potential misuse in spreading false information and academic misconduct. Consequently, there is an urgent need for a highly practical detection tool capable of accurately identifying the source of a given text. However, existing detection tools typically rely on access to LLMs and can only differentiate between machine-generated and human-authored text, failing to meet the requirements of fine-grained tracing, intermediary judgment, and rapid detection. Therefore, we propose LLMDet, a model-specific, secure, efficient, and extendable detection tool, that can source text from specific LLMs, such as GPT-2, OPT, LLaMA, and others. In LLMDet, we record the next-token probabilities of salient n-grams as features to calculate proxy perplexity for each LLM. By jointly analyzing the proxy perplexities of LLMs, we can determine the source of the generated text. Experimental results show that LLMDet yields impressive detection performance while ensuring speed and security, achieving 98.54% precision and x5.0 faster for recognizing human-authored text. Additionally, LLMDet can effortlessly extend its detection capabilities to a new open-source model. We will provide an open-source tool at this https URL."}, {"title": "On the Zero-Shot Generalization of Machine-Generated Text Detectors", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/1_Training-based Methods/Black-box/Unknown Source/Cross-domain transfer/On the Zero-Shot Generalization of Machine-Generated Text Detectors.pdf", "summary": {"paper_id": "survey-34_paper-9_On the Zero-Shot Generalization of Machine-Generated Text Detectors", "core_task": {"text": "zero-shot generalization of machine-generated text detectors", "query_variants": ["zero-shot generalization of machine-generated text detectors", "generalization of synthetic text detection models without training examples", "cross-domain transfer of automated generated content classifiers"]}, "contributions": [{"name": "Zero-shot generalization of machine-generated text detectors to unseen generators", "description": "The paper addresses the challenge of detecting machine-generated text from new, unseen language models that detectors were not trained on. This is critical because collecting training data from large or proprietary models is difficult, and detectors must cover a rapidly growing range of LLMs in real applications.", "author_claim_text": "How will the detector perform on a different generator that it is not trained on? This question is important due to multiple reasons: (1) LLMs are becoming increasingly large and expensive... (2) The number of released LLMs is growing rapidly.", "source_hint": "Introduction §1"}, {"name": "Need for detectors that generalize across multiple LLM generators", "description": "The authors are motivated by the practical necessity for detectors to work across diverse and evolving LLMs, especially when training data from large or proprietary models is unavailable or expensive to obtain, making zero-shot generalization essential for real-world deployment.", "author_claim_text": "In a real application scenario, the detector needs to cover a wide range of LLMs (including the ones the detector is not trained on), instead of only one generator.", "source_hint": "Introduction §1"}, {"name": "Systematic evaluation of detector generalization across LLM families", "description": "The authors collect generation data from 13 popular language models across three domains (news, reviews, knowledge) and train ELECTRA-large binary classifiers on each generator's data. They systematically evaluate cross-generator performance using an Acc-Gap metric to measure generalization ability.", "author_claim_text": "We collect generation data from a wide range of LLMs. We then train neural detectors on data from each generator and test its performance on other generators.", "source_hint": "Introduction §1, Methodology §2, Experiment Setup §3"}, {"name": "Ensemble training with pruned data from medium-sized models", "description": "The authors propose training detectors on mixed data from multiple generators while strategically pruning out large-version models. They demonstrate that detectors trained on medium-sized model data can effectively generalize to larger versions with minimal performance loss.", "author_claim_text": "As a concrete application, we demonstrate that robust detectors can be built on an ensemble of training data from medium-sized models.", "source_hint": "Abstract, §4.2"}]}, "abs": "The rampant proliferation of large language models, fluent enough to generate text indistinguishable from human-written language, gives unprecedented importance to the detection of machine-generated text. This work is motivated by an important research question: How will the detectors of machine-generated text perform on outputs of a new generator, that the detectors were not trained on? We begin by collecting generation data from a wide range of LLMs, and train neural detectors on data from each generator and test its performance on held-out generators. While none of the detectors can generalize to all generators, we observe a consistent and interesting pattern that the detectors trained on data from a medium-size LLM can zero-shot generalize to the larger version. As a concrete application, we demonstrate that robust detectors can be built on an ensemble of training data from medium-sized models."}, {"title": "From Text to Source: Results in Detecting Large Language Model-Generated Content", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/1_Training-based Methods/Black-box/Unknown Source/Cross-domain transfer/From Text to Source: Results in Detecting Large Language Model-Generated Content.pdf", "summary": {"paper_id": "survey-34_paper-10_From Text to Source: Results in Detecting Large Language Model-Generated Content", "core_task": {"text": "detecting large language model-generated content", "query_variants": ["detecting large language model-generated content", "identification of text produced by large language models", "recognizing content generated by LLMs"]}, "contributions": [{"name": "Cross-Model Detection of LLM-Generated Text", "description": "The paper addresses the challenge of detecting text generated by Large Language Models (LLMs) when the specific source model is unknown. This involves determining whether a classifier trained on one LLM's output can generalize to detect text from different LLMs without additional training.", "author_claim_text": "concerns about misinformation and ethical implications. Addressing these concerns necessitates the development of robust methods to detect and attribute text generated by LLMs.", "source_hint": "Abstract"}, {"name": "Real-World Scenarios with Unknown Source Models", "description": "The authors are motivated by practical situations where the identity of the text-generating model is unknown, necessitating detection methods that can generalize across different LLMs. This is crucial for maintaining trust in online platforms, content moderation, and responsible AI deployment.", "author_claim_text": "Our motivation for this research stems from real-life scenarios where we often lack knowledge of the specific model used to generate a piece of text.", "source_hint": "Introduction §1"}, {"name": "Comprehensive Cross-Model Detection Evaluation", "description": "The authors conduct extensive experiments evaluating classifier transferability across 55 models from 10 families, varying in size from 124M to 70B parameters. They assess the impact of model size, family, conversational fine-tuning, quantization, and watermarking on detection performance using a DeBERTaV3-base encoder classifier.", "author_claim_text": "We systematically examine a wide range of LLM sizes, spanning from base models to exceptionally large ones, and encompassing diverse model families such as GPT-2, LLaMA, Pythia, OPT and others", "source_hint": "Introduction §1"}, {"name": "Model Attribution Framework", "description": "The authors develop a multi-faceted attribution framework consisting of five subtasks: identifying the exact source model (51 classes), classifying model family (12 classes), determining model size (6 bins), detecting quantization, and detecting watermarking. Each task uses appropriate metrics (AUC for detection, F1 for attribution).", "author_claim_text": "The research also explores Model Attribution, encompassing source model identification, model family, and model size classification, in addition to quantization and watermarking detection.", "source_hint": "Abstract"}]}, "abs": "The widespread use of Large Language Models (LLMs), celebrated for their ability to generate human-like text, has raised concerns about misinformation and ethical implications. Addressing these concerns necessitates the development of robust methods to detect and attribute text generated by LLMs. This paper investigates \"Cross-Model Detection,\" by evaluating whether a classifier trained to distinguish between source LLM-generated and human-written text can also detect text from a target LLM without further training. The study comprehensively explores various LLM sizes and families, and assesses the impact of conversational fine-tuning techniques, quantization, and watermarking on classifier generalization. The research also explores Model Attribution, encompassing source model identification, model family, and model size classification, in addition to quantization and watermarking detection. Our results reveal several key findings: a clear inverse relationship between classifier effectiveness and model size, with larger LLMs being more challenging to detect, especially when the classifier is trained on data from smaller models. Training on data from similarly sized LLMs can improve detection performance from larger models but may lead to decreased performance when dealing with smaller models. Additionally, model attribution experiments show promising results in identifying source models and model families, highlighting detectable signatures in LLM-generated text, with particularly remarkable outcomes in watermarking detection, while no detectable signatures of quantization were observed. Overall, our study contributes valuable insights into the interplay of model size, family, and training data in LLM detection and attribution."}, {"title": "ConDA: Contrastive Domain Adaptation for AI-generated Text Detection", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/1_Training-based Methods/Black-box/Unknown Source/Cross-domain transfer/ConDA: Contrastive Domain Adaptation for AI-generated Text Detection.pdf", "summary": {"paper_id": "survey-34_paper-11_ConDA: Contrastive Domain Adaptation for AI-generated Text Detection", "core_task": {"text": "AI-generated text detection in news articles", "query_variants": ["AI-generated text detection in news articles", "identifying machine-generated content in journalistic texts", "detecting synthetic text within news media articles"]}, "contributions": [{"name": "Label scarcity for AI-generated text detection across multiple LLMs", "description": "The paper addresses the challenge of detecting AI-generated news text when labeled training data is unavailable for newer or target language models, while unlabeled text from these generators exists. This data bottleneck prevents building effective supervised detectors for emerging LLMs.", "author_claim_text": "Given the surge in development of new LLMs, acquiring labeled training data for supervised detectors is a bottleneck. However, there might be plenty of unlabeled text data available, without information on which generator it came from.", "source_hint": "Abstract"}, {"name": "Preventing malicious use of LLMs for disinformation at scale", "description": "The authors are motivated by the risk that increasingly convincing language models can be exploited by malicious actors to propagate misinformation, disinformation, propaganda, or spam at scale, especially with the accessibility of public-facing APIs and recent concerns about AI-generated news websites.", "author_claim_text": "Given the potential malicious nature in which these LLMs can be used to generate disinformation at scale, it is important to build effective detectors for such AI-generated text.", "source_hint": "Abstract and Introduction"}, {"name": "ConDA: Contrastive Domain Adaptation framework", "description": "ConDA frames AI-generated text detection as an unsupervised domain adaptation problem where different LLMs are treated as different domains. The framework combines contrastive learning (using synonym replacement transformations) with Maximum Mean Discrepancy (MMD) to learn domain-invariant representations from labeled source data and unlabeled target data.", "author_claim_text": "We develop a Contrastive Domain Adaptation framework, called ConDA, that blends standard domain adaptation techniques with the representation power of contrastive learning to learn domain invariant representations that are effective for the final unsupervised detection task.", "source_hint": "Abstract and Section 3"}, {"name": "Training objective combining classification, contrastive, and domain discrepancy losses", "description": "The training objective integrates source classification losses (cross-entropy on original and perturbed text), contrastive losses for both source and target domains (bringing positive pairs closer while separating negatives), and MMD loss to minimize distribution discrepancy between source and target generators.", "author_claim_text": "The final training objective for our main framework is: L = (1-λ1)/2[LS_CE + LS'_CE] + λ1/2[LS_ctr + LT_ctr] + λ2MMD(S,T)", "source_hint": "Section 3.2, Equation 4"}]}, "abs": "Large language models (LLMs) are increasingly being used for generating text in a variety of use cases, including journalistic news articles. Given the potential malicious nature in which these LLMs can be used to generate disinformation at scale, it is important to build effective detectors for such AI-generated text. Given the surge in development of new LLMs, acquiring labeled training data for supervised detectors is a bottleneck. However, there might be plenty of unlabeled text data available, without information on which generator it came from. In this work we tackle this data problem, in detecting AI-generated news text, and frame the problem as an unsupervised domain adaptation task. Here the domains are the different text generators, i.e. LLMs, and we assume we have access to only the labeled source data and unlabeled target data. We develop a Contrastive Domain Adaptation framework, called ConDA, that blends standard domain adaptation techniques with the representation power of contrastive learning to learn domain invariant representations that are effective for the final unsupervised detection task. Our experiments demonstrate the effectiveness of our framework, resulting in average performance gains of 31.7% from the best performing baselines, and within 0.8% margin of a fully supervised detector. All our code and data is available at this https URL."}, {"title": "Deepfake Videos in the Wild: Analysis and Detection", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/1_Training-based Methods/Black-box/Unknown Source/Detection in the wild/Deepfake Videos in the Wild: Analysis and Detection.pdf", "summary": {"paper_id": "survey-34_paper-12_Deepfake Videos in the Wild: Analysis and Detection", "core_task": {"text": "deepfake video detection in real-world settings", "query_variants": ["deepfake video detection in real-world settings", "detecting manipulated videos in practical environments", "identifying synthetic video content under real-world conditions"]}, "contributions": [{"name": "Limited understanding of real-world deepfake videos and detection generalization", "description": "Existing deepfake detection research has been conducted with limited knowledge of actual deepfake videos appearing in the wild, creating uncertainty about whether detection methods trained on research datasets can generalize to real-world deepfakes produced by the Internet community.", "author_claim_text": "little effort has gone towards understanding deepfake videos in the wild, leading to a limited understanding of the real-world applicability of research contributions in this space", "source_hint": "Abstract"}, {"name": "Need to bridge knowledge gap between research and real-world deepfakes", "description": "The authors are motivated to address the disconnect between academic deepfake research and real-world deepfake content, enabling better understanding of whether existing detection schemes are ready for practical deployment against deepfakes found in the wild.", "author_claim_text": "To bridge this gap in knowledge, we make the following contributions", "source_hint": "Abstract"}, {"name": "DF-W dataset collection methodology", "description": "The authors develop a systematic data collection methodology that searches YouTube, Bilibili, and Reddit using keywords, identifies potential deepfake channels and videos, filters content through manual and automated verification, and downloads videos at highest available resolution to build the DF-W dataset.", "author_claim_text": "We introduce a new deepfake dataset called DF-W, comprised of deepfake videos created and shared by the Internet community", "source_hint": "Introduction §1, Section 3.1"}, {"name": "Modified video-level detection decision metric", "description": "The authors adapt an existing video-level decision metric to handle multiple faces per frame (not all fake) and videos with mixed real and fake frames, computing frame probability as the maximum across all faces and video probability by averaging top percentile frame scores.", "author_claim_text": "we propose a modified version of a video-level decision heuristic, originally introduced by Li et al", "source_hint": "Section 5.1"}]}, "abs": "AI-manipulated videos, commonly known as deepfakes, are an emerging problem. Recently, researchers in academia and industry have contributed several (self-created) benchmark deepfake datasets, and deepfake detection algorithms. However, little effort has gone towards understanding deepfake videos in the wild, leading to a limited understanding of the real-world applicability of research contributions in this space. Even if detection schemes are shown to perform well on existing datasets, it is unclear how well the methods generalize to real-world deepfakes. To bridge this gap in knowledge, we make the following contributions: First, we collect and present the largest dataset of deepfake videos in the wild, containing 1,869 videos from YouTube and Bilibili, and extract over 4.8M frames of content. Second, we present a comprehensive analysis of the growth patterns, popularity, creators, manipulation strategies, and production methods of deepfake content in the real-world. Third, we systematically evaluate existing defenses using our new dataset, and observe that they are not ready for deployment in the real-world. Fourth, we explore the potential for transfer learning schemes and competition-winning techniques to improve defenses."}, {"title": "Ghostbuster: Detecting Text Ghostwritten by Large Language Models", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/1_Training-based Methods/Black-box/Unknown Source/Surrogate model/Ghostbuster: Detecting Text Ghostwritten by Large Language Models.pdf", "summary": {"paper_id": "survey-34_paper-13_Ghostbuster: Detecting Text Ghostwritten by Large Language Models", "core_task": {"text": "detecting AI-generated text", "query_variants": ["detecting AI-generated text", "identification of machine-generated text", "distinguishing synthetic text from human-written content"]}, "contributions": [{"name": "Detecting AI-generated text with strong generalization", "description": "The paper addresses the challenge of reliably detecting whether text was generated by large language models like ChatGPT, particularly when existing detectors exhibit poor generalization across domains, prompts, and models, and when high false positive rates raise ethical concerns about misclassifying genuine human work.", "author_claim_text": "concerns that students are submitting assignments ghostwritten by language models have led many schools to adapt by restricting the use of ChatGPT in classrooms and homework assignments", "source_hint": "Introduction paragraph 1"}, {"name": "Need for trustworthy and generalizable AI text detection", "description": "The authors are motivated by concerns about authenticity and trustworthiness of text across domains (education, news), the brittleness of existing detectors, and ethical issues arising from false accusations, especially given that non-native English speakers are disproportionately flagged as using AI.", "author_claim_text": "the high false positive rates of these models raise potential ethical concerns because they jeopardize students whose genuine work is misclassified as AI-generated", "source_hint": "Introduction paragraph 2"}, {"name": "Structured search over probability-based features", "description": "Ghostbuster computes token probabilities from multiple weaker language models (unigram, trigram, GPT-3 ada and davinci), then performs a structured search over vector and scalar functions that combine these probabilities into features, and finally trains a linear classifier on the selected features.", "author_claim_text": "Ghostbuster passes paired human-authored and AI-generated documents through a series of weaker language models, running a structured search over possible combinations of their features, and then training a classifier on the selected features", "source_hint": "Abstract"}, {"name": "Three-stage training procedure", "description": "The method first converts documents into token probability vectors using weaker language models, then selects features via structured search over combinations of these probabilities using predefined vector and scalar operations, and finally trains a logistic regression classifier on the selected and handcrafted features.", "author_claim_text": "Ghostbuster uses a three-stage training process: probability computation, feature selection, and classifier training", "source_hint": "Section 4"}]}, "abs": "We introduce Ghostbuster, a state-of-the-art system for detecting AI-generated text. Our method works by passing documents through a series of weaker language models, running a structured search over possible combinations of their features, and then training a classifier on the selected features to predict whether documents are AI-generated. Crucially, Ghostbuster does not require access to token probabilities from the target model, making it useful for detecting text generated by black-box models or unknown model versions. In conjunction with our model, we release three new datasets of human- and AI-generated text as detection benchmarks in the domains of student essays, creative writing, and news articles. We compare Ghostbuster to a variety of existing detectors, including DetectGPT and GPTZero, as well as a new RoBERTa baseline. Ghostbuster achieves 99.0 F1 when evaluated across domains, which is 5.9 F1 higher than the best preexisting model. It also outperforms all previous approaches in generalization across writing domains (+7.5 F1), prompting strategies (+2.1 F1), and language models (+4.4 F1). We also analyze the robustness of our system to a variety of perturbations and paraphrasing attacks and evaluate its performance on documents written by non-native English speakers."}, {"title": "GLTR: Statistical Detection and Visualization of Generated Text", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/1_Training-based Methods/White-box/Full access/Word rank/GLTR: Statistical Detection and Visualization of Generated Text.pdf", "summary": {"paper_id": "survey-34_paper-14_GLTR: Statistical Detection and Visualization of Generated Text", "core_task": {"text": "Statistical detection and visualization of generated text", "query_variants": ["Statistical detection and visualization of generated text", "Identifying and displaying machine-generated text using statistical methods", "Statistical identification and visual representation of synthetic text"]}, "contributions": [{"name": "Detection of automatically generated text by non-experts", "description": "The paper addresses the challenge of detecting machine-generated text in an era where large language models can produce output indistinguishable from human writing. The problem is particularly acute for non-expert readers who lack tools and training to identify such text.", "author_claim_text": "The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by and explained to non-experts.", "source_hint": "Abstract"}, {"name": "Preventing malicious use of text generation at scale", "description": "The authors are motivated by real-world cases of abuse, including fake political comments and potential for false articles or reviews. They argue that forensic techniques are necessary to detect automatically generated text, and these should be accurate yet accessible to non-experts with minimal setup.", "author_claim_text": "Instances of malicious autonomously generated text at scale are rare but often high-profile, for instance when a simple generation system was used to create fake comments in opposition to net neutrality. Other scenarios include the possibility of generating false articles or misleading reviews.", "source_hint": "Introduction paragraph 2"}, {"name": "GLTR visual detection tool with statistical tests", "description": "GLTR is a visual tool that highlights text passages using three statistical tests: word probability, absolute rank of words in the predicted distribution, and entropy of the predicted distribution. The tool uses detection models like BERT and GPT-2 to compute these metrics and presents them through color-coded overlays and histograms.", "author_claim_text": "We develop GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across common sampling schemes.", "source_hint": "Abstract and Method section"}, {"name": "Three statistical tests for generation detection", "description": "The authors propose three tests based on the assumption that generation systems sample from the head of the distribution. The tests check whether generated words come from the top of the distribution and whether the context produces overly confident predictions with low entropy.", "author_claim_text": "We therefore propose three simple tests, using a detection model, to assess whether text is generated in this way: (Test 1) the probability of the word, (Test 2) the absolute rank of a word, and (Test 3) the entropy of the predicted distribution.", "source_hint": "Method section paragraph 2"}]}, "abs": "The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by and explained to non-experts. We develop GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across common sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54% to 72% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs"}, {"title": "Origin Tracing and Detecting of LLMs", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/1_Training-based Methods/White-box/Partial access/Contrastive logits feature/Origin Tracing and Detecting of LLMs.pdf", "summary": {"paper_id": "survey-34_paper-15_Origin Tracing and Detecting of LLMs", "core_task": {"text": "tracing and detecting origins of large language model generated text", "query_variants": ["tracing and detecting origins of large language model generated text", "identifying and tracking sources of text produced by large language models", "detection and attribution of content generated by large language models"]}, "contributions": [{"name": "Origin tracing and detection of LLM-generated texts", "description": "The paper addresses the challenge of determining whether a text is AI-generated and identifying which specific LLM produced it, as the proliferation of LLMs makes origin tracing increasingly difficult yet critical for preventing abuse and model theft.", "author_claim_text": "while more and more companies and institutions release their LLMs, the origin can be hard to trace. Since LLMs are heading towards the time of AGI, similar to the origin tracing in anthropology, it is of great importance to trace the origin of LLMs.", "source_hint": "Abstract"}, {"name": "Preventing AI abuse and ensuring LLM accountability", "description": "The authors are motivated by the need to prevent misuse of LLMs in contexts requiring originality and to enable accountability by tracing texts back to specific service providers, especially as future LLMs may be trained on AI-generated data.", "author_claim_text": "AI-generated context detection is useful in releasing texts that require strict censoring or originality such as official documents, consultation, and student submissions to avoid abuse of AI systems. Further, a more critical and applicable field is to trace the origin of LLMs.", "source_hint": "Introduction paragraph 1"}, {"name": "Sniffer: contrastive feature extraction across LLMs", "description": "Sniffer extracts model-wise features by computing and aligning token-level perplexities across multiple known LLMs, then crafts heuristic features (percent-of-low-perplexity scores, correlation coefficients) that capture discrepancies between models, enabling origin tracing via a simple linear classifier.", "author_claim_text": "we introduce a novel algorithm that leverages the contrastive features between LLMs and extracts model-wise features to trace the text origins.", "source_hint": "Abstract and Methods section 3"}, {"name": "Low-resource linear classifier for origin tracing", "description": "After feature extraction, a linear classifier is trained on a small amount of labeled data to map the low-dimensional feature vectors to different LLM origins, enabling both white-box and black-box detection with minimal data requirements.", "author_claim_text": "we utilize a simple linear classifier to project the extracted features to specific origins including known origins that have open-source models and unknown origins that are black boxes to users.", "source_hint": "Methods section 3.1"}]}, "abs": "The extraordinary performance of large language models (LLMs) heightens the importance of detecting whether the context is generated by an AI system. More importantly, while more and more companies and institutions release their LLMs, the origin can be hard to trace. Since LLMs are heading towards the time of AGI, similar to the origin tracing in anthropology, it is of great importance to trace the origin of LLMs. In this paper, we first raise the concern of the origin tracing of LLMs and propose an effective method to trace and detect AI-generated contexts. We introduce a novel algorithm that leverages the contrastive features between LLMs and extracts model-wise features to trace the text origins. Our proposed method works under both white-box and black-box settings therefore can be widely generalized to detect various LLMs.(e.g. can be generalized to detect GPT-3 models without the GPT-3 models). Also, our proposed method requires only limited data compared with the supervised learning methods and can be extended to trace new-coming model origins. We construct extensive experiments to examine whether we can trace the origins of given texts. We provide valuable observations based on the experimental results, such as the difficulty level of AI origin tracing, and the AI origin similarities, and call for ethical concerns of LLM providers. We are releasing all codes and data as a toolkit and benchmark for future AI origin tracing and detecting studies. \\footnote{We are releasing all available resource at \\url{this https URL}.}"}, {"title": "SeqXGPT: Sentence-Level AI-Generated Text Detection", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/1_Training-based Methods/White-box/Partial access/Logits as waves/SeqXGPT: Sentence-Level AI-Generated Text Detection.pdf", "summary": {"paper_id": "survey-34_paper-16_SeqXGPT: Sentence-Level AI-Generated Text Detection", "core_task": {"text": "sentence-level AI-generated text detection", "query_variants": ["sentence-level AI-generated text detection", "detecting machine-generated text at sentence granularity", "identification of AI-produced content in individual sentences"]}, "contributions": [{"name": "Sentence-level AI-generated text detection challenge", "description": "Existing AIGT detection methods focus on determining whether entire documents are AI-generated, but fail to address scenarios where documents contain a mix of human-written and AI-modified sentences. This limitation leads to higher false positive/negative rates and lacks fine-grained analysis capabilities.", "author_claim_text": "Current works only consider document-level AIGT detection, therefore, in this paper, we first introduce a sentence-level detection challenge", "source_hint": "Abstract"}, {"name": "Need for fine-grained detection in real-world AI-assisted writing", "description": "The motivation stems from practical usage patterns where users employ LLMs to polish or modify portions of documents rather than generating complete texts. Fine-grained sentence-level detection is necessary to accurately identify such mixed-authorship content and reduce misidentification risks.", "author_claim_text": "users often modify partial texts with LLMs rather than put full trust in LLMs to generate a whole document. Therefore, it is important to explore fine-grained (e.g. sentence-level) AIGT detection", "source_hint": "Introduction paragraph 2"}, {"name": "SeqXGPT framework using log probability features", "description": "SeqXGPT extracts word-wise log probability lists from multiple white-box LLMs as foundational features. These temporal wave-like features are processed through convolutional networks to extract local patterns, followed by self-attention layers to capture long-range dependencies for contextualized representations.", "author_claim_text": "we propose Sequence X (Check) GPT, a novel method that utilizes log probability lists from white-box LLMs as features for sentence-level AIGT detection", "source_hint": "Abstract and Section 4.3"}, {"name": "Sequence labeling approach for sentence classification", "description": "Rather than classifying sentences independently, SeqXGPT treats the entire document as input for sequence labeling, assigning labels to each word. The final sentence category is determined by selecting the most frequently occurring word-level label within that sentence.", "author_claim_text": "We employ a sequence labeling approach to train SeqXGPT, and select the most frequent word-wise category as the sentence category", "source_hint": "Introduction paragraph 4"}]}, "abs": "Widely applied large language models (LLMs) can generate human-like content, raising concerns about the abuse of LLMs. Therefore, it is important to build strong AI-generated text (AIGT) detectors. Current works only consider document-level AIGT detection, therefore, in this paper, we first introduce a sentence-level detection challenge by synthesizing a dataset that contains documents that are polished with LLMs, that is, the documents contain sentences written by humans and sentences modified by LLMs. Then we propose \\textbf{Seq}uence \\textbf{X} (Check) \\textbf{GPT}, a novel method that utilizes log probability lists from white-box LLMs as features for sentence-level AIGT detection. These features are composed like \\textit{waves} in speech processing and cannot be studied by LLMs. Therefore, we build SeqXGPT based on convolution and self-attention networks. We test it in both sentence and document-level detection challenges. Experimental results show that previous methods struggle in solving sentence-level AIGT detection, while our method not only significantly surpasses baseline methods in both sentence and document-level detection challenges but also exhibits strong generalization capabilities."}, {"title": "Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/2_Zero-shot Methods/Black-box/Known Source/Codes detection/Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization.pdf", "summary": {"paper_id": "survey-34_paper-17_Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization", "core_task": {"text": "Aspect-based and query-based text summarization", "query_variants": ["Aspect-based and query-based text summarization", "Query-focused and aspect-oriented document summarization", "Text summarization driven by aspects and queries"]}, "contributions": [{"name": "Limited exploration of LLMs for aspect or query-based summarization", "description": "While recent studies show LLMs perform well on generic news summarization, their capabilities for more practical and diverse summarization tasks such as aspect-based or query-based summarization across multiple domains remain largely unexplored.", "author_claim_text": "the performance of LLMs for more practical applications like aspect or query-based summaries is underexplored", "source_hint": "Abstract"}, {"name": "Need for diverse and nuanced summarization evaluation", "description": "The authors are motivated by the fact that aspect- and query-based summarization tasks are more challenging and valuable for real-world applications, requiring deeper document comprehension tailored to specific user interests and needs, unlike generic summarization.", "author_claim_text": "Aspect- or query-based summarization represents a more diverse and nuanced form of text summarization that has garnered significant attention within the NLP community", "source_hint": "Introduction paragraph 2"}, {"name": "Comprehensive evaluation of ChatGPT on four benchmark datasets", "description": "The authors systematically evaluate ChatGPT's zero-shot performance on four diverse benchmark datasets (QMSum, SQuALITY, CovidET, NEWTS) covering Reddit posts, news articles, dialogue meetings, and stories, using Rouge scores as the primary metric.", "author_claim_text": "we present a comprehensive evaluation of ChatGPT's performance on four distinct aspect-based and query-based text summarization tasks", "source_hint": "Introduction paragraph 3"}, {"name": "Domain-specific prompt design for diverse summarization tasks", "description": "The authors design and apply task-specific prompts for each dataset, including zero-shot prompts for most tasks and one-shot prompts for CovidET, tailored to handle different input lengths and output requirements across domains.", "author_claim_text": "Here we list prompts used in our experiments for generated summaries", "source_hint": "Section 3.1"}]}, "abs": "Text summarization has been a crucial problem in natural language processing (NLP) for several decades. It aims to condense lengthy documents into shorter versions while retaining the most critical information. Various methods have been proposed for text summarization, including extractive and abstractive summarization. The emergence of large language models (LLMs) like GPT3 and ChatGPT has recently created significant interest in using these models for text summarization tasks. Recent studies \\cite{goyal2022news, zhang2023benchmarking} have shown that LLMs-generated news summaries are already on par with humans. However, the performance of LLMs for more practical applications like aspect or query-based summaries is underexplored. To fill this gap, we conducted an evaluation of ChatGPT's performance on four widely used benchmark datasets, encompassing diverse summaries from Reddit posts, news articles, dialogue meetings, and stories. Our experiments reveal that ChatGPT's performance is comparable to traditional fine-tuning methods in terms of Rouge scores. Moreover, we highlight some unique differences between ChatGPT-generated summaries and human references, providing valuable insights into the superpower of ChatGPT for diverse text summarization tasks. Our findings call for new directions in this area, and we plan to conduct further research to systematically examine the characteristics of ChatGPT-generated summaries through extensive human evaluation."}, {"title": "Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/2_Zero-shot Methods/Black-box/Known Source/Database Retrieval/Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense.pdf", "summary": {"paper_id": "survey-34_paper-18_Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense", "core_task": {"text": "detecting AI-generated text through paraphrase attacks and retrieval-based defense", "query_variants": ["detecting AI-generated text through paraphrase attacks and retrieval-based defense", "identifying machine-generated content under paraphrasing adversarial attacks using retrieval defenses", "AI text detection against paraphrase adversarial examples with retrieval-based methods"]}, "contributions": [{"name": "Vulnerability of AI-generated text detectors to paraphrase attacks", "description": "Existing detectors of AI-generated text (watermarking, GPTZero, DetectGPT, OpenAI's classifier) have not been thoroughly tested against paraphrase attacks, where AI-generated text is rewritten by another model to evade detection while preserving semantics.", "author_claim_text": "the robustness of these detection algorithms to paraphrases of AI-generated text remains unclear", "source_hint": "Abstract"}, {"name": "Preventing malicious usage of large language models", "description": "The authors are motivated by the need to defend against malicious applications of LLMs, including fake news generation and academic dishonesty, which has driven the development of detection methods that must be robust to adversarial attacks.", "author_claim_text": "The rise in malicious usage of large language models, such as fake content creation and academic plagiarism, has motivated the development of approaches that identify AI-generated text", "source_hint": "Abstract"}, {"name": "DIPPER: controllable discourse-level paraphrase generation model", "description": "DIPPER is an 11B parameter T5-XXL-based paraphraser trained on paragraph-level paraphrase data from multiple English translations. It supports context-aware paraphrasing, content reordering, and controllable lexical diversity through scalar control knobs at inference time.", "author_claim_text": "we build a 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, condition on surrounding context, and control lexical diversity and content reordering", "source_hint": "Abstract"}, {"name": "Retrieval-based defense for detecting paraphrased AI-generated text", "description": "The proposed defense involves an LLM API provider maintaining a database of all generated sequences. A candidate text is compared semantically to this database using IR methods (BM-25) and semantic embeddings (P-SP) to detect paraphrased AI-generated content.", "author_claim_text": "we introduce a simple defense that relies on retrieving semantically-similar generations and must be maintained by a language model API provider", "source_hint": "Abstract"}]}, "abs": "The rise in malicious usage of large language models, such as fake content creation and academic plagiarism, has motivated the development of approaches that identify AI-generated text, including those based on watermarking or outlier detection. However, the robustness of these detection algorithms to paraphrases of AI-generated text remains unclear. To stress test these detectors, we build a 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, condition on surrounding context, and control lexical diversity and content reordering. Using DIPPER to paraphrase text generated by three large language models (including GPT3.5-davinci-003) successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier. For example, DIPPER drops detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of 1%), without appreciably modifying the input semantics.\nTo increase the robustness of AI-generated text detection to paraphrase attacks, we introduce a simple defense that relies on retrieving semantically-similar generations and must be maintained by a language model API provider. Given a candidate text, our algorithm searches a database of sequences previously generated by the API, looking for sequences that match the candidate text within a certain threshold. We empirically verify our defense using a database of 15M generations from a fine-tuned T5-XXL model and find that it can detect 80% to 97% of paraphrased generations across different settings while only classifying 1% of human-written sequences as AI-generated. We open-source our models, code and data."}, {"title": "DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/2_Zero-shot Methods/Black-box/Known Source/N-gram divergence/DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text.pdf", "summary": {"paper_id": "survey-34_paper-19_DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text", "core_task": {"text": "training-free detection of GPT-generated text", "query_variants": ["training-free detection of GPT-generated text", "zero-shot identification of text produced by generative pre-trained transformers", "detecting machine-generated content without training classifiers"]}, "contributions": [{"name": "Detecting GPT-generated text in the era of advanced LLMs", "description": "The paper addresses the challenge that existing detection methods for AI-generated text cannot keep pace with rapidly evolving large language models like ChatGPT and GPT-4. Training-based detectors lack flexibility, fail to generalize to new domains or models, and provide no explanations for their decisions.", "author_claim_text": "current research on detection methods lags behind the rapid evolution of LLMs. Conventional training-based methods have limitations in flexibility, particularly when adapting to new domains, and they often lack explanatory power.", "source_hint": "Abstract"}, {"name": "Need for robust, explainable, and training-free detection", "description": "The authors are motivated by the urgent need for detection methods that can adapt to continuously updated models without retraining, provide interpretable evidence for decisions (especially important in educational contexts), and work robustly across languages and under text revision attacks.", "author_claim_text": "there is a pressing demand to effectively detect GPT-generated text to match the rapid advancements of LLMs. Moreover, when formulating the detection methodology, an essential focus lies on explainability, an aspect that is often absent in existing methods", "source_hint": "Introduction §1"}, {"name": "Divergent N-Gram Analysis (DNA-GPT) framework", "description": "DNA-GPT truncates candidate text at a chosen ratio, uses the prefix to regenerate multiple continuations via the target LLM, then compares these regenerations to the original suffix. The method exploits the observation that machine-generated text exhibits lower diversity and higher likelihood overlap than human text when conditioned on the same prefix.", "author_claim_text": "we propose a novel training-free detection strategy called Divergent N-Gram Analysis (DNA-GPT). Given a text, we first truncate it in the middle and then use only the preceding portion as input to the LLMs to regenerate the new remaining parts.", "source_hint": "Abstract and Methodology §3"}, {"name": "Black-box detection via BScore (n-gram overlap)", "description": "For black-box scenarios (no token probabilities available), DNA-GPT computes a BScore by measuring weighted n-gram overlap between the original suffix and multiple regenerated suffixes. Higher overlap indicates machine generation, and the method also provides explainable evidence via overlapping n-grams.", "author_claim_text": "we compare their n-gram similarity to distinguish human- and GPT-written text. Based on our assumption, the human-generated Y0 will have a much lower overlap with Ω, compared with GPT-generated text.", "source_hint": "Methodology §3.1"}]}, "abs": "Large language models (LLMs) have notably enhanced the fluency and diversity of machine-generated text. However, this progress also presents a significant challenge in detecting the origin of a given text, and current research on detection methods lags behind the rapid evolution of LLMs. Conventional training-based methods have limitations in flexibility, particularly when adapting to new domains, and they often lack explanatory power. To address this gap, we propose a novel training-free detection strategy called Divergent N-Gram Analysis (DNA-GPT). Given a text, we first truncate it in the middle and then use only the preceding portion as input to the LLMs to regenerate the new remaining parts. By analyzing the differences between the original and new remaining parts through N-gram analysis in black-box or probability divergence in white-box, we unveil significant discrepancies between the distribution of machine-generated text and the distribution of human-written text. We conducted extensive experiments on the most advanced LLMs from OpenAI, including text-davinci-003, GPT-3.5-turbo, and GPT-4, as well as open-source models such as GPT-NeoX-20B and LLaMa-13B. Results show that our zero-shot approach exhibits state-of-the-art performance in distinguishing between human and GPT-generated text on four English and one German dataset, outperforming OpenAI's own classifier, which is trained on millions of text. Additionally, our methods provide reasonable explanations and evidence to support our claim, which is a unique feature of explainable detection. Our method is also robust under the revised text attack and can additionally solve model sourcing. Codes are available at this https URL."}, {"title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/2_Zero-shot Methods/Black-box/Known Source/Probability curve/DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature.pdf", "summary": {"paper_id": "survey-34_paper-20_DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature", "core_task": {"text": "zero-shot machine-generated text detection", "query_variants": ["zero-shot machine-generated text detection", "detecting synthetic text without training examples", "identifying AI-generated content in unsupervised setting"]}, "contributions": [{"name": "Zero-shot machine-generated text detection", "description": "The paper addresses the challenge of detecting whether text was generated by a large language model without requiring training data, fine-tuning, or explicit watermarking. Existing zero-shot methods based on raw log probabilities ignore useful structural information in the model's probability function.", "author_claim_text": "we study the zero-shot machine-generated text detection problem, in which we use only the raw log probabilities computed by a generative model to determine if a candidate passage was sampled from it", "source_hint": "Introduction and Discussion"}, {"name": "Need for reliable LLM-generated text detection", "description": "The authors are motivated by concerns that LLMs may be misused in education, journalism, and other contexts where human authorship is important. Humans perform only slightly better than chance at detecting machine-generated text, making automated detection tools necessary.", "author_claim_text": "The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text", "source_hint": "Abstract and Introduction"}, {"name": "Probability curvature-based detection criterion", "description": "The method identifies that LLM-generated text occupies regions where the log probability function has negative curvature, meaning minor perturbations tend to have lower probability. This property distinguishes machine-generated from human-written text and forms the basis for detection.", "author_claim_text": "we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM", "source_hint": "Abstract and Introduction"}, {"name": "DetectGPT algorithm using perturbation discrepancy", "description": "DetectGPT generates minor perturbations of a candidate passage using a generic pre-trained model like T5, then compares the original passage's log probability with the average log probability of perturbations. A large positive difference indicates the passage is likely machine-generated.", "author_claim_text": "To test if a passage came from a source model pθ, DetectGPT compares the log probability of the candidate passage under pθ with the average log probability of several perturbations of the passage under pθ", "source_hint": "Introduction and Section 4"}]}, "abs": "The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See this https URL for code, data, and other project information."}, {"title": "Smaller Language Models are Better Black-box Machine-Generated Text Detectors", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/2_Zero-shot Methods/Black-box/Known Source/Smaller model as a proxy/Smaller Language Models are Better Black-box Machine-Generated Text Detectors.pdf", "summary": {"paper_id": "survey-34_paper-21_Smaller Language Models are Better Black-box Machine-Generated Text Detectors", "core_task": {"text": "black-box machine-generated text detection", "query_variants": ["black-box machine-generated text detection", "detecting machine-generated text without model access", "identifying automatically generated text using black-box methods"]}, "contributions": [{"name": "Detecting machine-generated text without knowing the generator model", "description": "The paper addresses the challenge of identifying machine-generated text when the generating model is unknown or inaccessible, which is critical for verifying authenticity of news articles, product reviews, and other content in real-world scenarios where the generator model cannot be queried directly.", "author_claim_text": "we often want to distinguish between machine-generated text and human-written text in situations where we do not know which model could have been used as the generator — and even if we do know the generator, we might not have access to its likelihood function", "source_hint": "Introduction paragraph 3"}, {"name": "Preventing abuse of LLMs for unethical purposes", "description": "The authors are motivated by the need to prevent malicious uses of language models, such as generating fake news, phishing attempts, and astroturfing, which become increasingly feasible as LLM-generated text achieves human-like fluency and becomes difficult for humans to distinguish.", "author_claim_text": "such LLM-based tools can be abused for unethical purposes like phishing, astroturfing, and generating fake news. As such, we need to be able to reliably and automatically detect machine generated text.", "source_hint": "Introduction paragraph 1"}, {"name": "Cross-detection using surrogate detector models with curvature test", "description": "The authors propose using a surrogate detector model (different from the generator) to compute local optimality (curvature) by comparing the likelihood of target sequences to perturbed neighbors. This enables detection of machine-generated text without access to the actual generator model.", "author_claim_text": "we set out to explore whether it is possible to use the curvature metric measured on one language model (a detector model) to identify machine-generated text generated by another language model (the generator)", "source_hint": "Introduction paragraph 4"}, {"name": "Extensive empirical analysis across model families, sizes, and training stages", "description": "The methodology involves systematically testing 23 detector models of varying sizes, architectures, and training checkpoints against 15 generator models to identify which characteristics make effective universal detectors for machine-generated text.", "author_claim_text": "We conduct an extensive empirical analysis by experimenting on a slew of models with different sizes (from tens of millions to billions of parameters), architectures (GPTs, OPTs, Pythias) and pre-training data (Webtext and the Pile) and also from different training stages", "source_hint": "Introduction paragraph 5"}]}, "abs": "With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures. To this end, there have been a slew of methods proposed to detect machine-generated text. Most of these methods need access to the logits of the target model or need the ability to sample from the target. One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether the detector and generator were trained on the same data is not critically important to the detection success. For instance the OPT-125M model has an AUC of 0.81 in detecting ChatGPT generations, whereas a larger model from the GPT family, GPTJ-6B, has AUC of 0.45."}, {"title": "Artificial Text Detection with Multiple Training Strategies", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/2_Zero-shot Methods/Black-box/Known Source/Uncommon n-grams/Artificial Text Detection with Multiple Training Strategies.pdf", "summary": {"paper_id": "survey-34_paper-22_Artificial Text Detection with Multiple Training Strategies", "core_task": {"text": "Artificial text detection and model attribution", "query_variants": ["Artificial text detection and model attribution", "Identifying machine-generated text and tracing source models", "Detection of synthetic text and attribution to generative models"]}, "contributions": [{"name": "Detecting which generative model produced artificial text", "description": "The paper addresses the challenge of identifying which specific text generation model (from a predefined list) was used to create a given artificial text, as generative models can be misused to produce fake news, reviews, and political content.", "author_claim_text": "such models can be abused to generate product reviews, fake news, and even fake political content. The paper proposes a solution for the Russian Artificial Text Detection in the Dialogue shared task 2022 (RuATD 2022) to distinguish which model within the list is used to generate this text.", "source_hint": "Abstract"}, {"name": "Preventing misuse of text generation models for malicious purposes", "description": "The authors are motivated by the dual-use nature of text generation technologies, which can be exploited to create harmful content like fake news and propaganda, necessitating effective detection methods to mitigate these security risks.", "author_claim_text": "the success of natural language generation has drawn dual-use concerns. On the one hand, applications such as summary generation and machine translation are positive. On the other hand, related techniques may also enable adversaries to generate neural fake news, targeted propaganda and even fake political content.", "source_hint": "Introduction"}, {"name": "DeBERTa pre-trained model with multiple training strategies", "description": "The authors propose using the DeBERTa family of pre-trained language models (mDeBERTa and DeBERTa) combined with multiple training strategies including adversarial training (FGM), child-tuning, and In-trust loss function for multi-class artificial text detection.", "author_claim_text": "We introduce the DeBERTa pre-trained language model with multiple training strategies for this shared task.", "source_hint": "Abstract and Section 2"}, {"name": "Adversarial training with Fast Gradient Method", "description": "The method incorporates adversarial training using Fast Gradient Method (FGM) to add perturbations to inputs during training, making the model more robust by normalizing gradients to prevent excessive disturbance.", "author_claim_text": "The common method in adversarial training is the Fast Gradient Method. The idea of FGM is straightforward. Increasing the loss is to increase the gradient", "source_hint": "Section 2.2.1"}]}, "abs": "As the deep learning rapidly promote, the artificial texts created by generative models are commonly used in news and social media. However, such models can be abused to generate product reviews, fake news, and even fake political content. The paper proposes a solution for the Russian Artificial Text Detection in the Dialogue shared task 2022 (RuATD 2022) to distinguish which model within the list is used to generate this text. We introduce the DeBERTa pre-trained language model with multiple training strategies for this shared task. Extensive experiments conducted on the RuATD dataset validate the effectiveness of our proposed method. Moreover, our submission ranked second place in the evaluation phase for RuATD 2022 (Multi-Class)."}, {"title": "On the Coherence of Fake News Articles", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/2_Zero-shot Methods/Black-box/Known Source/Uncommon n-grams/On the Coherence of Fake News Articles.pdf", "summary": {"paper_id": "survey-34_paper-23_On the Coherence of Fake News Articles", "core_task": {"text": "analyzing textual coherence of fake news articles", "query_variants": ["analyzing textual coherence of fake news articles", "examining discourse coherence in misinformation text", "evaluating textual consistency of disinformation articles"]}, "contributions": [{"name": "Lack of understanding of textual coherence differences between fake and legitimate news", "description": "The paper addresses the gap in understanding how fake news articles differ from legitimate ones in terms of textual coherence, a dimension that has not been systematically studied despite growing interest in fake news detection using textual content.", "author_claim_text": "In this computational social science study, we analyze the textual coherence of fake news articles vis-a-vis legitimate ones.", "source_hint": "Abstract"}, {"name": "Need for more generic and robust fake news detection methods", "description": "The authors are motivated by the need to develop fake news detection approaches that rely on high-level features rather than easily circumventable low-level lexical patterns, making detection methods more transferable across domains and harder for fake news authors to evade.", "author_claim_text": "moving from low-level lexical features to higher level ones such as topical novelty, emotions and rhetorical structures would yield more 'generic' fake news identification methods that are more robust to being easily tricked by fake news peddlers", "source_hint": "Discussion §6.1"}, {"name": "Word embedding-based coherence scoring using average word2vec", "description": "The first method represents each sentence as the average of pre-trained word2vec vectors for its constituent words, then computes document coherence as the mean pairwise cosine similarity between all sentence representations.", "author_claim_text": "Each sentence si in document D is then represented as the average of the word2vec vectors of the words it contains", "source_hint": "§4, Coherence using Word Embeddings"}, {"name": "Explicit Semantic Analysis (ESA) based coherence scoring", "description": "The second method uses ESA to represent sentences as vectors in Wikipedia concept space by averaging word-level ESA vectors, then computes coherence as mean pairwise similarity between sentence representations.", "author_claim_text": "The ESA representation for each sentence is then simply the average of the ESA vectors of the words that it contains", "source_hint": "§4, Coherence using Explicit Semantic Analysis"}]}, "abs": "The generation and spread of fake news within new and online media sources is emerging as a phenomenon of high societal significance. Combating them using data-driven analytics has been attracting much recent scholarly interest. In this study, we analyze the textual coherence of fake news articles vis-a-vis legitimate ones. We develop three computational formulations of textual coherence drawing upon the state-of-the-art methods in natural language processing and data science. Two real-world datasets from widely different domains which have fake/legitimate article labellings are then analyzed with respect to textual coherence. We observe apparent differences in textual coherence across fake and legitimate news articles, with fake news articles consistently scoring lower on coherence as compared to legitimate news ones. While the relative coherence shortfall of fake news articles as compared to legitimate ones form the main observation from our study, we analyze several aspects of the differences and outline potential avenues of further inquiry."}, {"title": "Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/2_Zero-shot Methods/Black-box/Unknown Source/Intrinsic dimension/Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts.pdf", "summary": {"paper_id": "survey-34_paper-24_Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts", "core_task": {"text": "Detecting AI-generated texts using intrinsic dimension estimation", "query_variants": ["Detecting AI-generated texts using intrinsic dimension estimation", "Identifying machine-generated text through intrinsic dimensionality analysis", "AI text detection via intrinsic dimension measurement"]}, "contributions": [{"name": "Difficulty distinguishing AI-generated from human-written texts", "description": "Modern large language models produce increasingly human-like text, making it challenging to detect AI-generated content. Existing detection methods fail to generalize across different generation models, text domains, and are vulnerable to adversarial attacks.", "author_claim_text": "Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society.", "source_hint": "Abstract, first sentence"}, {"name": "Need for robust domain-agnostic AI text detection", "description": "The authors are motivated by the need for detection methods that work reliably across different text domains, languages, and generation models without requiring access to the generator itself, while avoiding bias against non-native speakers.", "author_claim_text": "it becomes increasingly important to study the properties of human texts that are invariant over different text domains and varying proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method.", "source_hint": "Abstract, second sentence"}, {"name": "Persistent homology dimension estimator for text intrinsic dimensionality", "description": "The authors propose using persistent homology dimension (PHD) to estimate the intrinsic dimensionality of text embeddings. They develop an efficient algorithm based on minimal spanning trees and growth rate analysis to compute PHD for individual text samples.", "author_claim_text": "we propose such an invariant for human-written texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings for a given text sample.", "source_hint": "Abstract, third sentence; Section 3"}, {"name": "Score-based artificial text detector using intrinsic dimension", "description": "The authors construct a simple single-feature classifier for artificial text detection using PHD as the feature, training a logistic regression on datasets of real and generated texts. The method treats each text sample as a separate manifold and estimates its dimension from contextualized token embeddings.", "author_claim_text": "This property allows us to build a score-based artificial text detector.", "source_hint": "Abstract; Section 4"}]}, "abs": "Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over different text domains and varying proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant for human-written texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings for a given text sample. We show that the average intrinsic dimensionality of fluent texts in a natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\\approx 1.5$ lower, with a clear statistical separation between human-generated and AI-generated distributions. This property allows us to build a score-based artificial text detector. The proposed detector's accuracy is stable over text domains, generator models, and human writer proficiency levels, outperforming SOTA detectors in model-agnostic and cross-domain scenarios by a significant margin."}, {"title": "DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/2_Zero-shot Methods/White-box/Full access/Log-Rank ratio/DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text.pdf", "summary": {"paper_id": "survey-34_paper-25_DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text", "core_task": {"text": "zero-shot detection of machine-generated text", "query_variants": ["zero-shot detection of machine-generated text", "identifying synthetic text without training examples", "machine-generated content detection without labeled data"]}, "contributions": [{"name": "Distinguishing machine-generated from human-written text", "description": "The paper addresses the challenge of automatically detecting machine-generated text at scale, as manual identification is impractical given the proliferation of LLMs and the volume of text they produce.", "author_claim_text": "With the rapid progress of Large language models (LLMs) and the huge amount of text they generated, it becomes more and more impractical to manually distinguish whether a text is machine-generated.", "source_hint": "Abstract, paragraph 1"}, {"name": "Preventing malicious use of LLMs", "description": "The authors are motivated by the need to mitigate negative impacts of LLMs, including plagiarism in education, spread of misinformation, and manipulation of public opinion, which pose serious societal risks.", "author_claim_text": "Given the growing use of LLMs in social media and education, it prompts us to develop methods to detect machine-generated text, preventing malicious usage such as plagiarism, misinformation, and propaganda.", "source_hint": "Abstract, paragraph 1"}, {"name": "DetectLLM-LRR using Log-Likelihood Log-Rank Ratio", "description": "DetectLLM-LRR is a perturbation-free zero-shot method that combines log-likelihood and log-rank statistics in a ratio to distinguish machine-generated from human-written text, offering efficiency without requiring text perturbations.", "author_claim_text": "We introduce two novel zero-shot methods for detecting machine-generated text by leveraging the log rank information. One is called DetectLLM-LRR, which is fast and efficient", "source_hint": "Abstract, paragraph 2"}, {"name": "DetectLLM-NPR using Normalized Perturbed log-Rank", "description": "DetectLLM-NPR is a perturbation-based zero-shot method that exploits the observation that machine-generated texts are more sensitive to minor rewrites, using normalized perturbed log-rank scores to achieve higher detection accuracy.", "author_claim_text": "the other is called DetectLLM-NPR, which is more accurate, but slower due to the need for perturbations.", "source_hint": "Abstract, paragraph 2"}]}, "abs": "With the rapid progress of large language models (LLMs) and the huge amount of text they generated, it becomes more and more impractical to manually distinguish whether a text is machine-generated. Given the growing use of LLMs in social media and education, it prompts us to develop methods to detect machine-generated text, preventing malicious usage such as plagiarism, misinformation, and propaganda. Previous work has studied several zero-shot methods, which require no training data. These methods achieve good performance, but there is still a lot of room for improvement. In this paper, we introduce two novel zero-shot methods for detecting machine-generated text by leveraging the log rank information. One is called DetectLLM-LRR, which is fast and efficient, and the other is called DetectLLM-NPR, which is more accurate, but slower due to the need for perturbations. Our experiments on three datasets and seven language models show that our proposed methods improve over the state of the art by 3.9 and 1.75 AUROC points absolute. Moreover, DetectLLM-NPR needs fewer perturbations than previous work to achieve the same level of performance, which makes it more practical for real-world use. We also investigate the efficiency--performance trade-off based on users preference on these two measures and we provide intuition for using them in practice effectively. We release the data and the code of both methods in this https URL"}, {"title": "Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/2_Zero-shot Methods/White-box/Partial access/Recent methods/Bayesian surrogate model/Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model.pdf", "summary": {"paper_id": "survey-34_paper-26_Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model", "core_task": {"text": "detecting machine-generated text from large language models", "query_variants": ["detecting machine-generated text from large language models", "identifying synthetic text produced by large language models", "recognition of LLM-generated content and text"]}, "contributions": [{"name": "Inefficiency of probability curvature-based LLM-generated text detection", "description": "The paper addresses the computational inefficiency of DetectGPT, which requires hundreds of queries to the source LLM to estimate local probability curvature for detecting a single text passage. This level of expense is impractical for large models like LLaMA, ChatGPT, and GPT-4.", "author_claim_text": "DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires querying the source LLM with hundreds of its perturbations.", "source_hint": "Abstract"}, {"name": "Practical deployment of LLM-generated text detection", "description": "The authors are motivated by the need to prevent serious social problems from LLM misuse, such as fake news creation and academic cheating. Existing detection methods either lack generalization or suffer from prohibitive computational costs, making practical deployment challenging.", "author_claim_text": "Mitigating these negative impacts has become a pressing issue for the community.", "source_hint": "Introduction §1"}, {"name": "Bayesian surrogate model with Gaussian process", "description": "The authors propose using a Gaussian process as a surrogate model to map text samples to LLM probabilities. The GP model provides non-parametric flexibility, resistance to overfitting in low-data regimes, and Bayesian uncertainty estimates that guide the selection of typical samples for efficient curvature estimation.", "author_claim_text": "we propose to incorporate a Bayesian surrogate model, which allows us to select typical samples based on Bayesian uncertainty and interpolate scores from typical samples to other samples, to improve query efficiency.", "source_hint": "Abstract and Introduction §1"}, {"name": "Sequential typical sample selection via Bayesian uncertainty", "description": "The method performs sequential selection of typical samples by iteratively choosing the sample with the highest predictive uncertainty from the current GP model, querying the source LLM for its score, and updating the GP. This uncertainty-based selection strategy is inspired by active learning approaches.", "author_claim_text": "At each step, we select the sample that the current GP model is most uncertain about, score it using the source LLM, and update the GP accordingly.", "source_hint": "Introduction §1 and Methodology §3.4"}]}, "abs": "The detection of machine-generated text, especially from large language models (LLMs), is crucial in preventing serious social problems resulting from their misuse. Some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance. Although the recent DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires querying the source LLM with hundreds of its perturbations. This paper aims to bridge this gap. Concretely, we propose to incorporate a Bayesian surrogate model, which allows us to select typical samples based on Bayesian uncertainty and interpolate scores from typical samples to other samples, to improve query efficiency. Empirical results demonstrate that our method significantly outperforms existing approaches under a low query budget. Notably, when detecting the text generated by LLaMA family models, our method with just 2 or 3 queries can outperform DetectGPT with 200 queries."}, {"title": "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/2_Zero-shot Methods/White-box/Partial access/Recent methods/Conditional probability curvature/Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature.pdf", "summary": {"paper_id": "survey-34_paper-27_Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature", "core_task": {"text": "zero-shot detection of machine-generated text", "query_variants": ["zero-shot detection of machine-generated text", "identifying synthetic text without training examples", "machine-generated content detection without labeled data"]}, "contributions": [{"name": "High computational cost of zero-shot machine-generated text detection", "description": "Existing zero-shot detectors like DetectGPT achieve good detection accuracy but require around one hundred model calls to generate perturbation texts, leading to prohibitive computational costs that hinder practical deployment.", "author_claim_text": "The leading zero-shot detector, DetectGPT, showcases commendable performance but is marred by its intensive computational costs.", "source_hint": "Abstract"}, {"name": "Need for trustworthy AI systems to distinguish machine-generated content", "description": "The ability of large language models to produce fluent content presents both productivity opportunities and societal risks such as fake news, malicious reviews, and plagiarism, making reliable detection methods essential for trustworthy AI.", "author_claim_text": "To build trustworthy AI systems, it is imperative to distinguish between machine-generated and human-authored content.", "source_hint": "Abstract and Introduction"}, {"name": "Conditional probability curvature for text detection", "description": "The authors propose using conditional probability curvature as a detection metric, based on the hypothesis that machines and humans select different words given a context. This curvature is computed by comparing the conditional probability of the original text against alternative token choices.", "author_claim_text": "we introduce the concept of conditional probability curvature to elucidate discrepancies in word choices between LLMs and humans within a given context", "source_hint": "Abstract and Section 2.3"}, {"name": "Fast-DetectGPT detection algorithm", "description": "Fast-DetectGPT replaces DetectGPT's costly perturbation-based approach with conditional independent sampling of alternative tokens, requiring only a single model forward pass instead of multiple model calls, thereby achieving significant speedup while improving accuracy.", "author_claim_text": "we present Fast-DetectGPT, an optimized zero-shot detector, which substitutes DetectGPT's perturbation step with a more efficient sampling step", "source_hint": "Abstract and Section 2.3"}]}, "abs": "Large language models (LLMs) have shown the ability to produce fluent and cogent content, presenting both productivity opportunities and societal risks. To build trustworthy AI systems, it is imperative to distinguish between machine-generated and human-authored content. The leading zero-shot detector, DetectGPT, showcases commendable performance but is marred by its intensive computational costs. In this paper, we introduce the concept of conditional probability curvature to elucidate discrepancies in word choices between LLMs and humans within a given context. Utilizing this curvature as a foundational metric, we present **Fast-DetectGPT**, an optimized zero-shot detector, which substitutes DetectGPT's perturbation step with a more efficient sampling step. Our evaluations on various datasets, source models, and test conditions indicate that Fast-DetectGPT not only surpasses DetectGPT by a relative around 75% in both the white-box and black-box settings but also accelerates the detection process by a factor of 340, as detailed in Table 1. See \\url{this https URL} for code, data, and results."}, {"title": "GPT-who: An Information Density-based Machine-Generated Text Detector", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/2_Zero-shot Methods/White-box/Partial access/Recent methods/Uniform information density/GPT-who: An Information Density-based Machine-Generated Text Detector.pdf", "summary": {"paper_id": "survey-34_paper-28_GPT-who: An Information Density-based Machine-Generated Text Detector", "core_task": {"text": "detecting machine-generated text from human-written text", "query_variants": ["detecting machine-generated text from human-written text", "distinguishing automated text from human-authored content", "identifying synthetic text versus human-composed writing"]}, "contributions": [{"name": "Detecting machine-generated text from sophisticated LLMs", "description": "The paper addresses the challenge of distinguishing texts generated by advanced Large Language Models from human-written texts, particularly as LLMs become increasingly capable of producing human-like quality text that can be misused for generating misinformation, plagiarism, and other harmful content.", "author_claim_text": "The recent ubiquity of Large Language Models (LLMs) has led to more assessments of their potential risks. These risks include its capability to generate misinformation, memorized content, plagiarized content, toxic speech, and hallucinated content.", "source_hint": "Introduction paragraph 1"}, {"name": "Mitigating risks of LLM misuse through interpretable detection", "description": "The authors are motivated by the need to prevent malicious uses of LLMs, including generation of toxic content, misinformation, and terrorism recruitment. They emphasize the importance of developing interpretable and computationally efficient detection methods that can identify which specific LLMs generated a text, which is crucial for future policymaking.", "author_claim_text": "Due to such potential for misuse, we must develop techniques to distinguish human-written texts from LLM-generated ones to mitigate these risks.", "source_hint": "Introduction paragraph 2 and Related Work section 2.2"}, {"name": "GPT-who: UID-based statistical detector", "description": "The proposed method extracts Uniform Information Density features from text by calculating token surprisals using an off-the-shelf language model (GPT2-XL). These features include mean surprisal, variance of surprisal, local differences in surprisal, and maximum/minimum UID spans. A simple logistic regression classifier then learns to distinguish authors based on these interpretable features without requiring LLM fine-tuning.", "author_claim_text": "We propose GPT-who, the first psycholinguistically-inspired domain-agnostic statistical detector. This detector employs UID-based features to model the unique statistical signature of each LLM and human author for accurate detection.", "source_hint": "Abstract and Section 3"}, {"name": "Maximum and minimum UID span features", "description": "The authors introduce novel features that identify the most and least uniform consecutive token spans (using a sliding window of 20 tokens) within an article. These extremity spans capture the full range of surprisal fluctuations and provide additional discriminative power for authorship detection.", "author_claim_text": "In addition to previously used approximations of UID, we also craft a new set of features using the most and least uniform segments of an article.", "source_hint": "Section 3.1, Maximum and minimum UID spans subsection"}]}, "abs": "The Uniform Information Density (UID) principle posits that humans prefer to spread information evenly during language production. We examine if this UID principle can help capture differences between Large Language Models (LLMs)-generated and human-generated texts. We propose GPT-who, the first psycholinguistically-inspired domain-agnostic statistical detector. This detector employs UID-based features to model the unique statistical signature of each LLM and human author for accurate detection. We evaluate our method using 4 large-scale benchmark datasets and find that GPT-who outperforms state-of-the-art detectors (both statistical- & non-statistical) such as GLTR, GPTZero, DetectGPT, OpenAI detector, and ZeroGPT by over $20$% across domains. In addition to better performance, it is computationally inexpensive and utilizes an interpretable representation of text articles. We find that GPT-who can distinguish texts generated by very sophisticated LLMs, even when the overlying text is indiscernible. UID-based measures for all datasets and code are available at this https URL."}, {"title": "Exploring the Role of Visual Content in Fake News Detection", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/2_Zero-shot Methods/White-box/Partial access/Traditional methods/Entropy/Exploring the Role of Visual Content in Fake News Detection.pdf", "summary": {"paper_id": "survey-34_paper-29_Exploring the Role of Visual Content in Fake News Detection", "core_task": {"text": "multimedia fake news detection using visual content", "query_variants": ["multimedia fake news detection using visual content", "detecting misinformation in multimodal news with image features", "identifying false news through visual and multimedia analysis"]}, "contributions": [{"name": "Limited understanding of visual content's role in fake news detection", "description": "The paper addresses the gap in understanding how visual content (images and videos) contributes to detecting fake news on social media, despite its recognized importance in fake news propagation and credibility.", "author_claim_text": "Despite the importance of visual content, our understanding about the role of visual content in fake news detection is still limited.", "source_hint": "Introduction §1.1"}, {"name": "Proliferation of multimedia fake news on social media", "description": "The authors are motivated by the increasing use of multimedia content in fake news to attract attention and mislead consumers, causing significant negative societal effects in political, economic, and social domains.", "author_claim_text": "With the development of multimedia technology, fake news attempts to utilize multimedia content with images or videos to attract and mislead consumers for rapid dissemination, which makes visual content an important part of fake news.", "source_hint": "Introduction §1.1"}, {"name": "Comprehensive review of visual features and detection methods", "description": "The authors systematically review four types of visual features (forensics, semantic, statistical, and context features) and categorize existing detection approaches into content-based and knowledge-based methods for multimedia fake news detection.", "author_claim_text": "This chapter presents a comprehensive review of the visual content in fake news, including the basic concepts, effective visual features, representative detection methods and challenging issues of multimedia fake news detection.", "source_hint": "Abstract"}, {"name": "Visual content provides multiple cues for fake news detection", "description": "The paper concludes that visual content offers diverse signals for detecting fake news through forensics traces, semantic characteristics, statistical patterns, and contextual information, which can be effectively combined for detection.", "author_claim_text": "Visual content also tells abundant cues for detecting fake news. To capture the distinctive characteristics of fake news, works extracted visual features from visual content (generally, images and videos), which can be categorized into four types: forensics features, semantic features, statistical features and context features.", "source_hint": "Chapter 2 introduction"}]}, "abs": "The increasing popularity of social media promotes the proliferation of fake news, which has caused significant negative societal effects. Therefore, fake news detection on social media has recently become an emerging research area of great concern. With the development of multimedia technology, fake news attempts to utilize multimedia content with images or videos to attract and mislead consumers for rapid dissemination, which makes visual content an important part of fake news. Despite the importance of visual content, our understanding of the role of visual content in fake news detection is still limited. This chapter presents a comprehensive review of the visual content in fake news, including the basic concepts, effective visual features, representative detection methods and challenging issues of multimedia fake news detection. This chapter can help readers to understand the role of visual content in fake news detection, and effectively utilize visual content to assist in detecting multimedia fake news."}, {"title": "Release Strategies and the Social Impacts of Language Models", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/2_Zero-shot Methods/White-box/Partial access/Traditional methods/Log probability/Release Strategies and the Social Impacts of Language Models.pdf", "summary": {"paper_id": "survey-34_paper-30_Release Strategies and the Social Impacts of Language Models", "core_task": {"text": "Release strategies and social impacts of language models", "query_variants": ["Release strategies and social impacts of language models", "Deployment approaches and societal effects of large language models", "Distribution methods and community consequences of LLMs"]}, "contributions": [{"name": "Balancing benefits and risks of releasing powerful language models", "description": "The paper addresses the challenge of how to responsibly release increasingly capable language models that offer significant benefits while also posing risks of misuse, requiring careful consideration of release strategies and social impacts.", "author_claim_text": "Large language models have a wide range of beneficial applications for society, yet also raise concerns about potential misuse.", "source_hint": "Overview"}, {"name": "Need for responsible AI development and deployment practices", "description": "The authors are motivated by the need to establish better norms and practices for AI publication and release, particularly as language models become more capable, to ensure responsible development while enabling beneficial applications.", "author_claim_text": "We believe that the AI community will eventually need to tackle the issue of publication norms in a more comprehensive fashion, and we hope that this report will help inform that discussion.", "source_hint": "Overview and Conclusion"}, {"name": "Staged release strategy", "description": "The authors implemented a staged release approach where they progressively released larger and more capable versions of their language model over time, allowing for monitoring of impacts and gathering feedback before releasing the most capable version.", "author_claim_text": "We released increasingly capable models over time, starting with a smaller 117M parameter model in February 2019, followed by larger models.", "source_hint": "Section 1 Staged Release"}, {"name": "Partnership-based access model", "description": "The authors established partnerships with external organizations and researchers to explore applications, gather diverse perspectives on impacts, and inform responsible deployment decisions through collaborative engagement.", "author_claim_text": "We partnered with a variety of organizations and researchers to better understand the beneficial use cases and challenges of GPT-2.", "source_hint": "Section 2 Partnerships"}]}, "abs": "Large language models have a range of beneficial uses: they can assist in prose, poetry, and programming; analyze dataset biases; and more. However, their flexibility and generative capabilities also raise misuse concerns. This report discusses OpenAI's work related to the release of its GPT-2 language model. It discusses staged release, which allows time between model releases to conduct risk and benefit analyses as model sizes increased. It also discusses ongoing partnership-based research and provides recommendations for better coordination and responsible publication in AI."}, {"title": "Detecting Fake News Using Machine Learning : A Systematic Literature Review", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/2_Zero-shot Methods/White-box/Partial access/Traditional methods/Perplexity/Detecting Fake News Using Machine Learning : A Systematic Literature Review.pdf", "summary": {"paper_id": "survey-34_paper-31_Detecting Fake News Using Machine Learning : A Systematic Literature Review", "core_task": {"text": "Detecting fake news using machine learning", "query_variants": ["Detecting fake news using machine learning", "Machine learning approaches for identifying misinformation", "Automated classification of false information with ML techniques"]}, "contributions": [{"name": "Difficulty in detecting fake news spread through social media platforms", "description": "The paper addresses the challenge of detecting fake news on social media platforms, where unverified users can easily spread false information that harms individuals, organizations, or political parties. Human verification is insufficient, necessitating automated detection methods.", "author_claim_text": "Detecting the fake news is a big challenge because it is not an easy task. If the fake news is not detected early then the people can spread it to others and all the people will start believing it.", "source_hint": "Introduction paragraph 2"}, {"name": "Need to prevent harm from fake news and protect reputations", "description": "The authors are motivated by the harmful impact of fake news on individuals, organizations, and political entities. Fake news can mislead public opinion, damage reputations, and influence critical decisions such as elections, making automated detection essential.", "author_claim_text": "Fake news is spread to harm the reputation of a person or an organization. It can be a propaganda against someone that can be a political party or an organization.", "source_hint": "Abstract and Introduction paragraph 1"}, {"name": "Systematic literature review of machine learning classifiers for fake news detection", "description": "The authors conduct a systematic literature review to examine how various supervised machine learning classifiers (e.g., SVM, Naïve Bayes, Logistic Regression, Neural Networks) are applied to detect fake news. They analyze training methods, feature extraction models (TF-IDF, N-Gram, Bag of Words), and classifier performance.", "author_claim_text": "Use of machine learning classifiers for detecting the fake news is described in this systematic literature review.", "source_hint": "Abstract and Methodology section"}, {"name": "Machine learning classifiers can automatically detect fake news after proper training", "description": "The paper concludes that supervised machine learning classifiers, when properly trained with labeled datasets, can effectively and automatically detect fake news on social media platforms, addressing the challenge of manual verification.", "author_claim_text": "Machine learning classifiers are using for different purposes and these can also be used for detecting the fake news. The classifiers are first trained with a data set called training data set. After that, these classifiers can automatically detect fake news.", "source_hint": "Conclusion paragraph 1"}]}, "abs": "Internet is one of the important inventions and a large number of persons are its users. These persons use this for different purposes. There are different social media platforms that are accessible to these users. Any user can make a post or spread the news through the online platforms. These platforms do not verify the users or their posts. So some of the users try to spread fake news through these platforms. These news can be propaganda against an individual, society, organization or political party. A human being is unable to detect all these fake news. So there is a need for machine learning classifiers that can detect these fake news automatically. Use of machine learning classifiers for detecting fake news is described in this systematic literature review."}, {"title": "Watermarking Text Generated by Black-Box Language Models", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/3_Watermarking Methods/Black-box/Known Source/Latest methods/Watermarking Text Generated by Black-Box Language Models.pdf", "summary": {"paper_id": "survey-34_paper-32_Watermarking Text Generated by Black-Box Language Models", "core_task": {"text": "watermarking text generated by black-box language models", "query_variants": ["watermarking text generated by black-box language models", "embedding watermarks in text from proprietary language models", "watermark insertion for black-box LM generated text"]}, "contributions": [{"name": "Watermarking text from black-box language models", "description": "Existing watermark-based detection methods require white-box access to language models (i.e., access to output probability distributions), making them inapplicable when only black-box API access is available. Third-party developers using API services cannot autonomously inject watermarks into generated text.", "author_claim_text": "However, this method is not applicable in many real-world scenarios where only black-box language models are available. For instance, third-parties that develop API-based vertical applications cannot watermark text themselves because API providers only supply generated text and withhold probability distributions to shield their commercial interests.", "source_hint": "Abstract, paragraph 2"}, {"name": "Enabling third-party watermarking for black-box LLM services", "description": "The authors are motivated by the need to enable third-party application developers who use black-box language model APIs to independently watermark and authenticate their generated content, especially as policies require labeling generated content for application approval.", "author_claim_text": "To allow third-parties to autonomously inject watermarks into generated text, we develop a watermarking framework for black-box language model usage scenarios.", "source_hint": "Abstract, paragraph 3"}, {"name": "Binary encoding function for watermark representation", "description": "The method constructs a binary encoding function that computes a random binary value (bit-0 or bit-1) for each word based on the hash of the word and its preceding word. In non-watermarked text, these encodings follow a Bernoulli distribution with equal probabilities.", "author_claim_text": "Specifically, we first define a binary encoding function to compute a random binary encoding corresponding to a word. The encodings computed for non-watermarked text conform to a Bernoulli distribution, wherein the probability of a word representing bit-1 being approximately 0.5.", "source_hint": "Abstract, paragraph 3"}, {"name": "Context-based synonym substitution for watermark injection", "description": "The watermark is injected by selectively replacing words representing bit-0 with context-based synonyms representing bit-1, using BERT to generate synonyms and employing sentence-level and word-level similarity assessments to maintain semantic quality.", "author_claim_text": "To inject a watermark, we alter the distribution by selectively replacing words representing bit-0 with context-based synonyms that represent bit-1.", "source_hint": "Abstract, paragraph 3"}]}, "abs": "LLMs now exhibit human-like skills in various fields, leading to worries about misuse. Thus, detecting generated text is crucial. However, passive detection methods are stuck in domain specificity and limited adversarial robustness. To achieve reliable detection, a watermark-based method was proposed for white-box LLMs, allowing them to embed watermarks during text generation. The method involves randomly dividing the model vocabulary to obtain a special list and adjusting the probability distribution to promote the selection of words in the list. A detection algorithm aware of the list can identify the watermarked text. However, this method is not applicable in many real-world scenarios where only black-box language models are available. For instance, third-parties that develop API-based vertical applications cannot watermark text themselves because API providers only supply generated text and withhold probability distributions to shield their commercial interests. To allow third-parties to autonomously inject watermarks into generated text, we develop a watermarking framework for black-box language model usage scenarios. Specifically, we first define a binary encoding function to compute a random binary encoding corresponding to a word. The encodings computed for non-watermarked text conform to a Bernoulli distribution, wherein the probability of a word representing bit-1 being approximately 0.5. To inject a watermark, we alter the distribution by selectively replacing words representing bit-0 with context-based synonyms that represent bit-1. A statistical test is then used to identify the watermark. Experiments demonstrate the effectiveness of our method on both Chinese and English datasets. Furthermore, results under re-translation, polishing, word deletion, and synonym substitution attacks reveal that it is arduous to remove the watermark without compromising the original semantics."}, {"title": "Tracing Text Provenance via Context-Aware Lexical Substitution", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/3_Watermarking Methods/Black-box/Known Source/Latest methods/Tracing Text Provenance via Context-Aware Lexical Substitution.pdf", "summary": {"paper_id": "survey-34_paper-33_Tracing Text Provenance via Context-Aware Lexical Substitution", "core_task": {"text": "text watermarking via context-aware lexical substitution", "query_variants": ["text watermarking via context-aware lexical substitution", "lexical replacement for text watermarking using contextual awareness", "context-sensitive word substitution for watermarking textual content"]}, "contributions": [{"name": "Limitations of existing text watermarking methods", "description": "Existing text watermarking approaches suffer from fragility to cross-media transmissions (format-based methods), failure to preserve overall sentence meaning (WordNet-based synonym substitution), impairment of logical and semantic coherence (transformer-based methods modifying function words), and poor transferability across different text types.", "author_claim_text": "traditional text watermarking methods embed watermarks by slightly altering text format like line spacing and font, which, however, are fragile to cross-media transmissions like OCR. natural language watermarking methods represent watermarks by replacing words in original sentences with synonyms from handcrafted lexical resources (e.g., WordNet), but they do not consider the substitution's impact on the overall sentence's meaning. a transformer-based network was proposed to embed watermarks by modifying the unobtrusive words (e.g., function words), which also impair the sentence's logical and semantic coherence. Besides, one well-trained network fails on other different types of text content.", "source_hint": "Abstract"}, {"name": "Tracing text provenance to prevent misuse", "description": "The authors are motivated by the need to protect text content from theft and misuse, including claiming ownership of confidential documents and identifying distributors of misleading machine-generated content such as fake news, which can cause significant losses to individuals and society.", "author_claim_text": "Text content created by humans or language models is often stolen or misused by adversaries. Tracing text provenance can help claim the ownership of text content or identify the malicious users who distribute misleading content like machine-generated fake news.", "source_hint": "Introduction paragraph 1"}, {"name": "Context-aware lexical substitution using BERT", "description": "The authors propose a lexical substitution approach that uses BERT's masked language modeling to generate candidate words and a separate BERT model to measure semantic relatedness between candidates and the original sentence. This ensures substitutions preserve overall sentence meaning by considering context.", "author_claim_text": "we employ BERT to suggest LS candidates by inferring the semantic relatedness between the candidates and the original sentence. Based on this, a selection strategy in terms of synchronicity and substitutability is further designed to test whether a word is exactly suitable for carrying the watermark signal.", "source_hint": "Abstract"}, {"name": "Synchronicity and substitutability tests", "description": "To address the challenge that context-aware substitution generates different candidates in original versus watermarked sentences, the authors design synchronicity tests (ensuring identical candidate sets at embedding and extraction) and substitutability tests (verifying words are suitable for watermark signals) to enable reliable message encoding and decoding.", "author_claim_text": "we design an LS-based sequence incremental watermarking scheme with a selection strategy in terms of synchronicity and substitutability, which enables the embedding and extraction sides can locate the same words and generate identical candidates for message encoding and decoding.", "source_hint": "Introduction paragraph 5"}]}, "abs": "Text content created by humans or language models is often stolen or misused by adversaries. Tracing text provenance can help claim the ownership of text content or identify the malicious users who distribute misleading content like machine-generated fake news. There have been some attempts to achieve this, mainly based on watermarking techniques. Specifically, traditional text watermarking methods embed watermarks by slightly altering text format like line spacing and font, which, however, are fragile to cross-media transmissions like OCR. Considering this, natural language watermarking methods represent watermarks by replacing words in original sentences with synonyms from handcrafted lexical resources (e.g., WordNet), but they do not consider the substitution's impact on the overall sentence's meaning. Recently, a transformer-based network was proposed to embed watermarks by modifying the unobtrusive words (e.g., function words), which also impair the sentence's logical and semantic coherence. Besides, one well-trained network fails on other different types of text content. To address the limitations mentioned above, we propose a natural language watermarking scheme based on context-aware lexical substitution (LS). Specifically, we employ BERT to suggest LS candidates by inferring the semantic relatedness between the candidates and the original sentence. Based on this, a selection strategy in terms of synchronicity and substitutability is further designed to test whether a word is exactly suitable for carrying the watermark signal. Extensive experiments demonstrate that, under both objective and subjective metrics, our watermarking scheme can well preserve the semantic integrity of original sentences and has a better transferability than existing methods. Besides, the proposed LS approach outperforms the state-of-the-art approach on the Stanford Word Substitution Benchmark."}, {"title": "Natural Language Adversarial Defense through Synonym Encoding", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/3_Watermarking Methods/Black-box/Known Source/Traditional methods/Natural Language Adversarial Defense through Synonym Encoding.pdf", "summary": {"paper_id": "survey-34_paper-34_Natural Language Adversarial Defense through Synonym Encoding", "core_task": {"text": "adversarial defense against synonym substitution attacks in text classification", "query_variants": ["adversarial defense against synonym substitution attacks in text classification", "defending text classifiers from adversarial synonym replacement perturbations", "robustness methods for synonym-based adversarial examples in NLP classification"]}, "contributions": [{"name": "Vulnerability of NLP models to synonym substitution attacks", "description": "Deep learning models in NLP are vulnerable to adversarial attacks, particularly synonym substitution attacks that preserve syntactic structure and semantics while fooling models. Existing defense methods are insufficient, with adversarial training requiring massive examples and certified defenses like IBP being computationally expensive and hard to scale.", "author_claim_text": "deep learning models are recently known to be vulnerable to various types of adversarial perturbations, but relatively few works are done on the defense side. Especially, there exists few effective defense method against the successful synonym substitution based attacks", "source_hint": "Abstract"}, {"name": "Need for scalable defense preserving benign accuracy", "description": "Existing certified defense methods face challenges in scaling to large models and datasets due to high computational complexity, and they significantly reduce accuracy on benign data. There is a need for an efficient defense method that maintains both robustness and accuracy without requiring extra data or architectural modifications.", "author_claim_text": "Such defenses, however, are hard to scale to large datasets and neural networks such as BERT due to the high complexity, and result in lower accuracy on benign data due to the looser upper bounds.", "source_hint": "Introduction, Section 2.3"}, {"name": "Synonym Encoding Method (SEM)", "description": "SEM constructs an encoder that maps synonym clusters to unique encodings by clustering synonyms based on Euclidean distance in embedding space. The encoder is inserted before the input layer without modifying the model architecture, forcing all synonymous neighbors of an input to share the same code, thereby smoothing the classification boundary.", "author_claim_text": "we propose a novel adversarial defense method called Synonym Encoding Method (SEM). Specifically, SEM inserts an encoder before the input layer of the target model to map each cluster of synonyms to a unique encoding", "source_hint": "Abstract, Section 3.2"}, {"name": "Synonym clustering algorithm based on word frequency", "description": "The method iterates through the dictionary in descending word frequency order, assigning each word either the code of its closest encoded synonym or itself as a new code, then propagating this code to non-encoded synonyms. This ensures high-frequency words serve as cluster representatives.", "author_claim_text": "we iterate through the word dictionary in the descending order of word frequency and try to find suitable code for each word", "source_hint": "Section 3.2, Algorithm 1"}]}, "abs": "In the area of natural language processing, deep learning models are recently known to be vulnerable to various types of adversarial perturbations, but relatively few works are done on the defense side. Especially, there exists few effective defense method against the successful synonym substitution based attacks that preserve the syntactic structure and semantic information of the original text while fooling the deep learning models. We contribute in this direction and propose a novel adversarial defense method called Synonym Encoding Method (SEM). Specifically, SEM inserts an encoder before the input layer of the target model to map each cluster of synonyms to a unique encoding and trains the model to eliminate possible adversarial perturbations without modifying the network architecture or adding extra data. Extensive experiments demonstrate that SEM can effectively defend the current synonym substitution based attacks and block the transferability of adversarial examples. SEM is also easy and efficient to scale to large models and big datasets."}, {"title": "Robust Multi-bit Natural Language Watermarking through Invariant Features", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/3_Watermarking Methods/Black-box/Known Source/Traditional methods/Robust Multi-bit Natural Language Watermarking through Invariant Features.pdf", "summary": {"paper_id": "survey-34_paper-35_Robust Multi-bit Natural Language Watermarking through Invariant Features", "core_task": {"text": "natural language watermarking with robustness to text corruption", "query_variants": ["natural language watermarking with robustness to text corruption", "text watermarking resilient to character-level noise and perturbations", "linguistic watermark methods robust against textual modifications and degradation"]}, "contributions": [{"name": "Lack of robust multi-bit watermarking for natural language", "description": "Existing natural language watermarking methods either suffer from high error rates during message extraction when embedding more bits or do not consider robustness under corruption, leading to low accuracy when watermarked texts are modified through deletion, insertion, or substitution attacks.", "author_claim_text": "To effectively combat piracy and protect copyrights, a multi-bit watermarking framework should be able to embed adequate bits of information and extract the watermarks in a robust manner despite possible corruption.", "source_hint": "Abstract"}, {"name": "Protection of valuable natural language content from piracy", "description": "The proliferation of valuable text content in subscription media, web novels, and language model outputs creates a need for copyright protection through watermarking that can trace leakage points and identify ownership, especially given concerns about illegal distribution and potential misuse of AI-generated content.", "author_claim_text": "Recent years have witnessed a proliferation of valuable original natural language contents found in subscription-based media outlets, web novel platforms, and outputs of large language models. However, these contents are susceptible to illegal piracy and potential misuse without proper security measures.", "source_hint": "Introduction paragraph 1"}, {"name": "Invariant feature-based watermark embedding framework", "description": "The authors propose a two-phase framework where Phase 1 identifies invariant features (semantic keywords via NER and YAKE, or syntactic dependencies via parsing) to determine mask positions, and Phase 2 uses an infill model to generate valid watermarked texts by filling these masks with top-k candidates sorted alphabetically.", "author_claim_text": "we explore ways to advance both payload and robustness by following a well-known proposition from image watermarking and identify features in natural language that are invariant to minor corruption.", "source_hint": "Abstract and Introduction"}, {"name": "Corruption-resistant infill model", "description": "A finetuned BERT-based infill model trained to output consistent word distributions when given original and corrupted masked sequences by minimizing reverse KL divergence between their predictions, using a masking strategy aligned with the invariant features rather than random masking.", "author_claim_text": "Through a systematic analysis of the possible sources of errors, we further propose a corruption-resistant infill model.", "source_hint": "Abstract and Section 3.3"}]}, "abs": "Recent years have witnessed a proliferation of valuable original natural language contents found in subscription-based media outlets, web novel platforms, and outputs of large language models. However, these contents are susceptible to illegal piracy and potential misuse without proper security measures. This calls for a secure watermarking system to guarantee copyright protection through leakage tracing or ownership identification. To effectively combat piracy and protect copyrights, a multi-bit watermarking framework should be able to embed adequate bits of information and extract the watermarks in a robust manner despite possible corruption. In this work, we explore ways to advance both payload and robustness by following a well-known proposition from image watermarking and identify features in natural language that are invariant to minor corruption. Through a systematic analysis of the possible sources of errors, we further propose a corruption-resistant infill model. Our full method improves upon the previous work on robustness by +16.8% point on average on four datasets, three corruption types, and two corruption ratios. Code available at this https URL."}, {"title": "Tamperproof IoT with Blockchain", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/3_Watermarking Methods/Black-box/Known Source/Traditional methods/Tamperproof IoT with Blockchain.pdf", "summary": {"paper_id": "survey-34_paper-36_Tamperproof IoT with Blockchain", "core_task": {"text": "Tamper-resistant property of Blockchain for securing IoT systems", "query_variants": ["Tamper-resistant property of Blockchain for securing IoT systems", "Immutability characteristic of Blockchain for IoT security applications", "Blockchain tamper-proof features protecting Internet of Things infrastructures"]}, "contributions": [{"name": "Inadequate data security and trust limiting IoT adoption", "description": "The paper addresses the challenge that insufficient security and trust mechanisms are preventing widespread adoption of Internet of Things systems, particularly regarding data integrity and tamper protection.", "author_claim_text": "inadequate data security and trust are seriously limiting its adoption", "source_hint": "Introduction paragraph 1"}, {"name": "Blockchain's tamper-resistance for ensuring IoT trust", "description": "The authors are motivated by the potential of Blockchain's native tamper-resistant properties to address trust issues in IoT systems, enabling secure data collection and sharing without requiring traditional third-party arbitration.", "author_claim_text": "We believe such tamper-resistance property can be of significant value in ensuring trust in IoT systems", "source_hint": "Introduction paragraph 1"}, {"name": "IoT testbed with Ethereum-based Blockchain implementation", "description": "The authors constructed a physical testbed combining IoT devices (Raspberry Pi with temperature sensors) and mining workstations, implementing an Ethereum-based Blockchain to validate tamper-resistance through experimental scenarios.", "author_claim_text": "An IoT testbed was constructed in our lab, and Ethereum based Blockchain was built in the testbed", "source_hint": "Introduction paragraph 2"}, {"name": "Tamper-resistance experiments on full and light nodes", "description": "The authors conducted systematic experiments simulating various hacking scenarios on different node types (mining nodes, non-mining nodes, light nodes) to analyze block validation processes and chain reorganization mechanisms.", "author_claim_text": "A number of tamper-resistance experiments have been carried out and analyzed to examine the process of block validation in Blockchain on both full node and light node", "source_hint": "Introduction paragraph 2"}]}, "abs": "We investigate the tamper-resistant property of Blockchain and its effectiveness for IoT systems. In particular, we implemented an IoT testbed, and built a Blockchain into the testbed. A number of tamper-resistance experiments were conducted and analyzed to corroborate the process of block validation in Blockchain. Our analysis and experimental results demonstrate the tamper-resistant capability of Blockchain in securing trust in IoT systems. The demonstration video is provided at [1]."}, {"title": "ALTER: Auxiliary Text Rewriting Tool for Natural Language Generation", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/3_Watermarking Methods/Black-box/Known Source/Traditional methods/ALTER: Auxiliary Text Rewriting Tool for Natural Language Generation.pdf", "summary": {"paper_id": "survey-34_paper-37_ALTER: Auxiliary Text Rewriting Tool for Natural Language Generation", "core_task": {"text": "auxiliary text rewriting for natural language generation tasks", "query_variants": ["auxiliary text rewriting for natural language generation tasks", "text reformulation using auxiliary information in NLG", "supporting text modification for language generation applications"]}, "contributions": [{"name": "Lack of gold references and standard evaluation for generative text editing tasks", "description": "The paper identifies that generative modeling of editing text with control attributes (GMETCA) suffers from inconsistent evaluation methods and insufficient human-generated gold references, which makes system comparison difficult and hinders automated error analysis and reproducibility of human-like editing.", "author_claim_text": "The research on GMETCA are impeded by the lack of standard evaluation practices. Different evaluation methods make system comparison across publications difficult. Moreover, the lack of human generated gold references hinders the progress of related research", "source_hint": "Introduction paragraph 2"}, {"name": "Need for task-specific feedback and traceable revision histories in text rewriting", "description": "The authors are motivated by the observation that annotators often fail to recognize subtle textual patterns associated with attributes (e.g., gender indicators), and that task-specific feedback can significantly improve both user engagement and annotation quality in text rewriting tasks.", "author_claim_text": "it is important to incorporate task-specific feedback to achieve the improvement of user engagement and quality of results. Feedback is particularly important for GMETCA according to our user study, because annotators fail to capture the weak associations between certain textual patterns and attribute values", "source_hint": "Introduction paragraph 3"}, {"name": "ALTER tool with auxiliary edit support and instant feedback", "description": "ALTER is a web-based text rewriting tool that provides annotators with instant task-specific feedback through multiple evaluation models (perplexity, word mover distance, edit distance, and classification scores) and supports flexible auxiliary editing modes including word-level operations and recommendations.", "author_claim_text": "we design ALTER, an AuxiLiary TExt Rewriting tool, to collect gold references for GMETCA. Our tool contains multiple models to provide feedback on rewriting quality and also allows easy incorporation of more task-specific evaluation models", "source_hint": "Introduction paragraph 4"}, {"name": "Word-level revision history recording with edit operations", "description": "The tool records detailed word-level revision histories by decomposing text transformations into sequences of atomic edit operations (insertions, deletions, replacements, reordering), enabling analysis of human editing behaviors and providing supervision signals for generative models.", "author_claim_text": "our tool has a module to record word-level revision histories with edit operations. The revisions are decomposed into a sequence of word-level edit operations, such as insertions (I), deletions (D), and replacements (R)", "source_hint": "Introduction paragraph 5"}]}, "abs": "In this paper, we describe ALTER, an auxiliary text rewriting tool that facilitates the rewriting process for natural language generation tasks, such as paraphrasing, text simplification, fairness-aware text rewriting, and text style transfer. Our tool is characterized by two features, i) recording of word-level revision histories and ii) flexible auxiliary edit support and feedback to annotators. The text rewriting assist and traceable rewriting history are potentially beneficial to the future research of natural language generation."}, {"title": "Advancing Beyond Identification: Multi-bit Watermark for Large Language Models", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/3_Watermarking Methods/White-box/Known Source/Multi-bit watermark/Color-listing/Advancing Beyond Identification: Multi-bit Watermark for Large Language Models.pdf", "summary": {"paper_id": "survey-34_paper-38_Advancing Beyond Identification: Multi-bit Watermark for Large Language Models", "core_task": {"text": "Multi-bit watermarking for large language model generated text", "query_variants": ["Multi-bit watermarking for large language model generated text", "Multiple bit embedding watermarks in LLM output text", "Multi-bit watermark schemes for text generated by large language models"]}, "contributions": [{"name": "Tracing adversary users beyond machine-text identification", "description": "Existing zero-bit watermarking methods only detect machine-generated text but cannot trace the specific user responsible for malicious content generation. For high-stakes misuses like disinformation campaigns, identifying the adversary user is essential for accountability and counteraction.", "author_claim_text": "when it comes to more pernicious misuses of large language models, such as the dissemination of misinformation and war propaganda on social media platforms for political or financial gains, the stakes are considerably higher, potentially leading to the erosion of social trust. In such circumstances, merely identifying the machine-generated text may not suffice for the language model providers. Instead, the ability to trace back to the adversary user responsible for generating the content becomes pivotal in counteracting such misuses.", "source_hint": "Introduction paragraph 2"}, {"name": "Enabling accountability for malicious LLM misuse", "description": "The authors are motivated by the need to hold adversary users accountable for harmful content generated via language model APIs. Multi-bit watermarking enables tracing malicious users without storing expensive query logs, allowing API providers to ban users and collaborate with law enforcement while preserving ordinary users' privacy.", "author_claim_text": "By doing so, the API providers can take a precursory measure to ban these users from their systems and allow media and social platforms, along with API providers, to collaborate with law enforcement authorities and take more decisive actions. All in all, watermarking the user information (or part thereof) can hold the adversary user accountable for potential harms facilitated through language model APIs without having to store user queries", "source_hint": "Introduction paragraph 2"}, {"name": "Multi-bit watermark via Position Allocation (MPAC)", "description": "MPAC allocates each generated token pseudo-randomly to a sub-unit (position) of the multi-bit message. The message content at that position determines which subset of the vocabulary (colorlist) is biased during generation. This independent encoding of message positions enables embedding longer messages without latency overhead and improves robustness under corruption.", "author_claim_text": "We propose Multi-bit Watermark via Position Allocation, embedding traceable multi-bit information during language model generation. Through allocating tokens onto different parts of the messages, we embed longer messages in high corruption settings without added latency. By independently embedding sub-units of messages, the proposed method outperforms the existing works in terms of robustness and latency.", "source_hint": "Abstract and Method §3.2"}, {"name": "Colorlisting for increased load capacity", "description": "Instead of using only a binary greenlist, MPAC partitions the vocabulary into multiple colored lists (determined by greenlist proportion gamma). This allows encoding multiple states per token, increasing the message capacity by converting binary messages to higher radix representations.", "author_claim_text": "To increase load capacity, we can further partition the vocabulary into multiple 'colored' lists instead of a single green list, effectively encoding multiple states for every token.", "source_hint": "Abstract and Method §3.2"}]}, "abs": "We show the viability of tackling misuses of large language models beyond the identification of machine-generated text. While existing zero-bit watermark methods focus on detection only, some malicious misuses demand tracing the adversary user for counteracting them. To address this, we propose Multi-bit Watermark via Position Allocation, embedding traceable multi-bit information during language model generation. Through allocating tokens onto different parts of the messages, we embed longer messages in high corruption settings without added latency. By independently embedding sub-units of messages, the proposed method outperforms the existing works in terms of robustness and latency. Leveraging the benefits of zero-bit watermarking, our method enables robust extraction of the watermark without any model access, embedding and extraction of long messages ($\\geq$ 32-bit) without finetuning, and maintaining text quality, while allowing zero-bit detection all at the same time. Code is released here: this https URL"}, {"title": "Three Bricks to Consolidate Watermarks for Large Language Models", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/3_Watermarking Methods/White-box/Known Source/Multi-bit watermark/Secret key per message/Three Bricks to Consolidate Watermarks for Large Language Models.pdf", "summary": {"paper_id": "survey-34_paper-39_Three Bricks to Consolidate Watermarks for Large Language Models", "core_task": {"text": "watermarking for large language models", "query_variants": ["watermarking for large language models", "embedding watermarks in LLMs", "watermark techniques for generative language models"]}, "contributions": [{"name": "Unreliable false positive rates in LLM watermark detection", "description": "Existing watermark detection methods for LLMs rely on statistical tests whose theoretical guarantees do not hold in practice, particularly at low false positive rates. The authors find that empirical FPRs are much higher than theoretical predictions, especially when tested at scale with more than 1k negative examples.", "author_claim_text": "current approaches focus their study on sensitivity (True Positive Rate: TPR) rather than on specificity (linked to False Positive Rate: FPR). The FPR has never been empirically checked at interesting scales (with more than 1k negative examples). Our large-scale experiments reveal that hypotheses of previous works do not hold and that their detection thresholds largely underestimate the false positives at low FPR.", "source_hint": "Introduction paragraph 4"}, {"name": "Preventing misuse of LLMs through reliable watermarking", "description": "The authors are motivated by the need to prevent harmful misuse of LLMs, including generating fake news, impersonation, scams, and academic dishonesty. Watermarking offers a technical means to enforce fair and responsible usage through regulations, especially since passive forensics cannot reliably distinguish generated from natural text.", "author_claim_text": "The misuse of Large Language Models (LLMs) like ChatGPT, Claude, or the open-sourced LLaMA may become a threat as their availability and capabilities expand. LLMs might help generate fake news by reducing costs to spread disinformation at scale, with a potential impact on public opinion and democratic outcomes. They could help impersonate people, facilitate scams, or make student assessments impossible.", "source_hint": "Introduction paragraph 1"}, {"name": "Non-asymptotic statistical tests for watermark detection", "description": "The authors introduce new statistical tests based on binomial and gamma distributions that provide rigorous theoretical guarantees for false positive rates. These tests replace the flawed Z-tests used in prior work and are validated to maintain accurate FPR control even at extremely low rates below 10^-6.", "author_claim_text": "This work provides grounded statistical tests that theoretically guarantee false positive-rates and accurate p-values in real-world regimes. We validate them empirically and show that they provide a close-to-perfect control of the FPR, even at low values (< 10−6).", "source_hint": "Introduction paragraph 4, Section III-B"}, {"name": "Rectified scoring strategy for watermark detection", "description": "To address the independence assumption violations caused by repeated token sequences, the authors propose a revised scoring strategy that only counts tokens whose watermark context plus current token tuple has not been previously encountered during detection. This heuristic mitigates the impact of repetitions on false positive rates.", "author_claim_text": "We guarantee FPR thanks to new statistical tests and by scoring only tokens for which {watermark context + current token} has not been scored.", "source_hint": "Section III-C"}]}, "abs": "The task of discerning between generated and natural texts is increasingly challenging. In this context, watermarking emerges as a promising technique for ascribing generated text to a specific model. It alters the sampling generation process so as to leave an invisible trace in the generated output, facilitating later detection. This research consolidates watermarks for large language models based on three theoretical and empirical considerations. First, we introduce new statistical tests that offer robust theoretical guarantees which remain valid even at low false-positive rates (less than 10$^{\\text{-6}}$). Second, we compare the effectiveness of watermarks using classical benchmarks in the field of natural language processing, gaining insights into their real-world applicability. Third, we develop advanced detection schemes for scenarios where access to the LLM is available, as well as multi-bit watermarking."}, {"title": "Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/3_Watermarking Methods/White-box/Known Source/Training-based watermark/Logits deviation w semantic embeddings/Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy.pdf", "summary": {"paper_id": "survey-34_paper-40_Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy", "core_task": {"text": "Watermarking conditional text generation for AI detection", "query_variants": ["Watermarking conditional text generation for AI detection", "Embedding watermarks in conditional language model outputs for detection", "Watermark insertion for AI-generated conditional text identification"]}, "contributions": [{"name": "Performance degradation when applying LM watermarks to conditional text generation", "description": "Existing watermarking methods designed for language models cause significant performance drops (up to 96.99% with hard watermarks and 27.54% with soft watermarks) when directly applied to conditional text generation tasks like summarization and data-to-text generation, because they randomly partition vocabulary without considering task-specific input context.", "author_claim_text": "watermarking algorithms designed for LMs cannot be seamlessly applied to conditional text generation (CTG) tasks without a notable decline in downstream task performance", "source_hint": "Abstract"}, {"name": "Need for effective AI detection in conditional text generation without quality loss", "description": "The authors are motivated by the need to detect AI-generated content to mitigate risks such as misinformation and malicious use, while maintaining high-quality outputs in conditional text generation tasks where input-output semantic overlap is substantial.", "author_claim_text": "To mitigate potential risks associated with language models (LMs), recent AI detection research proposes incorporating watermarks into machine-generated text", "source_hint": "Abstract and Introduction"}, {"name": "Semantic-aware watermarking algorithm", "description": "The proposed method uses hash functions to embed watermarks while strategically partitioning the vocabulary based on word vector similarity to the input source. Semantically related tokens are included in the green list to reduce adverse impacts, balancing randomness with semantic relevance to the input context.", "author_claim_text": "we introduce a simple yet effective semantic-aware watermarking algorithm that considers the characteristics of conditional text generation with the input context", "source_hint": "Abstract and Method section"}, {"name": "Vocabulary partitioning using model embeddings and similarity matrix", "description": "The method constructs a similarity matrix from model embeddings and uses a hyperparameter k to identify top-k semantically related tokens for each input token. These tokens are added to the green list before randomly partitioning the remaining vocabulary, ensuring coherent and contextually appropriate generation.", "author_claim_text": "we strategically bifurcate the vocabulary to balance randomness and semantic relatedness to the input source using word vector similarity", "source_hint": "Method section, Algorithm 1"}]}, "abs": "To mitigate potential risks associated with language models, recent AI detection research proposes incorporating watermarks into machine-generated text through random vocabulary restrictions and utilizing this information for detection. While these watermarks only induce a slight deterioration in perplexity, our empirical investigation reveals a significant detriment to the performance of conditional text generation. To address this issue, we introduce a simple yet effective semantic-aware watermarking algorithm that considers the characteristics of conditional text generation and the input context. Experimental results demonstrate that our proposed method yields substantial improvements across various text generation models, including BART and Flan-T5, in tasks such as summarization and data-to-text generation while maintaining detection ability."}, {"title": "A Semantic Invariant Robust Watermark for Large Language Models", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/3_Watermarking Methods/White-box/Known Source/Training-based watermark/Logits deviation w semantic embeddings/A Semantic Invariant Robust Watermark for Large Language Models.pdf", "summary": {"paper_id": "survey-34_paper-41_A Semantic Invariant Robust Watermark for Large Language Models", "core_task": {"text": "watermarking text generated by large language models", "query_variants": ["watermarking text generated by large language models", "embedding signatures in LLM-generated text", "marking content produced by large language models"]}, "contributions": [{"name": "Trade-off between attack robustness and security robustness in LLM watermarking", "description": "Existing watermarking methods for large language models cannot simultaneously achieve strong attack robustness (resistance to text modifications) and security robustness (difficulty of inferring watermarking rules). Methods using fewer preceding tokens for watermark generation are vulnerable to decryption, while those using more tokens lack robustness against semantic-preserving attacks.", "author_claim_text": "prior algorithms face a trade-off between attack robustness and security robustness. This is because the watermark logits for a token are determined by a certain number of preceding tokens; a small number leads to low security robustness, while a large number results in insufficient attack robustness.", "source_hint": "Abstract"}, {"name": "Need for watermarks robust to semantic-preserving modifications", "description": "The authors aim to create a watermarking scheme where watermark signals depend on semantic content rather than specific token sequences. This approach ensures that semantically equivalent modifications (like paraphrasing or synonym substitution) preserve the watermark, while semantic diversity provides security against rule inference attacks.", "author_claim_text": "The core motivation is generating watermark logits for each token based on the preceding tokens' semantics rather than their token IDs. Thus, semantically invariant text modifications do not alter the watermark logits, while the diversity of text semantics increases watermark complexity and guarantees security against watermark cracking.", "source_hint": "Introduction"}, {"name": "Semantic invariant watermarking using embedding LLM and watermark model", "description": "The method employs an auxiliary embedding language model (e.g., BERT) to extract semantic embeddings from preceding tokens. These embeddings are then fed into a trained watermark model—a neural network with fully connected layers—that transforms them into watermark logits, which are added to the generative LLM's logits during text generation.", "author_claim_text": "we utilize another embedding LLM to generate semantic embeddings for all preceding tokens, and then these semantic embeddings are transformed into the watermark logits through our trained watermark model.", "source_hint": "Abstract and Section 4.1"}, {"name": "Watermark model training with similarity and normalization losses", "description": "The watermark model is trained using two loss functions: a similarity loss that ensures watermark logit similarities correlate with text embedding similarities across a broad range (-1 to 1), and a normalization loss that enforces unbiased token preference and balanced scores by centering means to zero and making absolute values uniform.", "author_claim_text": "To achieve these goals, two training objectives are adopted: a similarity loss and a normalization loss. For the similarity loss, to ensure the diversity of the watermark logits, we first rescale the similarity values between text embeddings to range from -1 to 1, and then make the similarity of the generated watermark logits fit this similarity.", "source_hint": "Introduction and Section 4.2"}]}, "abs": "Watermark algorithms for large language models (LLMs) have achieved extremely high accuracy in detecting text generated by LLMs. Such algorithms typically involve adding extra watermark logits to the LLM's logits at each generation step. However, prior algorithms face a trade-off between attack robustness and security robustness. This is because the watermark logits for a token are determined by a certain number of preceding tokens; a small number leads to low security robustness, while a large number results in insufficient attack robustness. In this work, we propose a semantic invariant watermarking method for LLMs that provides both attack robustness and security robustness. The watermark logits in our work are determined by the semantics of all preceding tokens. Specifically, we utilize another embedding LLM to generate semantic embeddings for all preceding tokens, and then these semantic embeddings are transformed into the watermark logits through our trained watermark model. Subsequent analyses and experiments demonstrated the attack robustness of our method in semantically invariant settings: synonym substitution and text paraphrasing settings. Finally, we also show that our watermark possesses adequate security robustness. Our code and data are available at \\href{this https URL}{this https URL\\_Watermark}. Additionally, our algorithm could also be accessed through MarkLLM \\citep{pan2024markllm} \\footnote{this https URL}."}, {"title": "REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/3_Watermarking Methods/White-box/Known Source/Training-based watermark/Message encoding w reparameterization/REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models.pdf", "summary": {"paper_id": "survey-34_paper-42_REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models", "core_task": {"text": "watermarking text generated by large language models", "query_variants": ["watermarking text generated by large language models", "embedding signatures in LLM-generated text", "marking text produced by large-scale language models"]}, "contributions": [{"name": "Malicious exploitation and IP protection of LLM-generated content", "description": "The paper addresses the challenge of protecting intellectual property in LLM-generated texts and preventing malicious uses such as spam and plagiarism, while existing watermarking methods either distort semantics or have limited signature capacity.", "author_claim_text": "the generated content is prone to malicious exploitation, including spamming and plagiarism. To address the challenges, REMARK-LLM proposes...", "source_hint": "Abstract"}, {"name": "Need for ownership verification and content tracing in LLMs", "description": "The authors are motivated by the need to enable LLM proprietors to monitor content utilization, validate ownership, detect plagiarism, protect copyrights, and track potential misinformation generated by LLMs.", "author_claim_text": "there is a need to devise mechanisms to claim ownership of LLM-generated text and trace the distribution of the generated content", "source_hint": "Introduction §1"}, {"name": "Learning-based message encoding module", "description": "A Seq2Seq-based module that encodes LLM-generated texts and binary signatures into latent feature space, producing a watermarked distribution over the vocabulary while preserving semantic integrity.", "author_claim_text": "a learning-based message encoding module to infuse binary signatures into LLM-generated texts", "source_hint": "Abstract"}, {"name": "Reparameterization module with Gumbel-Softmax", "description": "A module that uses Gumbel-Softmax to convert dense token distributions from the encoder into sparse, differentiable representations suitable for message decoding, bridging the gap between continuous and discrete token spaces.", "author_claim_text": "a reparameterization module to transform the dense distributions from the message encoding to the sparse distribution of the watermarked textual tokens", "source_hint": "Abstract and Section 4.1"}]}, "abs": "We present REMARK-LLM, a novel efficient, and robust watermarking framework designed for texts generated by large language models (LLMs). Synthesizing human-like content using LLMs necessitates vast computational resources and extensive datasets, encapsulating critical intellectual property (IP). However, the generated content is prone to malicious exploitation, including spamming and plagiarism. To address the challenges, REMARK-LLM proposes three new components: (i) a learning-based message encoding module to infuse binary signatures into LLM-generated texts; (ii) a reparameterization module to transform the dense distributions from the message encoding to the sparse distribution of the watermarked textual tokens; (iii) a decoding module dedicated for signature extraction; Furthermore, we introduce an optimized beam search algorithm to guarantee the coherence and consistency of the generated content. REMARK-LLM is rigorously trained to encourage the preservation of semantic integrity in watermarked content, while ensuring effective watermark retrieval. Extensive evaluations on multiple unseen datasets highlight REMARK-LLM proficiency and transferability in inserting 2 times more signature bits into the same texts when compared to prior art, all while maintaining semantic integrity. Furthermore, REMARK-LLM exhibits better resilience against a spectrum of watermark detection and removal attacks."}, {"title": "What's my role? Modelling responsibility for AI-based safety-critical systems", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/3_Watermarking Methods/White-box/Known Source/Training-free watermark/Exponential minimum sampling/What's my role? Modelling responsibility for AI-based safety-critical systems.pdf", "summary": {"paper_id": "survey-34_paper-43_What's my role? Modelling responsibility for AI-based safety-critical systems", "core_task": {"text": "modelling responsibility for AI-based safety-critical systems", "query_variants": ["modelling responsibility for AI-based safety-critical systems", "accountability frameworks for artificial intelligence in safety-critical applications", "responsibility attribution in AI-driven critical safety systems"]}, "contributions": [{"name": "Responsibility gaps and liability sinks in AI-based safety-critical systems", "description": "The paper addresses the challenge that AI-based safety-critical systems create responsibility gaps due to complex development cycles, uncertainty in black-box AI components, and dynamic operating environments. This makes it difficult to attribute responsibility for harm, while human operators may unfairly absorb blame for AI system outputs they did not create.", "author_claim_text": "Many authors have commented on the 'responsibility gap' where it is difficult for developers and manufacturers to be held responsible for behaviour of an AI-SCS which contributes to harm. Instead, a human operator of the AI-SCS can become a 'liability sink' absorbing blame for the consequences of AI-SCS outputs they weren't responsible for creating.", "source_hint": "Abstract and Introduction §1"}, {"name": "Need to prevent unfair blame and learn from accidents in AI-SCS", "description": "The authors are motivated by the need to identify responsible actors after accidents to learn from mistakes and prevent future incidents. They aim to provide clarity on responsibility issues to avoid unfair blame on operators or developers and support safety assurance processes for AI-based safety-critical systems.", "author_claim_text": "Following an accident it will be important to identify causal contributions and the different responsible actors behind those to learn from mistakes and prevent similar future events. Our aim is to present a practical method to precisely capture responsibility relationships, and provide clarity on the previously identified responsibility issues.", "source_hint": "Abstract and Introduction §1"}, {"name": "Role responsibility modelling notation with typed relationships", "description": "The authors extend prior work on responsibility modelling and task analysis to develop a notation for modelling role responsibility relationships in AI-SCS. The notation uses the formulation 'Actor is responsible for Occurrence' and includes elements for actors (individuals, institutions, AI systems), occurrences (decisions, actions, omissions), resources, and typed relationships (responsibility types, uses, subordinate to, association, acts as).", "author_claim_text": "We use a core conceptual formulation Actor(A) is responsible for Occurrence(O) to create detailed role responsibility models, including related tasks and resources. Our notation is an adapted version of the task responsibility notation presented by Lock, incorporating the responsibility concepts provided by Porter et al.", "source_hint": "Abstract, Introduction §1, and Method §3.1"}, {"name": "HAZOP-style analysis method with responsibility-specific guidewords", "description": "The authors propose an analysis method that uses guidewords (insufficient/partial, misassigned, overloaded, duplicated/conflict, missing/never/omission, early/late/ordering, incorrect/value) to systematically review responsibility models for safety impacts. This method helps identify issues such as missing responsibilities, conflicts, and burden of risk distribution.", "author_claim_text": "We propose an analysis method to review the models for safety impacts. Taking inspiration from the work of Lock and Baxter we have developed a set of guidewords used to prompt discussion and identification of issues for occurrences.", "source_hint": "Abstract, Introduction §1, and Method §3.2.2"}]}, "abs": "AI-Based Safety-Critical Systems (AI-SCS) are being increasingly deployed in the real world. These can pose a risk of harm to people and the environment. Reducing that risk is an overarching priority during development and operation. As more AI-SCS become autonomous, a layer of risk management via human intervention has been removed. Following an accident it will be important to identify causal contributions and the different responsible actors behind those to learn from mistakes and prevent similar future events. Many authors have commented on the \"responsibility gap\" where it is difficult for developers and manufacturers to be held responsible for harmful behaviour of an AI-SCS. This is due to the complex development cycle for AI, uncertainty in AI performance, and dynamic operating environment. A human operator can become a \"liability sink\" absorbing blame for the consequences of AI-SCS outputs they weren't responsible for creating, and may not have understanding of.\nThis cross-disciplinary paper considers different senses of responsibility (role, moral, legal and causal), and how they apply in the context of AI-SCS safety. We use a core concept (Actor(A) is responsible for Occurrence(O)) to create role responsibility models, producing a practical method to capture responsibility relationships and provide clarity on the previously identified responsibility issues. Our paper demonstrates the approach with two examples: a retrospective analysis of the Tempe Arizona fatal collision involving an autonomous vehicle, and a safety focused predictive role-responsibility analysis for an AI-based diabetes co-morbidity predictor. In both examples our primary focus is on safety, aiming to reduce unfair or disproportionate blame being placed on operators or developers. We present a discussion and avenues for future research."}, {"title": "Undetectable Watermarks for Language Models", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/3_Watermarking Methods/White-box/Known Source/Training-free watermark/Hashing of blocks/Undetectable Watermarks for Language Models.pdf", "summary": {"paper_id": "survey-34_paper-44_Undetectable Watermarks for Language Models", "core_task": {"text": "watermarking language model outputs", "query_variants": ["watermarking language model outputs", "embedding signatures in text generated by language models", "marking outputs produced by neural language generation systems"]}, "contributions": [{"name": "Detecting AI-generated text without degrading output quality", "description": "Existing watermarking schemes for language models noticeably alter the output distribution, making it possible for users to detect the watermark and potentially degrading text quality. The paper addresses whether watermarks can be embedded without any observable change to the model's output distribution.", "author_claim_text": "Is it possible to introduce a watermark without incurring any detectable change to the output distribution?", "source_hint": "Abstract"}, {"name": "Preventing misuse while maintaining model quality", "description": "The authors are motivated by the need to distinguish AI-generated text from human-written text to prevent misuse (e.g., academic dishonesty), while ensuring that watermarking does not degrade the quality or utility of state-of-the-art language models, which would be unacceptable to model creators and users.", "author_claim_text": "the creator of a state-of-the-art LLM is unlikely to intentionally degrade the quality of their model, making Property (1) necessary for any practical watermarking scheme", "source_hint": "Introduction §1"}, {"name": "Cryptographically undetectable watermarking scheme", "description": "The authors propose a watermarking scheme that uses pseudorandom functions (PRFs) derived from one-way functions. The scheme embeds watermarks by biasing token selection based on a secret key, ensuring that without the key, the watermarked distribution is computationally indistinguishable from the original model's distribution.", "author_claim_text": "We construct undetectable watermarks based on the existence of one-way functions, a standard assumption in cryptography.", "source_hint": "Abstract"}, {"name": "Empirical entropy-based detection", "description": "The authors introduce the concept of empirical entropy to quantify the randomness used in generating a specific output. Detection of watermarks is conditioned on outputs having sufficiently high empirical entropy, ensuring that only outputs with enough randomness are watermarked and detectable.", "author_claim_text": "We introduce a formal notion that we call empirical entropy, and show that this condition is necessary.", "source_hint": "Introduction §1"}]}, "abs": "Recent advances in the capabilities of large language models such as GPT-4 have spurred increasing concern about our ability to detect AI-generated text. Prior works have suggested methods of embedding watermarks in model outputs, by noticeably altering the output distribution. We ask: Is it possible to introduce a watermark without incurring any detectable change to the output distribution?\nTo this end we introduce a cryptographically-inspired notion of undetectable watermarks for language models. That is, watermarks can be detected only with the knowledge of a secret key; without the secret key, it is computationally intractable to distinguish watermarked outputs from those of the original model. In particular, it is impossible for a user to observe any degradation in the quality of the text. Crucially, watermarks should remain undetectable even when the user is allowed to adaptively query the model with arbitrarily chosen prompts. We construct undetectable watermarks based on the existence of one-way functions, a standard assumption in cryptography."}, {"title": "Provable Robust Watermarking for AI-Generated Text", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/3_Watermarking Methods/White-box/Known Source/Training-free watermark/Logits deviation w fixed split/Provable Robust Watermarking for AI-Generated Text.pdf", "summary": {"paper_id": "survey-34_paper-45_Provable Robust Watermarking for AI-Generated Text", "core_task": {"text": "Watermarking large language model generated text", "query_variants": ["Watermarking large language model generated text", "Embedding watermarks in LLM output text", "Watermark techniques for text produced by large language models"]}, "contributions": [{"name": "Lack of provable robustness guarantees for LLM watermarks against post-processing attacks", "description": "Existing watermarking methods for large language models lack formal guarantees of robustness when adversaries edit or paraphrase the watermarked text, making it difficult to ensure reliable detection after post-processing attacks.", "author_claim_text": "The most notable challenge for the watermarking approach is that the planted patterns could be post-processed away.", "source_hint": "Introduction paragraph 4"}, {"name": "Addressing safety challenges and misuse of LLMs through provably robust watermarking", "description": "The authors are motivated by the need to prevent misuse of LLMs such as generating fake news, academic dishonesty, and contaminating training data, which requires a watermarking approach that can reliably prove text origin even after adversarial modifications.", "author_claim_text": "the ease of using LLMs has raised concerns about their potential misuse. For example, LLMs could be used to generate fake news, contaminate web content, or assist in academic dishonesty.", "source_hint": "Introduction paragraph 1"}, {"name": "Unigram-Watermark with fixed Green-Red vocabulary split", "description": "The method randomly partitions the vocabulary into a fixed green list and red list, increases logits of green tokens by delta during generation, and detects watermarks by counting green tokens in suspect text using a z-statistic test.", "author_claim_text": "We propose a robust and high-quality watermark method, Unigram-Watermark, by extending an existing approach with a simplified fixed grouping strategy.", "source_hint": "Abstract and Section 2.3"}, {"name": "Rigorous theoretical framework for quantifying watermark effectiveness and robustness", "description": "The framework formally defines watermarking schemes with quality guarantees measured by Renyi divergence, Type I/II error bounds for detection correctness, and security properties quantifying robustness to adversarial edits via edit distance.", "author_claim_text": "We propose a rigorous theoretical framework to quantify the effectiveness and robustness of LLM watermarks.", "source_hint": "Abstract and Section 2.1"}]}, "abs": "We study the problem of watermarking large language models (LLMs) generated text -- one of the most promising approaches for addressing the safety challenges of LLM usage. In this paper, we propose a rigorous theoretical framework to quantify the effectiveness and robustness of LLM watermarks. We propose a robust and high-quality watermark method, Unigram-Watermark, by extending an existing approach with a simplified fixed grouping strategy. We prove that our watermark method enjoys guaranteed generation quality, correctness in watermark detection, and is robust against text editing and paraphrasing. Experiments on three varying LLMs and two datasets verify that our Unigram-Watermark achieves superior detection accuracy and comparable generation quality in perplexity, thus promoting the responsible use of LLMs. Code is available at this https URL."}, {"title": "A Watermark for Large Language Models", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/3_Watermarking Methods/White-box/Known Source/Training-free watermark/Logits deviation w green-red list/A Watermark for Large Language Models.pdf", "summary": {"paper_id": "survey-34_paper-46_A Watermark for Large Language Models", "core_task": {"text": "watermarking large language model output", "query_variants": ["watermarking large language model output", "embedding signatures in LLM generated text", "tracing provenance of large language model generations"]}, "contributions": [{"name": "Detecting machine-generated text from large language models", "description": "The paper addresses the challenge of identifying synthetic text produced by large language models to mitigate potential harms such as social engineering, fake news creation, academic cheating, and contamination of training datasets with synthetic data.", "author_claim_text": "the ability to detect and audit the usage of machine-generated text becomes a key principle of harm reduction for large language models", "source_hint": "Introduction paragraph 1"}, {"name": "Mitigating harms of large language models through output detection", "description": "The authors are motivated by the increasing risks of malicious uses of LLMs, including automated bots for manipulation, fake content generation, and academic dishonesty. They seek to enable third parties to detect and audit machine-generated text as a harm reduction strategy.", "author_claim_text": "Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable", "source_hint": "Abstract and Introduction paragraph 1"}, {"name": "Soft red list watermarking with green token promotion", "description": "The method partitions the vocabulary into green and red lists using a hash of the previous token, then adds a constant delta to the logits of green tokens before sampling. This soft promotion adaptively enforces the watermark in high-entropy contexts while minimally affecting low-entropy sequences.", "author_claim_text": "The watermark works by selecting a randomized set of 'green' tokens before a word is generated, and then softly promoting use of green tokens during sampling", "source_hint": "Abstract and Algorithm 2 in Section 3"}, {"name": "Statistical test for watermark detection using z-statistic", "description": "The authors develop a one-proportion z-test that evaluates whether the observed number of green tokens significantly exceeds what would be expected by chance under the null hypothesis that text was generated without knowledge of the watermark rule, enabling rigorous confidence measures.", "author_claim_text": "We propose a statistical test for detecting the watermark with interpretable p-values", "source_hint": "Abstract and Section 2"}]}, "abs": "Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of \"green\" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security."}, {"title": "StealthInk: A Multi-bit and Stealthy Watermark for Large Language Models", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/3_Watermarking Methods/White-box/Known Source/Training-free watermark/Reweight strategy w ciphers/StealthInk: A Multi-bit and Stealthy Watermark for Large Language Models.pdf", "summary": {"paper_id": "survey-34_paper-47_StealthInk: A Multi-bit and Stealthy Watermark for Large Language Models", "core_task": {"text": "multi-bit watermarking for large language models", "query_variants": ["multi-bit watermarking for large language models", "embedding multiple bits as watermarks in LLMs", "multi-bit watermark insertion for generative language models"]}, "contributions": [{"name": "Existing watermarking schemes compromise text distribution or lack multi-bit capacity", "description": "Current LLM watermarking methods either distort the original text distribution generated by the model or only support zero-bit watermarks that enable detection without embedding provenance information like userID or TimeStamp.", "author_claim_text": "Existing approaches, however, either compromise the distribution of original generated text by LLMs or are limited to embedding zero-bit information that only allows for watermark detection but ignores identification.", "source_hint": "Abstract"}, {"name": "Need for traceability and prevention of LLM misuse", "description": "The authors are motivated by the need to address ethical concerns around LLM misuse, including fake news generation, scams, and plagiarism, by enabling fast traceability of AI-generated text through embedded provenance information.", "author_claim_text": "A key concern is their potential for facilitating the creation of fake news, which could be disseminated at scale, influencing public opinion and democratic processes more easily. Moreover, there is a risk of misuse of LLMs for scams or academic plagiarism.", "source_hint": "Introduction §1"}, {"name": "StealthInk multi-bit watermarking scheme with novel reweighting strategy", "description": "StealthInk uses a novel token sampling probability reweighting strategy that doubles probabilities of certain tokens while zeroing others (red tokens), preserving the expected distribution over random permutations while embedding multi-bit messages.", "author_claim_text": "We propose StealthInk, a stealthy multi-bit watermarking scheme that preserves the original text distribution while enabling the embedding of provenance information, such as userID, TimeStamp, and modelID, within LLM-generated text.", "source_hint": "Abstract"}, {"name": "K-shot stealthiness guarantee for multi-bit watermarking", "description": "The authors extend the stealthiness concept from zero-bit to multi-bit watermarking, proving that when each token is generated with unbiasedness and ciphers are independent, multiple prompt-response pairs maintain K-shot stealthiness.", "author_claim_text": "We adopt the definition of a stealthy or unbiased watermark used in (Hu et al., 2024; Wu et al., 2024) and extend the concept to multi-bit watermarking.", "source_hint": "Methodology §4.1"}]}, "abs": "Watermarking for large language models (LLMs) offers a promising approach to identifying AI-generated text. Existing approaches, however, either compromise the distribution of original generated text by LLMs or are limited to embedding zero-bit information that only allows for watermark detection but ignores identification. We present StealthInk, a stealthy multi-bit watermarking scheme that preserves the original text distribution while enabling the embedding of provenance data, such as userID, TimeStamp, and modelID, within LLM-generated text. This enhances fast traceability without requiring access to the language model's API or prompts. We derive a lower bound on the number of tokens necessary for watermark detection at a fixed equal error rate, which provides insights on how to enhance the capacity. Comprehensive empirical evaluations across diverse tasks highlight the stealthiness, detectability, and resilience of StealthInk, establishing it as an effective solution for LLM watermarking applications."}, {"title": "Robust Distortion-free Watermarks for Language Models", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/3_Watermarking Methods/White-box/Known Source/Training-free watermark/Sampling w randomized number/Robust Distortion-free Watermarks for Language Models.pdf", "summary": {"paper_id": "survey-34_paper-48_Robust Distortion-free Watermarks for Language Models", "core_task": {"text": "watermarking text from language models", "query_variants": ["watermarking text from language models", "embedding watermarks in LM generated text", "text watermarking for neural language model outputs"]}, "contributions": [{"name": "Lack of robust, distortion-free watermarks for language models", "description": "Prior watermarking methods for language models fail to simultaneously preserve the original text distribution and remain detectable after adversarial edits or cropping of the watermarked text.", "author_claim_text": "Existing watermarks either distort the model's sampling distribution, thus altering the API functionality, or are not robust to editing or cropping the text.", "source_hint": "Introduction, paragraph 3"}, {"name": "Need for provenance of language model-generated content", "description": "The proliferation of synthetic text from language models necessitates reliable forensic tools to attribute text to specific models, enabling platform moderators, educators, and model providers to enforce policies and track model usage or misuse.", "author_claim_text": "The ability of language models to mass produce human-like text creates an acute, renewed emphasis on the importance of provenance of generated content.", "source_hint": "Introduction, paragraph 1"}, {"name": "Distortion-free watermarking via inverse transform sampling", "description": "The authors propose a watermark using inverse transform sampling, where a decoder maps uniform random variables and permutations to tokens. Detection correlates permuted token indices with the random sequence, ensuring the watermark is distortion-free and robust to edits.", "author_claim_text": "We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling.", "source_hint": "Abstract; Section 2.4"}, {"name": "Distortion-free watermarking via exponential minimum sampling", "description": "The authors propose a second watermark using exponential minimum sampling, mapping random vectors to tokens via a decoder. This method is also distortion-free and robust, with detection based on alignment costs derived from the sampling mechanism.", "author_claim_text": "We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling.", "source_hint": "Abstract; Section 2.5"}]}, "abs": "We propose a methodology for planting watermarks in text from an autoregressive language model that are robust to perturbations without changing the distribution over text up to a certain maximum generation budget. We generate watermarked text by mapping a sequence of random numbers -- which we compute using a randomized watermark key -- to a sample from the language model. To detect watermarked text, any party who knows the key can align the text to the random number sequence. We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling. We apply these watermarks to three language models -- OPT-1.3B, LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text ($p \\leq 0.01$) from $35$ tokens even after corrupting between $40$-$50\\%$ of the tokens via random edits (i.e., substitutions, insertions or deletions). For the Alpaca-7B model, we conduct a case study on the feasibility of watermarking responses to typical user instructions. Due to the lower entropy of the responses, detection is more difficult: around $25\\%$ of the responses -- whose median length is around $100$ tokens -- are detectable with $p \\leq 0.01$, and the watermark is also less robust to certain automated paraphrasing attacks we implement."}, {"title": "SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation", "path": "/mnt/data/zrz_survey_data/50_A Survey on Detection of LLMs-Generated Content/LLMs-generated content detection/3_Watermarking Methods/White-box/Known Source/Training-free watermark/Sentence-level w rejection sampling/SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation.pdf", "summary": {"paper_id": "survey-34_paper-49_SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation", "core_task": {"text": "semantic watermarking for text generation with paraphrastic robustness", "query_variants": ["semantic watermarking for text generation with paraphrastic robustness", "text generation watermarking robust to paraphrasing using semantic methods", "paraphrase-resistant semantic watermarks for generated text"]}, "contributions": [{"name": "Vulnerability of token-level watermarks to paraphrase attacks", "description": "Current watermarking methods for LLM-generated text operate at the token level, making them susceptible to paraphrase attacks that alter surface tokens while preserving semantics. This undermines the detectability of watermarks after text is paraphrased.", "author_claim_text": "existing watermarked generation algorithms employ token-level designs and therefore, are vulnerable to paraphrase attacks", "source_hint": "Abstract, first sentence"}, {"name": "Detecting machine-generated text to prevent LLM misuse", "description": "The authors are motivated by the urgent need to detect machine-generated text as LLMs produce increasingly realistic outputs, which raises concerns about misinformation, impersonation, and copyright violations.", "author_claim_text": "This problem is of extreme importance now that large language models (LLMs) such as GPT-4 generate realistic text, increasing risks of LLM misuse, such as generation of misinformation, impersonation, and copyright infringements", "source_hint": "Introduction, paragraph 1"}, {"name": "SEMSTAMP: Sentence-level semantic watermarking with LSH", "description": "SEMSTAMP operates on sentence-level semantic embeddings rather than tokens. It uses LSH to partition the semantic space into valid and blocked regions, then applies rejection sampling to generate sentences whose embeddings fall into valid regions, making the watermark robust to paraphrasing.", "author_claim_text": "We propose SEMSTAMP, a robust sentence-level semantic watermarking algorithm that uses locality-sensitive hashing (LSH) to partition the semantic space of sentences", "source_hint": "Abstract"}, {"name": "Paraphrase-robust sentence encoder via contrastive learning", "description": "The authors fine-tune a Sentence-BERT encoder using contrastive learning on paraphrased data to ensure that paraphrases of a sentence remain close in the embedding space, enhancing the robustness of LSH signatures under paraphrasing.", "author_claim_text": "we fine-tune a paraphrase-robust sentence encoder with a contrastive learning objective", "source_hint": "Abstract; Introduction §1"}]}, "abs": "Existing watermarking algorithms are vulnerable to paraphrase attacks because of their token-level design. To address this issue, we propose SemStamp, a robust sentence-level semantic watermarking algorithm based on locality-sensitive hashing (LSH), which partitions the semantic space of sentences. The algorithm encodes and LSH-hashes a candidate sentence generated by an LLM, and conducts sentence-level rejection sampling until the sampled sentence falls in watermarked partitions in the semantic embedding space. A margin-based constraint is used to enhance its robustness. To show the advantages of our algorithm, we propose a \"bigram\" paraphrase attack using the paraphrase that has the fewest bigram overlaps with the original sentence. This attack is shown to be effective against the existing token-level watermarking method. Experimental results show that our novel semantic watermark algorithm is not only more robust than the previous state-of-the-art method on both common and bigram paraphrase attacks, but also is better at preserving the quality of generation."}], "survey_topic_path": "50_A Survey on Detection of LLMs-Generated Content", "survey_topic": "A Survey on Detection of LLMs-Generated Content", "gt_paper_count": 50, "gt": {"name": "50_A Survey on Detection of LLMs-Generated Content", "subtopics": [{"name": "LLMs-generated content detection", "subtopics": [{"name": "1_Training-based Methods", "subtopics": [{"name": "Black-box", "subtopics": [{"name": "Known Source", "subtopics": [{"name": "Mixed decoding", "papers": ["RefGPT: Dialogue Generation of GPT, by GPT, and for GPT", "Automatic Detection of Generated Text is Easiest when Humans are Fooled"]}, {"name": "Mixed sources", "papers": ["Technical Report on the Pangram AI-Generated Text Classifier", "GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content", "Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays"]}, {"name": "Mixed strategies", "subtopics": [{"name": "Adversarial training", "papers": ["RADAR: Robust AI-Text Detection via Adversarial Learning"]}, {"name": "Graph structure and contrastive learning", "papers": ["CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Data Limitation With Contrastive Learning"]}, {"name": "Positive unlabeled training", "papers": ["Multiscale Positive-Unlabeled Detection of AI-Generated Texts"]}, {"name": "Proxy perplexity", "papers": ["LLMDet: A Third Party Large Language Models Generated Text Detection Tool"]}]}]}, {"name": "Unknown Source", "subtopics": [{"name": "Cross-domain transfer", "papers": ["On the Zero-Shot Generalization of Machine-Generated Text Detectors", "From Text to Source: Results in Detecting Large Language Model-Generated Content", "ConDA: Contrastive Domain Adaptation for AI-generated Text Detection"]}, {"name": "Detection in the wild", "papers": ["Deepfake Videos in the Wild: Analysis and Detection"]}, {"name": "Surrogate model", "papers": ["Ghostbuster: Detecting Text Ghostwritten by Large Language Models"]}]}]}, {"name": "White-box", "subtopics": [{"name": "Full access", "subtopics": [{"name": "Word rank", "papers": ["GLTR: Statistical Detection and Visualization of Generated Text"]}]}, {"name": "Partial access", "subtopics": [{"name": "Contrastive logits feature", "papers": ["Origin Tracing and Detecting of LLMs"]}, {"name": "Logits as waves", "papers": ["SeqXGPT: Sentence-Level AI-Generated Text Detection"]}]}]}]}, {"name": "2_Zero-shot Methods", "subtopics": [{"name": "Black-box", "subtopics": [{"name": "Known Source", "subtopics": [{"name": "Codes detection", "papers": ["Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization"]}, {"name": "Database Retrieval", "papers": ["Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense"]}, {"name": "N-gram divergence", "papers": ["DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text"]}, {"name": "Probability curve", "papers": ["DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature"]}, {"name": "Smaller model as a proxy", "papers": ["Smaller Language Models are Better Black-box Machine-Generated Text Detectors"]}, {"name": "Uncommon n-grams", "papers": ["Artificial Text Detection with Multiple Training Strategies", "On the Coherence of Fake News Articles"]}]}, {"name": "Unknown Source", "subtopics": [{"name": "Intrinsic dimension", "papers": ["Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts"]}]}]}, {"name": "White-box", "subtopics": [{"name": "Full access", "subtopics": [{"name": "Log-Rank ratio", "papers": ["DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text"]}]}, {"name": "Partial access", "subtopics": [{"name": "Recent methods", "subtopics": [{"name": "Bayesian surrogate model", "papers": ["Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model"]}, {"name": "Conditional probability curvature", "papers": ["Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature"]}, {"name": "Uniform information density", "papers": ["GPT-who: An Information Density-based Machine-Generated Text Detector"]}]}, {"name": "Traditional methods", "subtopics": [{"name": "Entropy", "papers": ["Exploring the Role of Visual Content in Fake News Detection"]}, {"name": "Log probability", "papers": ["Release Strategies and the Social Impacts of Language Models"]}, {"name": "Perplexity", "papers": ["Detecting Fake News Using Machine Learning : A Systematic Literature Review"]}]}]}]}]}, {"name": "3_Watermarking Methods", "subtopics": [{"name": "Black-box", "subtopics": [{"name": "Known Source", "subtopics": [{"name": "Latest methods", "papers": ["Watermarking Text Generated by Black-Box Language Models", "Tracing Text Provenance via Context-Aware Lexical Substitution"]}, {"name": "Traditional methods", "papers": ["Natural Language Adversarial Defense through Synonym Encoding", "Robust Multi-bit Natural Language Watermarking through Invariant Features", "Tamperproof IoT with Blockchain", "ALTER: Auxiliary Text Rewriting Tool for Natural Language Generation"]}]}]}, {"name": "White-box", "subtopics": [{"name": "Known Source", "subtopics": [{"name": "Multi-bit watermark", "subtopics": [{"name": "Color-listing", "papers": ["Advancing Beyond Identification: Multi-bit Watermark for Large Language Models"]}, {"name": "Secret key per message", "papers": ["Three Bricks to Consolidate Watermarks for Large Language Models"]}]}, {"name": "Training-based watermark", "subtopics": [{"name": "Logits deviation w semantic embeddings", "papers": ["Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy", "A Semantic Invariant Robust Watermark for Large Language Models"]}, {"name": "Message encoding w reparameterization", "papers": ["REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models"]}]}, {"name": "Training-free watermark", "subtopics": [{"name": "Exponential minimum sampling", "papers": ["What's my role? Modelling responsibility for AI-based safety-critical systems"]}, {"name": "Hashing of blocks", "papers": ["Undetectable Watermarks for Language Models"]}, {"name": "Logits deviation w fixed split", "papers": ["Provable Robust Watermarking for AI-Generated Text"]}, {"name": "Logits deviation w green-red list", "papers": ["A Watermark for Large Language Models"]}, {"name": "Reweight strategy w ciphers", "papers": ["StealthInk: A Multi-bit and Stealthy Watermark for Large Language Models"]}, {"name": "Sampling w randomized number", "papers": ["Robust Distortion-free Watermarks for Language Models"]}, {"name": "Sentence-level w rejection sampling", "papers": ["SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation"]}]}]}]}]}]}]}, "input_paper_count": 50, "input_content": "SYSTEM PROMPT:\nYou are a senior researcher and survey-author with deep experience in structuring\nhigh-quality academic survey papers.\n\nYour task is to organize a set of research papers into a **hierarchical topic tree**\n(bottom-up), given:\n- a survey topic,\n- for each paper: title, abstract, and an structured summary\n  (which may include research problem, motivation, methodology, and findings).\n\nYour goal is NOT just to cluster by surface similarity, but to produce a taxonomy\nthat would be considered **reasonable, informative, and defensible** in a top-tier survey paper.\n\n### Hard Constraints\n1. Output must be **strictly valid JSON**.\n2. Only leaf nodes may contain `\"papers\"`; all internal nodes must contain `\"subtopics\"`.\n3. **Every paper must appear exactly once** in the entire tree.\n4. NO duplicate papers anywhere.\n5. The tree must eventually merge into **one single root node**.\n\n### CLASSIFICATION RULES\n- Group papers by semantic similarity using both title + abstract.\n- Create meaningful names for leaf-level themes.\n\n### ANTI-DUPLICATION PROCEDURE (MANDATORY)\nBefore constructing the tree:\n1. Produce an internal list of all given paper titles.\n2. Assign each paper to exactly one leaf node.\n3. After assignment, verify that:\n   - the number of assigned papers equals the number of input papers,\n   - no paper appears in more than one group.\n\n### Output Format\nUse a JSON structure like this (replace placeholders with actual paper titles) and\nthe output you produce MUST be wrapped inside a fenced code block:\n\n```json\n{\n  \"name\": \"AI Research\",\n  \"subtopics\": [\n    {\n      \"name\": \"NLP\",\n      \"subtopics\": [\n        {\n          \"name\": \"Text Summarization\",\n          \"papers\": [\"<actual paper titles>\"]\n        },\n        {\n          \"name\": \"Machine Translation\",\n          \"papers\": [\"<actual paper titles>\"]\n        }\n      ]\n    }\n  ]\n}```\n\nUSER PROMPT:\nPerform a bottom-up hierarchical clustering of the following 50 papers and produce a JSON research topic tree.\n\nSurvey Topic: A Survey on Detection of LLMs-Generated Content\n\nPaper List:\nPaper 1:\n  Title: RefGPT: Dialogue Generation of GPT, by GPT, and for GPT\n  Abstract: Large Language Models (LLMs) have attained the impressive capability to resolve a wide range of NLP tasks by fine-tuning high-quality instruction data. However, collecting human-written data of high quality, especially multi-turn dialogues, is expensive and unattainable for most people. Though previous studies have used powerful LLMs to generate the dialogues automatically, they all suffer from generating untruthful dialogues because of the model hallucination. Therefore, we propose a method called RefGPT to generate enormous truthful and customized dialogues without worrying about factual errors caused by the model hallucination. RefGPT solves the model hallucination in dialogue generation by restricting the LLMs to leverage the given reference instead of reciting their own knowledge to generate dialogues. Additionally, RefGPT adds detailed controls on every utterance to enable high customization capability, which previous studies have ignored. On the basis of RefGPT, we also propose two high-quality dialogue datasets generated by GPT-4, namely RefGPT-Fact and RefGPT-Code. RefGPT-Fact is a dataset with 100k multi-turn dialogues based on factual knowledge and RefGPT-Code has 76k multi-turn dialogues covering a wide range of coding scenarios. Our code and datasets are released in this https URL.\n  Structured Summary:\n    - Untruthful dialogue generation due to LLM hallucination: Previous methods for automatically generating dialogues using LLMs risk producing untruthful and misleading content because models recite their own knowledge rather than relying on factual information, leading to hallucination.\n    - High cost of collecting human-written multi-turn dialogues: High-quality instruction datasets, particularly multi-turn dialogues in vertical domains, require extensive crowdsourcing with professional knowledge, making the cost unaffordable for most researchers and practitioners.\n    - RefGPT dialogue generation method: RefGPT generates dialogues by providing LLMs with plain text or documents as references, prompting the models to leverage these references instead of reciting their own knowledge, thereby resolving hallucination issues while enabling detailed control over structure, style, and content.\n    - RefGPT-Fact dataset: A dataset containing 100,000 multi-turn dialogues generated by GPT-4 using online encyclopedia websites as references, ensuring authenticity through reliable source material.\n\nPaper 2:\n  Title: Automatic Detection of Generated Text is Easiest when Humans are Fooled\n  Abstract: Recent advancements in neural language modelling make it possible to rapidly generate vast amounts of human-sounding text. The capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research interest, but humans and machines rely on different cues to make their decisions. Here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategies---top-$k$, nucleus sampling, and untruncated random sampling---and show that improvements in decoding methods have primarily optimized for fooling humans. This comes at the expense of introducing statistical abnormalities that make detection easy for automatic systems. We also show that though both human and automatic detector performance improve with longer excerpt length, even multi-sentence excerpts can fool expert human raters over 30% of the time. Our findings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems.\n  Structured Summary:\n    - Difficulty in detecting machine-generated text by both humans and automatic systems: The paper addresses the challenge that as neural language models become more capable of generating human-sounding text, both humans and automatic systems struggle to distinguish machine-generated content from human-written text, with each relying on fundamentally different detection cues.\n    - Public concern over deceptive machine-generated text and its societal impacts: The authors are motivated by the growing societal risks posed by machine-generated text, including its rapid spread, influence on political processes, and erosion of public trust, making detection increasingly imperative as generative technology matures.\n    - Benchmarking three sampling-based decoding strategies with BERT-based discriminators: The authors systematically evaluate three decoding strategies (top-k, nucleus sampling, and untruncated random sampling) by training BERT-based binary classifiers on balanced datasets of machine-generated and human-written text, varying excerpt length and priming conditions.\n    - Human evaluation study with expert raters on multi-length excerpts: The authors conduct human evaluation experiments where expert raters (university students given training examples) progressively view excerpts of increasing length (from 16 to 192 tokens) and classify them as human-written or machine-generated, comparing their performance to automatic discriminators.\n\nPaper 3:\n  Title: Technical Report on the Pangram AI-Generated Text Classifier\n  Abstract: We present Pangram Text, a transformer-based neural network trained to distinguish text written by large language models from text written by humans. Pangram Text outperforms zero-shot methods such as DetectGPT as well as leading commercial AI detection tools with over 38 times lower error rates on a comprehensive benchmark comprised of 10 text domains (student writing, creative writing, scientific writing, books, encyclopedias, news, email, scientific papers, short-form Q&A) and 8 open- and closed-source large language models. We propose a training algorithm, hard negative mining with synthetic mirrors, that enables our classifier to achieve orders of magnitude lower false positive rates on high-data domains such as reviews. Finally, we show that Pangram Text is not biased against nonnative English speakers and generalizes to domains and models unseen during training.\n  Structured Summary:\n    - High false positive rates in existing AI text detection methods: Existing AI detection methods, including commercial tools and zero-shot approaches, suffer from unacceptably high false positive rates that prevent their adoption in production settings such as academic integrity enforcement. Additionally, these methods show bias against nonnative English speakers.\n    - Need for robust detection with extremely low false positive rates: The authors are motivated by the practical requirement for AI detection tools that can be reliably deployed in real-world applications, particularly where false accusations of AI usage would be harmful, such as in academic contexts.\n    - Hard negative mining with synthetic mirrors training algorithm: The authors propose a training algorithm that alternates between training a classifier and identifying false positives (hard negatives) from a large pool of human text, then generating synthetic 'mirror' examples using LLMs that closely match the content and style of these false positives to add to the training set.\n    - Mirror prompting for synthetic data generation: A prompting strategy where LLMs are instructed to generate text that closely resembles human examples in topic, length, style, and tone, preventing the classifier from overfitting to superficial features like document length or topic choice.\n\nPaper 4:\n  Title: GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content\n  Abstract: This paper presents a novel approach for detecting ChatGPT-generated vs. human-written text using language models. To this end, we first collected and released a pre-processed dataset named OpenGPTText, which consists of rephrased content generated using ChatGPT. We then designed, implemented, and trained two different models for text classification, using Robustly Optimized BERT Pretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5), respectively. Our models achieved remarkable results, with an accuracy of over 97% on the test dataset, as evaluated through various metrics. Furthermore, we conducted an interpretability study to showcase our model's ability to extract and differentiate key features between human-written and ChatGPT-generated text. Our findings provide important insights into the effective use of language models to detect generated text.\n  Structured Summary:\n    - Distinguishing ChatGPT-generated text from human-written text: The paper addresses the challenge of detecting text generated by ChatGPT versus human-written content, which has become critical for verifying information authenticity in domains like legal proceedings and journalism. Existing detection methods face significant challenges with newer large language models like ChatGPT.\n    - Need for novel detection algorithms due to ChatGPT emergence: The authors are motivated by the fact that ChatGPT and similar large language models have rendered traditional detection methods inadequate. The widespread availability of ChatGPT makes developing effective detection algorithms an urgent necessity for maintaining information authenticity.\n    - OpenGPTText dataset collection and cleaning: The authors created OpenGPTText, a dataset of 29,395 text samples rephrased by ChatGPT from the OpenWebText corpus. They implemented a cleaning procedure to remove stylistic artifacts like Unicode quotation marks and excessive newlines, producing OpenGPTText-Final to enhance classifier robustness.\n    - RoBERTa-Sentinel model with frozen encoder and MLP classifier: The authors developed RoBERTa-Sentinel by freezing a pretrained RoBERTa model to preserve linguistic knowledge and adding an MLP classifier with GELU activation. The model extracts features from the [CLS] token's hidden state and classifies text through the MLP module.\n\nPaper 5:\n  Title: Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays\n  Abstract: As the use of AI tools by students has become more prevalent, instructors have started using AI detection tools like GPTZero and QuillBot to detect AI written text. However, the reliability of these detectors remains uncertain. In our study, we focused mostly on the success rate of GPTZero, the most-used AI detector, in identifying AI-generated texts based on different lengths of randomly submitted essays: short (40-100 word count), medium (100-350 word count), and long (350-800 word count). We gathered a data set consisting of twenty-eight AI-generated papers and fifty human-written papers. With this randomized essay data, papers were individually plugged into GPTZero and measured for percentage of AI generation and confidence. A vast majority of the AI-generated papers were detected accurately (ranging from 91-100% AI believed generation), while the human generated essays fluctuated; there were a handful of false positives. These findings suggest that although GPTZero is effective at detecting purely AI-generated content, its reliability in distinguishing human-authored texts is limited. Educators should therefore exercise caution when relying solely on AI detection tools.\n  Structured Summary:\n    - Uncertainty in reliability of AI detection tools for student essays: The paper addresses the challenge that while AI detection tools are increasingly used by educators to identify AI-generated student work, their reliability and accuracy in distinguishing between human-written and AI-generated texts remains unclear, particularly regarding false positives.\n    - Ensuring fair assessment of students and proper identification of AI use: The authors are motivated by the need to fairly evaluate students, ensuring that those who use AI are correctly identified while preventing false accusations against students who write their own work, which requires understanding the accuracy of AI detection tools.\n    - Comparative testing of GPTZero across essay length categories: The authors tested GPTZero by categorizing essays into three length groups (short, medium, long) and submitting both human-written and AI-generated essays to measure detection accuracy and false positive rates across different text lengths.\n    - Dataset construction with human and AI-generated essays: The methodology involved collecting fifty human-written student essays and twenty-eight AI-generated essays from ChatGPT, then systematically inputting each into GPTZero to record the tool's assessment of AI generation likelihood.\n\nPaper 6:\n  Title: RADAR: Robust AI-Text Detection via Adversarial Learning\n  Abstract: Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusations of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a robust AI-text detector via adversarial learning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic content to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5-Turbo.\n  Structured Summary:\n    - Vulnerability of AI-text detectors to LLM-based paraphrasing: Current AI-text detection methods fail to maintain reliable performance when AI-generated texts are paraphrased using large language models, creating challenges for distinguishing machine-generated content from human-written text.\n    - Addressing misuse risks and fairness concerns in AI-text detection: The inability to reliably detect AI-generated text creates serious socio-technological risks including fake content generation, academic plagiarism using LLMs, and potential false accusations against innocent human writers, motivating the need for robust detection methods.\n    - RADAR adversarial training framework with paraphraser and detector: RADAR introduces an adversarial learning framework where a paraphraser (tunable LLM) attempts to rewrite AI-text to evade detection, while a detector (separate tunable LLM) learns to identify both original and paraphrased AI-text. The two models update iteratively using feedback from each other.\n    - Clipped PPO with entropy penalty for paraphraser training: The paraphraser is optimized using Proximal Policy Optimization with clipping and an entropy penalty term to balance advantage maximization with generation diversity, treating text generation as a decision-making process with detector feedback as rewards.\n\nPaper 7:\n  Title: CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Data Limitation With Contrastive Learning\n  Abstract: Machine-Generated Text (MGT) detection, a task that discriminates MGT from Human-Written Text (HWT), plays a crucial role in preventing misuse of text generative models, which excel in mimicking human writing style recently. Latest proposed detectors usually take coarse text sequences as input and fine-tune pretrained models with standard cross-entropy loss. However, these methods fail to consider the linguistic structure of texts. Moreover, they lack the ability to handle the low-resource problem which could often happen in practice considering the enormous amount of textual data online. In this paper, we present a coherence-based contrastive learning model named CoCo to detect the possible MGT under low-resource scenario. To exploit the linguistic feature, we encode coherence information in form of graph into text representation. To tackle the challenges of low data resource, we employ a contrastive learning framework and propose an improved contrastive loss for preventing performance degradation brought by simple samples. The experiment results on two public datasets and two self-constructed datasets prove our approach outperforms the state-of-art methods significantly. Also, we surprisingly find that MGTs originated from up-to-date language models could be easier to detect than these from previous models, in our experiments. And we propose some preliminary explanations for this counter-intuitive phenomena. All the codes and datasets are open-sourced.\n  Structured Summary:\n    - MGT detection under low-resource scenarios with linguistic structure ignored: Existing MGT detectors treat texts as flat token sequences and rely on token-level distributions, ignoring high-level linguistic structure. They also suffer from limited annotated data, leading to poor accuracy in low-resource settings where training examples are scarce.\n    - Preventing misuse of text generative models: The authors are motivated by the need to prevent malicious uses of advanced text generation models (e.g., fake news, review forging, spamming), which produce human-like texts that are difficult for humans to distinguish at scale.\n    - Coherence graph construction with entity consistency: The authors construct an entity coherence graph to model sentence-level text structure based on Centering Theory. Entities are nodes, with edges representing entity co-occurrence within sentences (inner relation) and entity repetition across sentences (inter relation), capturing coherence differences between MGT and HWT.\n    - Coherence encoder module with relation-aware GCN and attention LSTM: The authors design a coherence encoder module that uses relation-aware GCN to process the coherence graph separately for each relation type, then aggregates node representations into sentence-level features, and applies attention LSTM to capture sentence-level coherence and generate a coherence-enhanced document representation.\n\nPaper 8:\n  Title: Multiscale Positive-Unlabeled Detection of AI-Generated Texts\n  Abstract: Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may impact the authenticity of texts. Previous works proposed methods to detect these AI-generated texts, including simple ML classifiers, pretrained-model-based zero-shot methods, and finetuned language classification models. However, mainstream detectors always fail on short texts, like SMSes, Tweets, and reviews. In this paper, a Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the difficulty of short-text detection without sacrificing long-texts. Firstly, we acknowledge the human-resemblance property of short machine texts, and rephrase AI text detection as a partial Positive-Unlabeled (PU) problem by regarding these short machine texts as partially ``unlabeled\". Then in this PU context, we propose the length-sensitive Multiscale PU Loss, where a recurrent model in abstraction is used to estimate positive priors of scale-variant corpora. Additionally, we introduce a Text Multiscaling module to enrich training corpora. Experiments show that our MPU method augments detection performance on long AI-generated texts, and significantly improves short-text detection of language model detectors. Language Models trained with MPU could outcompete existing detectors on various short-text and long-text detection benchmarks. The codes are available at this https URL and this https URL.\n  Structured Summary:\n    - Difficulty of detecting short AI-generated texts: Existing AI-generated text detection methods perform poorly on short texts such as SMS messages, tweets, and reviews, despite their effectiveness on longer texts. This creates a significant gap in detection capabilities for fragmented mobile media content.\n    - Threat of fake short texts in mobile media era: The authors are motivated by the vulnerability of smartphone users to LLM-generated fake short texts in daily life contexts. Current detectors lack comprehensive capability to handle both short and long texts, leaving users defenseless against such threats.\n    - Multiscale Positive-Unlabeled (MPU) training framework: The authors propose an MPU framework that treats short machine-generated texts as partially 'unlabeled' rather than clearly labeled as AI. This reframes AI text detection as a partial Positive-Unlabeled problem, enabling better handling of texts across different lengths.\n    - Length-sensitive Multiscale PU Loss: A novel loss function that adjusts the prior probability of a text being positive based on text length. An abstract recurrent model estimates length-variant priors, recognizing that distribution shifts gradually from long to short texts.\n\nPaper 9:\n  Title: LLMDet: A Third Party Large Language Models Generated Text Detection Tool\n  Abstract: Generated texts from large language models (LLMs) are remarkably close to high-quality human-authored text, raising concerns about their potential misuse in spreading false information and academic misconduct. Consequently, there is an urgent need for a highly practical detection tool capable of accurately identifying the source of a given text. However, existing detection tools typically rely on access to LLMs and can only differentiate between machine-generated and human-authored text, failing to meet the requirements of fine-grained tracing, intermediary judgment, and rapid detection. Therefore, we propose LLMDet, a model-specific, secure, efficient, and extendable detection tool, that can source text from specific LLMs, such as GPT-2, OPT, LLaMA, and others. In LLMDet, we record the next-token probabilities of salient n-grams as features to calculate proxy perplexity for each LLM. By jointly analyzing the proxy perplexities of LLMs, we can determine the source of the generated text. Experimental results show that LLMDet yields impressive detection performance while ensuring speed and security, achieving 98.54% precision and x5.0 faster for recognizing human-authored text. Additionally, LLMDet can effortlessly extend its detection capabilities to a new open-source model. We will provide an open-source tool at this https URL.\n  Structured Summary:\n    - Inability to trace text to specific LLMs and lack of secure, efficient detection: Current detection methods cannot identify which specific large language model generated a given text, and they require model access which compromises security and efficiency. They only perform binary classification between human and machine text.\n    - Need for practical detection with specificity, safety, efficiency, and extendibility: The authors are motivated by the need for a detection tool that can trace text to specific models for copyright protection, avoid accessing model parameters for security, operate with low resource requirements for widespread use, and adapt to new emerging models.\n    - Dictionary construction using n-gram next-token probabilities: The method constructs dictionaries offline by sampling generated text from each LLM, extracting frequent n-grams, and recording their next-token probability distributions. These dictionaries serve as prior information for computing proxy perplexity without accessing the models during detection.\n    - Proxy perplexity estimation for text detection: During detection, the tool computes proxy perplexity for input text across multiple LLMs using the pre-built dictionaries, then feeds these perplexity features into a trained classifier to identify the source model. This approach eliminates the need for model access at detection time.\n\nPaper 10:\n  Title: On the Zero-Shot Generalization of Machine-Generated Text Detectors\n  Abstract: The rampant proliferation of large language models, fluent enough to generate text indistinguishable from human-written language, gives unprecedented importance to the detection of machine-generated text. This work is motivated by an important research question: How will the detectors of machine-generated text perform on outputs of a new generator, that the detectors were not trained on? We begin by collecting generation data from a wide range of LLMs, and train neural detectors on data from each generator and test its performance on held-out generators. While none of the detectors can generalize to all generators, we observe a consistent and interesting pattern that the detectors trained on data from a medium-size LLM can zero-shot generalize to the larger version. As a concrete application, we demonstrate that robust detectors can be built on an ensemble of training data from medium-sized models.\n  Structured Summary:\n    - Zero-shot generalization of machine-generated text detectors to unseen generators: The paper addresses the challenge of detecting machine-generated text from new, unseen language models that detectors were not trained on. This is critical because collecting training data from large or proprietary models is difficult, and detectors must cover a rapidly growing range of LLMs in real applications.\n    - Need for detectors that generalize across multiple LLM generators: The authors are motivated by the practical necessity for detectors to work across diverse and evolving LLMs, especially when training data from large or proprietary models is unavailable or expensive to obtain, making zero-shot generalization essential for real-world deployment.\n    - Systematic evaluation of detector generalization across LLM families: The authors collect generation data from 13 popular language models across three domains (news, reviews, knowledge) and train ELECTRA-large binary classifiers on each generator's data. They systematically evaluate cross-generator performance using an Acc-Gap metric to measure generalization ability.\n    - Ensemble training with pruned data from medium-sized models: The authors propose training detectors on mixed data from multiple generators while strategically pruning out large-version models. They demonstrate that detectors trained on medium-sized model data can effectively generalize to larger versions with minimal performance loss.\n\nPaper 11:\n  Title: From Text to Source: Results in Detecting Large Language Model-Generated Content\n  Abstract: The widespread use of Large Language Models (LLMs), celebrated for their ability to generate human-like text, has raised concerns about misinformation and ethical implications. Addressing these concerns necessitates the development of robust methods to detect and attribute text generated by LLMs. This paper investigates \"Cross-Model Detection,\" by evaluating whether a classifier trained to distinguish between source LLM-generated and human-written text can also detect text from a target LLM without further training. The study comprehensively explores various LLM sizes and families, and assesses the impact of conversational fine-tuning techniques, quantization, and watermarking on classifier generalization. The research also explores Model Attribution, encompassing source model identification, model family, and model size classification, in addition to quantization and watermarking detection. Our results reveal several key findings: a clear inverse relationship between classifier effectiveness and model size, with larger LLMs being more challenging to detect, especially when the classifier is trained on data from smaller models. Training on data from similarly sized LLMs can improve detection performance from larger models but may lead to decreased performance when dealing with smaller models. Additionally, model attribution experiments show promising results in identifying source models and model families, highlighting detectable signatures in LLM-generated text, with particularly remarkable outcomes in watermarking detection, while no detectable signatures of quantization were observed. Overall, our study contributes valuable insights into the interplay of model size, family, and training data in LLM detection and attribution.\n  Structured Summary:\n    - Cross-Model Detection of LLM-Generated Text: The paper addresses the challenge of detecting text generated by Large Language Models (LLMs) when the specific source model is unknown. This involves determining whether a classifier trained on one LLM's output can generalize to detect text from different LLMs without additional training.\n    - Real-World Scenarios with Unknown Source Models: The authors are motivated by practical situations where the identity of the text-generating model is unknown, necessitating detection methods that can generalize across different LLMs. This is crucial for maintaining trust in online platforms, content moderation, and responsible AI deployment.\n    - Comprehensive Cross-Model Detection Evaluation: The authors conduct extensive experiments evaluating classifier transferability across 55 models from 10 families, varying in size from 124M to 70B parameters. They assess the impact of model size, family, conversational fine-tuning, quantization, and watermarking on detection performance using a DeBERTaV3-base encoder classifier.\n    - Model Attribution Framework: The authors develop a multi-faceted attribution framework consisting of five subtasks: identifying the exact source model (51 classes), classifying model family (12 classes), determining model size (6 bins), detecting quantization, and detecting watermarking. Each task uses appropriate metrics (AUC for detection, F1 for attribution).\n\nPaper 12:\n  Title: ConDA: Contrastive Domain Adaptation for AI-generated Text Detection\n  Abstract: Large language models (LLMs) are increasingly being used for generating text in a variety of use cases, including journalistic news articles. Given the potential malicious nature in which these LLMs can be used to generate disinformation at scale, it is important to build effective detectors for such AI-generated text. Given the surge in development of new LLMs, acquiring labeled training data for supervised detectors is a bottleneck. However, there might be plenty of unlabeled text data available, without information on which generator it came from. In this work we tackle this data problem, in detecting AI-generated news text, and frame the problem as an unsupervised domain adaptation task. Here the domains are the different text generators, i.e. LLMs, and we assume we have access to only the labeled source data and unlabeled target data. We develop a Contrastive Domain Adaptation framework, called ConDA, that blends standard domain adaptation techniques with the representation power of contrastive learning to learn domain invariant representations that are effective for the final unsupervised detection task. Our experiments demonstrate the effectiveness of our framework, resulting in average performance gains of 31.7% from the best performing baselines, and within 0.8% margin of a fully supervised detector. All our code and data is available at this https URL.\n  Structured Summary:\n    - Label scarcity for AI-generated text detection across multiple LLMs: The paper addresses the challenge of detecting AI-generated news text when labeled training data is unavailable for newer or target language models, while unlabeled text from these generators exists. This data bottleneck prevents building effective supervised detectors for emerging LLMs.\n    - Preventing malicious use of LLMs for disinformation at scale: The authors are motivated by the risk that increasingly convincing language models can be exploited by malicious actors to propagate misinformation, disinformation, propaganda, or spam at scale, especially with the accessibility of public-facing APIs and recent concerns about AI-generated news websites.\n    - ConDA: Contrastive Domain Adaptation framework: ConDA frames AI-generated text detection as an unsupervised domain adaptation problem where different LLMs are treated as different domains. The framework combines contrastive learning (using synonym replacement transformations) with Maximum Mean Discrepancy (MMD) to learn domain-invariant representations from labeled source data and unlabeled target data.\n    - Training objective combining classification, contrastive, and domain discrepancy losses: The training objective integrates source classification losses (cross-entropy on original and perturbed text), contrastive losses for both source and target domains (bringing positive pairs closer while separating negatives), and MMD loss to minimize distribution discrepancy between source and target generators.\n\nPaper 13:\n  Title: Deepfake Videos in the Wild: Analysis and Detection\n  Abstract: AI-manipulated videos, commonly known as deepfakes, are an emerging problem. Recently, researchers in academia and industry have contributed several (self-created) benchmark deepfake datasets, and deepfake detection algorithms. However, little effort has gone towards understanding deepfake videos in the wild, leading to a limited understanding of the real-world applicability of research contributions in this space. Even if detection schemes are shown to perform well on existing datasets, it is unclear how well the methods generalize to real-world deepfakes. To bridge this gap in knowledge, we make the following contributions: First, we collect and present the largest dataset of deepfake videos in the wild, containing 1,869 videos from YouTube and Bilibili, and extract over 4.8M frames of content. Second, we present a comprehensive analysis of the growth patterns, popularity, creators, manipulation strategies, and production methods of deepfake content in the real-world. Third, we systematically evaluate existing defenses using our new dataset, and observe that they are not ready for deployment in the real-world. Fourth, we explore the potential for transfer learning schemes and competition-winning techniques to improve defenses.\n  Structured Summary:\n    - Limited understanding of real-world deepfake videos and detection generalization: Existing deepfake detection research has been conducted with limited knowledge of actual deepfake videos appearing in the wild, creating uncertainty about whether detection methods trained on research datasets can generalize to real-world deepfakes produced by the Internet community.\n    - Need to bridge knowledge gap between research and real-world deepfakes: The authors are motivated to address the disconnect between academic deepfake research and real-world deepfake content, enabling better understanding of whether existing detection schemes are ready for practical deployment against deepfakes found in the wild.\n    - DF-W dataset collection methodology: The authors develop a systematic data collection methodology that searches YouTube, Bilibili, and Reddit using keywords, identifies potential deepfake channels and videos, filters content through manual and automated verification, and downloads videos at highest available resolution to build the DF-W dataset.\n    - Modified video-level detection decision metric: The authors adapt an existing video-level decision metric to handle multiple faces per frame (not all fake) and videos with mixed real and fake frames, computing frame probability as the maximum across all faces and video probability by averaging top percentile frame scores.\n\nPaper 14:\n  Title: Ghostbuster: Detecting Text Ghostwritten by Large Language Models\n  Abstract: We introduce Ghostbuster, a state-of-the-art system for detecting AI-generated text. Our method works by passing documents through a series of weaker language models, running a structured search over possible combinations of their features, and then training a classifier on the selected features to predict whether documents are AI-generated. Crucially, Ghostbuster does not require access to token probabilities from the target model, making it useful for detecting text generated by black-box models or unknown model versions. In conjunction with our model, we release three new datasets of human- and AI-generated text as detection benchmarks in the domains of student essays, creative writing, and news articles. We compare Ghostbuster to a variety of existing detectors, including DetectGPT and GPTZero, as well as a new RoBERTa baseline. Ghostbuster achieves 99.0 F1 when evaluated across domains, which is 5.9 F1 higher than the best preexisting model. It also outperforms all previous approaches in generalization across writing domains (+7.5 F1), prompting strategies (+2.1 F1), and language models (+4.4 F1). We also analyze the robustness of our system to a variety of perturbations and paraphrasing attacks and evaluate its performance on documents written by non-native English speakers.\n  Structured Summary:\n    - Detecting AI-generated text with strong generalization: The paper addresses the challenge of reliably detecting whether text was generated by large language models like ChatGPT, particularly when existing detectors exhibit poor generalization across domains, prompts, and models, and when high false positive rates raise ethical concerns about misclassifying genuine human work.\n    - Need for trustworthy and generalizable AI text detection: The authors are motivated by concerns about authenticity and trustworthiness of text across domains (education, news), the brittleness of existing detectors, and ethical issues arising from false accusations, especially given that non-native English speakers are disproportionately flagged as using AI.\n    - Structured search over probability-based features: Ghostbuster computes token probabilities from multiple weaker language models (unigram, trigram, GPT-3 ada and davinci), then performs a structured search over vector and scalar functions that combine these probabilities into features, and finally trains a linear classifier on the selected features.\n    - Three-stage training procedure: The method first converts documents into token probability vectors using weaker language models, then selects features via structured search over combinations of these probabilities using predefined vector and scalar operations, and finally trains a logistic regression classifier on the selected and handcrafted features.\n\nPaper 15:\n  Title: GLTR: Statistical Detection and Visualization of Generated Text\n  Abstract: The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by and explained to non-experts. We develop GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across common sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54% to 72% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs\n  Structured Summary:\n    - Detection of automatically generated text by non-experts: The paper addresses the challenge of detecting machine-generated text in an era where large language models can produce output indistinguishable from human writing. The problem is particularly acute for non-expert readers who lack tools and training to identify such text.\n    - Preventing malicious use of text generation at scale: The authors are motivated by real-world cases of abuse, including fake political comments and potential for false articles or reviews. They argue that forensic techniques are necessary to detect automatically generated text, and these should be accurate yet accessible to non-experts with minimal setup.\n    - GLTR visual detection tool with statistical tests: GLTR is a visual tool that highlights text passages using three statistical tests: word probability, absolute rank of words in the predicted distribution, and entropy of the predicted distribution. The tool uses detection models like BERT and GPT-2 to compute these metrics and presents them through color-coded overlays and histograms.\n    - Three statistical tests for generation detection: The authors propose three tests based on the assumption that generation systems sample from the head of the distribution. The tests check whether generated words come from the top of the distribution and whether the context produces overly confident predictions with low entropy.\n\nPaper 16:\n  Title: Origin Tracing and Detecting of LLMs\n  Abstract: The extraordinary performance of large language models (LLMs) heightens the importance of detecting whether the context is generated by an AI system. More importantly, while more and more companies and institutions release their LLMs, the origin can be hard to trace. Since LLMs are heading towards the time of AGI, similar to the origin tracing in anthropology, it is of great importance to trace the origin of LLMs. In this paper, we first raise the concern of the origin tracing of LLMs and propose an effective method to trace and detect AI-generated contexts. We introduce a novel algorithm that leverages the contrastive features between LLMs and extracts model-wise features to trace the text origins. Our proposed method works under both white-box and black-box settings therefore can be widely generalized to detect various LLMs.(e.g. can be generalized to detect GPT-3 models without the GPT-3 models). Also, our proposed method requires only limited data compared with the supervised learning methods and can be extended to trace new-coming model origins. We construct extensive experiments to examine whether we can trace the origins of given texts. We provide valuable observations based on the experimental results, such as the difficulty level of AI origin tracing, and the AI origin similarities, and call for ethical concerns of LLM providers. We are releasing all codes and data as a toolkit and benchmark for future AI origin tracing and detecting studies. \\footnote{We are releasing all available resource at \\url{this https URL}.}\n  Structured Summary:\n    - Origin tracing and detection of LLM-generated texts: The paper addresses the challenge of determining whether a text is AI-generated and identifying which specific LLM produced it, as the proliferation of LLMs makes origin tracing increasingly difficult yet critical for preventing abuse and model theft.\n    - Preventing AI abuse and ensuring LLM accountability: The authors are motivated by the need to prevent misuse of LLMs in contexts requiring originality and to enable accountability by tracing texts back to specific service providers, especially as future LLMs may be trained on AI-generated data.\n    - Sniffer: contrastive feature extraction across LLMs: Sniffer extracts model-wise features by computing and aligning token-level perplexities across multiple known LLMs, then crafts heuristic features (percent-of-low-perplexity scores, correlation coefficients) that capture discrepancies between models, enabling origin tracing via a simple linear classifier.\n    - Low-resource linear classifier for origin tracing: After feature extraction, a linear classifier is trained on a small amount of labeled data to map the low-dimensional feature vectors to different LLM origins, enabling both white-box and black-box detection with minimal data requirements.\n\nPaper 17:\n  Title: SeqXGPT: Sentence-Level AI-Generated Text Detection\n  Abstract: Widely applied large language models (LLMs) can generate human-like content, raising concerns about the abuse of LLMs. Therefore, it is important to build strong AI-generated text (AIGT) detectors. Current works only consider document-level AIGT detection, therefore, in this paper, we first introduce a sentence-level detection challenge by synthesizing a dataset that contains documents that are polished with LLMs, that is, the documents contain sentences written by humans and sentences modified by LLMs. Then we propose \\textbf{Seq}uence \\textbf{X} (Check) \\textbf{GPT}, a novel method that utilizes log probability lists from white-box LLMs as features for sentence-level AIGT detection. These features are composed like \\textit{waves} in speech processing and cannot be studied by LLMs. Therefore, we build SeqXGPT based on convolution and self-attention networks. We test it in both sentence and document-level detection challenges. Experimental results show that previous methods struggle in solving sentence-level AIGT detection, while our method not only significantly surpasses baseline methods in both sentence and document-level detection challenges but also exhibits strong generalization capabilities.\n  Structured Summary:\n    - Sentence-level AI-generated text detection challenge: Existing AIGT detection methods focus on determining whether entire documents are AI-generated, but fail to address scenarios where documents contain a mix of human-written and AI-modified sentences. This limitation leads to higher false positive/negative rates and lacks fine-grained analysis capabilities.\n    - Need for fine-grained detection in real-world AI-assisted writing: The motivation stems from practical usage patterns where users employ LLMs to polish or modify portions of documents rather than generating complete texts. Fine-grained sentence-level detection is necessary to accurately identify such mixed-authorship content and reduce misidentification risks.\n    - SeqXGPT framework using log probability features: SeqXGPT extracts word-wise log probability lists from multiple white-box LLMs as foundational features. These temporal wave-like features are processed through convolutional networks to extract local patterns, followed by self-attention layers to capture long-range dependencies for contextualized representations.\n    - Sequence labeling approach for sentence classification: Rather than classifying sentences independently, SeqXGPT treats the entire document as input for sequence labeling, assigning labels to each word. The final sentence category is determined by selecting the most frequently occurring word-level label within that sentence.\n\nPaper 18:\n  Title: Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization\n  Abstract: Text summarization has been a crucial problem in natural language processing (NLP) for several decades. It aims to condense lengthy documents into shorter versions while retaining the most critical information. Various methods have been proposed for text summarization, including extractive and abstractive summarization. The emergence of large language models (LLMs) like GPT3 and ChatGPT has recently created significant interest in using these models for text summarization tasks. Recent studies \\cite{goyal2022news, zhang2023benchmarking} have shown that LLMs-generated news summaries are already on par with humans. However, the performance of LLMs for more practical applications like aspect or query-based summaries is underexplored. To fill this gap, we conducted an evaluation of ChatGPT's performance on four widely used benchmark datasets, encompassing diverse summaries from Reddit posts, news articles, dialogue meetings, and stories. Our experiments reveal that ChatGPT's performance is comparable to traditional fine-tuning methods in terms of Rouge scores. Moreover, we highlight some unique differences between ChatGPT-generated summaries and human references, providing valuable insights into the superpower of ChatGPT for diverse text summarization tasks. Our findings call for new directions in this area, and we plan to conduct further research to systematically examine the characteristics of ChatGPT-generated summaries through extensive human evaluation.\n  Structured Summary:\n    - Limited exploration of LLMs for aspect or query-based summarization: While recent studies show LLMs perform well on generic news summarization, their capabilities for more practical and diverse summarization tasks such as aspect-based or query-based summarization across multiple domains remain largely unexplored.\n    - Need for diverse and nuanced summarization evaluation: The authors are motivated by the fact that aspect- and query-based summarization tasks are more challenging and valuable for real-world applications, requiring deeper document comprehension tailored to specific user interests and needs, unlike generic summarization.\n    - Comprehensive evaluation of ChatGPT on four benchmark datasets: The authors systematically evaluate ChatGPT's zero-shot performance on four diverse benchmark datasets (QMSum, SQuALITY, CovidET, NEWTS) covering Reddit posts, news articles, dialogue meetings, and stories, using Rouge scores as the primary metric.\n    - Domain-specific prompt design for diverse summarization tasks: The authors design and apply task-specific prompts for each dataset, including zero-shot prompts for most tasks and one-shot prompts for CovidET, tailored to handle different input lengths and output requirements across domains.\n\nPaper 19:\n  Title: Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense\n  Abstract: The rise in malicious usage of large language models, such as fake content creation and academic plagiarism, has motivated the development of approaches that identify AI-generated text, including those based on watermarking or outlier detection. However, the robustness of these detection algorithms to paraphrases of AI-generated text remains unclear. To stress test these detectors, we build a 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, condition on surrounding context, and control lexical diversity and content reordering. Using DIPPER to paraphrase text generated by three large language models (including GPT3.5-davinci-003) successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier. For example, DIPPER drops detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of 1%), without appreciably modifying the input semantics.\nTo increase the robustness of AI-generated text detection to paraphrase attacks, we introduce a simple defense that relies on retrieving semantically-similar generations and must be maintained by a language model API provider. Given a candidate text, our algorithm searches a database of sequences previously generated by the API, looking for sequences that match the candidate text within a certain threshold. We empirically verify our defense using a database of 15M generations from a fine-tuned T5-XXL model and find that it can detect 80% to 97% of paraphrased generations across different settings while only classifying 1% of human-written sequences as AI-generated. We open-source our models, code and data.\n  Structured Summary:\n    - Vulnerability of AI-generated text detectors to paraphrase attacks: Existing detectors of AI-generated text (watermarking, GPTZero, DetectGPT, OpenAI's classifier) have not been thoroughly tested against paraphrase attacks, where AI-generated text is rewritten by another model to evade detection while preserving semantics.\n    - Preventing malicious usage of large language models: The authors are motivated by the need to defend against malicious applications of LLMs, including fake news generation and academic dishonesty, which has driven the development of detection methods that must be robust to adversarial attacks.\n    - DIPPER: controllable discourse-level paraphrase generation model: DIPPER is an 11B parameter T5-XXL-based paraphraser trained on paragraph-level paraphrase data from multiple English translations. It supports context-aware paraphrasing, content reordering, and controllable lexical diversity through scalar control knobs at inference time.\n    - Retrieval-based defense for detecting paraphrased AI-generated text: The proposed defense involves an LLM API provider maintaining a database of all generated sequences. A candidate text is compared semantically to this database using IR methods (BM-25) and semantic embeddings (P-SP) to detect paraphrased AI-generated content.\n\nPaper 20:\n  Title: DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text\n  Abstract: Large language models (LLMs) have notably enhanced the fluency and diversity of machine-generated text. However, this progress also presents a significant challenge in detecting the origin of a given text, and current research on detection methods lags behind the rapid evolution of LLMs. Conventional training-based methods have limitations in flexibility, particularly when adapting to new domains, and they often lack explanatory power. To address this gap, we propose a novel training-free detection strategy called Divergent N-Gram Analysis (DNA-GPT). Given a text, we first truncate it in the middle and then use only the preceding portion as input to the LLMs to regenerate the new remaining parts. By analyzing the differences between the original and new remaining parts through N-gram analysis in black-box or probability divergence in white-box, we unveil significant discrepancies between the distribution of machine-generated text and the distribution of human-written text. We conducted extensive experiments on the most advanced LLMs from OpenAI, including text-davinci-003, GPT-3.5-turbo, and GPT-4, as well as open-source models such as GPT-NeoX-20B and LLaMa-13B. Results show that our zero-shot approach exhibits state-of-the-art performance in distinguishing between human and GPT-generated text on four English and one German dataset, outperforming OpenAI's own classifier, which is trained on millions of text. Additionally, our methods provide reasonable explanations and evidence to support our claim, which is a unique feature of explainable detection. Our method is also robust under the revised text attack and can additionally solve model sourcing. Codes are available at this https URL.\n  Structured Summary:\n    - Detecting GPT-generated text in the era of advanced LLMs: The paper addresses the challenge that existing detection methods for AI-generated text cannot keep pace with rapidly evolving large language models like ChatGPT and GPT-4. Training-based detectors lack flexibility, fail to generalize to new domains or models, and provide no explanations for their decisions.\n    - Need for robust, explainable, and training-free detection: The authors are motivated by the urgent need for detection methods that can adapt to continuously updated models without retraining, provide interpretable evidence for decisions (especially important in educational contexts), and work robustly across languages and under text revision attacks.\n    - Divergent N-Gram Analysis (DNA-GPT) framework: DNA-GPT truncates candidate text at a chosen ratio, uses the prefix to regenerate multiple continuations via the target LLM, then compares these regenerations to the original suffix. The method exploits the observation that machine-generated text exhibits lower diversity and higher likelihood overlap than human text when conditioned on the same prefix.\n    - Black-box detection via BScore (n-gram overlap): For black-box scenarios (no token probabilities available), DNA-GPT computes a BScore by measuring weighted n-gram overlap between the original suffix and multiple regenerated suffixes. Higher overlap indicates machine generation, and the method also provides explainable evidence via overlapping n-grams.\n\nPaper 21:\n  Title: DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature\n  Abstract: The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See this https URL for code, data, and other project information.\n  Structured Summary:\n    - Zero-shot machine-generated text detection: The paper addresses the challenge of detecting whether text was generated by a large language model without requiring training data, fine-tuning, or explicit watermarking. Existing zero-shot methods based on raw log probabilities ignore useful structural information in the model's probability function.\n    - Need for reliable LLM-generated text detection: The authors are motivated by concerns that LLMs may be misused in education, journalism, and other contexts where human authorship is important. Humans perform only slightly better than chance at detecting machine-generated text, making automated detection tools necessary.\n    - Probability curvature-based detection criterion: The method identifies that LLM-generated text occupies regions where the log probability function has negative curvature, meaning minor perturbations tend to have lower probability. This property distinguishes machine-generated from human-written text and forms the basis for detection.\n    - DetectGPT algorithm using perturbation discrepancy: DetectGPT generates minor perturbations of a candidate passage using a generic pre-trained model like T5, then compares the original passage's log probability with the average log probability of perturbations. A large positive difference indicates the passage is likely machine-generated.\n\nPaper 22:\n  Title: Smaller Language Models are Better Black-box Machine-Generated Text Detectors\n  Abstract: With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures. To this end, there have been a slew of methods proposed to detect machine-generated text. Most of these methods need access to the logits of the target model or need the ability to sample from the target. One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether the detector and generator were trained on the same data is not critically important to the detection success. For instance the OPT-125M model has an AUC of 0.81 in detecting ChatGPT generations, whereas a larger model from the GPT family, GPTJ-6B, has AUC of 0.45.\n  Structured Summary:\n    - Detecting machine-generated text without knowing the generator model: The paper addresses the challenge of identifying machine-generated text when the generating model is unknown or inaccessible, which is critical for verifying authenticity of news articles, product reviews, and other content in real-world scenarios where the generator model cannot be queried directly.\n    - Preventing abuse of LLMs for unethical purposes: The authors are motivated by the need to prevent malicious uses of language models, such as generating fake news, phishing attempts, and astroturfing, which become increasingly feasible as LLM-generated text achieves human-like fluency and becomes difficult for humans to distinguish.\n    - Cross-detection using surrogate detector models with curvature test: The authors propose using a surrogate detector model (different from the generator) to compute local optimality (curvature) by comparing the likelihood of target sequences to perturbed neighbors. This enables detection of machine-generated text without access to the actual generator model.\n    - Extensive empirical analysis across model families, sizes, and training stages: The methodology involves systematically testing 23 detector models of varying sizes, architectures, and training checkpoints against 15 generator models to identify which characteristics make effective universal detectors for machine-generated text.\n\nPaper 23:\n  Title: Artificial Text Detection with Multiple Training Strategies\n  Abstract: As the deep learning rapidly promote, the artificial texts created by generative models are commonly used in news and social media. However, such models can be abused to generate product reviews, fake news, and even fake political content. The paper proposes a solution for the Russian Artificial Text Detection in the Dialogue shared task 2022 (RuATD 2022) to distinguish which model within the list is used to generate this text. We introduce the DeBERTa pre-trained language model with multiple training strategies for this shared task. Extensive experiments conducted on the RuATD dataset validate the effectiveness of our proposed method. Moreover, our submission ranked second place in the evaluation phase for RuATD 2022 (Multi-Class).\n  Structured Summary:\n    - Detecting which generative model produced artificial text: The paper addresses the challenge of identifying which specific text generation model (from a predefined list) was used to create a given artificial text, as generative models can be misused to produce fake news, reviews, and political content.\n    - Preventing misuse of text generation models for malicious purposes: The authors are motivated by the dual-use nature of text generation technologies, which can be exploited to create harmful content like fake news and propaganda, necessitating effective detection methods to mitigate these security risks.\n    - DeBERTa pre-trained model with multiple training strategies: The authors propose using the DeBERTa family of pre-trained language models (mDeBERTa and DeBERTa) combined with multiple training strategies including adversarial training (FGM), child-tuning, and In-trust loss function for multi-class artificial text detection.\n    - Adversarial training with Fast Gradient Method: The method incorporates adversarial training using Fast Gradient Method (FGM) to add perturbations to inputs during training, making the model more robust by normalizing gradients to prevent excessive disturbance.\n\nPaper 24:\n  Title: On the Coherence of Fake News Articles\n  Abstract: The generation and spread of fake news within new and online media sources is emerging as a phenomenon of high societal significance. Combating them using data-driven analytics has been attracting much recent scholarly interest. In this study, we analyze the textual coherence of fake news articles vis-a-vis legitimate ones. We develop three computational formulations of textual coherence drawing upon the state-of-the-art methods in natural language processing and data science. Two real-world datasets from widely different domains which have fake/legitimate article labellings are then analyzed with respect to textual coherence. We observe apparent differences in textual coherence across fake and legitimate news articles, with fake news articles consistently scoring lower on coherence as compared to legitimate news ones. While the relative coherence shortfall of fake news articles as compared to legitimate ones form the main observation from our study, we analyze several aspects of the differences and outline potential avenues of further inquiry.\n  Structured Summary:\n    - Lack of understanding of textual coherence differences between fake and legitimate news: The paper addresses the gap in understanding how fake news articles differ from legitimate ones in terms of textual coherence, a dimension that has not been systematically studied despite growing interest in fake news detection using textual content.\n    - Need for more generic and robust fake news detection methods: The authors are motivated by the need to develop fake news detection approaches that rely on high-level features rather than easily circumventable low-level lexical patterns, making detection methods more transferable across domains and harder for fake news authors to evade.\n    - Word embedding-based coherence scoring using average word2vec: The first method represents each sentence as the average of pre-trained word2vec vectors for its constituent words, then computes document coherence as the mean pairwise cosine similarity between all sentence representations.\n    - Explicit Semantic Analysis (ESA) based coherence scoring: The second method uses ESA to represent sentences as vectors in Wikipedia concept space by averaging word-level ESA vectors, then computes coherence as mean pairwise similarity between sentence representations.\n\nPaper 25:\n  Title: Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts\n  Abstract: Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over different text domains and varying proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant for human-written texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings for a given text sample. We show that the average intrinsic dimensionality of fluent texts in a natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\\approx 1.5$ lower, with a clear statistical separation between human-generated and AI-generated distributions. This property allows us to build a score-based artificial text detector. The proposed detector's accuracy is stable over text domains, generator models, and human writer proficiency levels, outperforming SOTA detectors in model-agnostic and cross-domain scenarios by a significant margin.\n  Structured Summary:\n    - Difficulty distinguishing AI-generated from human-written texts: Modern large language models produce increasingly human-like text, making it challenging to detect AI-generated content. Existing detection methods fail to generalize across different generation models, text domains, and are vulnerable to adversarial attacks.\n    - Need for robust domain-agnostic AI text detection: The authors are motivated by the need for detection methods that work reliably across different text domains, languages, and generation models without requiring access to the generator itself, while avoiding bias against non-native speakers.\n    - Persistent homology dimension estimator for text intrinsic dimensionality: The authors propose using persistent homology dimension (PHD) to estimate the intrinsic dimensionality of text embeddings. They develop an efficient algorithm based on minimal spanning trees and growth rate analysis to compute PHD for individual text samples.\n    - Score-based artificial text detector using intrinsic dimension: The authors construct a simple single-feature classifier for artificial text detection using PHD as the feature, training a logistic regression on datasets of real and generated texts. The method treats each text sample as a separate manifold and estimates its dimension from contextualized token embeddings.\n\nPaper 26:\n  Title: DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text\n  Abstract: With the rapid progress of large language models (LLMs) and the huge amount of text they generated, it becomes more and more impractical to manually distinguish whether a text is machine-generated. Given the growing use of LLMs in social media and education, it prompts us to develop methods to detect machine-generated text, preventing malicious usage such as plagiarism, misinformation, and propaganda. Previous work has studied several zero-shot methods, which require no training data. These methods achieve good performance, but there is still a lot of room for improvement. In this paper, we introduce two novel zero-shot methods for detecting machine-generated text by leveraging the log rank information. One is called DetectLLM-LRR, which is fast and efficient, and the other is called DetectLLM-NPR, which is more accurate, but slower due to the need for perturbations. Our experiments on three datasets and seven language models show that our proposed methods improve over the state of the art by 3.9 and 1.75 AUROC points absolute. Moreover, DetectLLM-NPR needs fewer perturbations than previous work to achieve the same level of performance, which makes it more practical for real-world use. We also investigate the efficiency--performance trade-off based on users preference on these two measures and we provide intuition for using them in practice effectively. We release the data and the code of both methods in this https URL\n  Structured Summary:\n    - Distinguishing machine-generated from human-written text: The paper addresses the challenge of automatically detecting machine-generated text at scale, as manual identification is impractical given the proliferation of LLMs and the volume of text they produce.\n    - Preventing malicious use of LLMs: The authors are motivated by the need to mitigate negative impacts of LLMs, including plagiarism in education, spread of misinformation, and manipulation of public opinion, which pose serious societal risks.\n    - DetectLLM-LRR using Log-Likelihood Log-Rank Ratio: DetectLLM-LRR is a perturbation-free zero-shot method that combines log-likelihood and log-rank statistics in a ratio to distinguish machine-generated from human-written text, offering efficiency without requiring text perturbations.\n    - DetectLLM-NPR using Normalized Perturbed log-Rank: DetectLLM-NPR is a perturbation-based zero-shot method that exploits the observation that machine-generated texts are more sensitive to minor rewrites, using normalized perturbed log-rank scores to achieve higher detection accuracy.\n\nPaper 27:\n  Title: Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model\n  Abstract: The detection of machine-generated text, especially from large language models (LLMs), is crucial in preventing serious social problems resulting from their misuse. Some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance. Although the recent DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires querying the source LLM with hundreds of its perturbations. This paper aims to bridge this gap. Concretely, we propose to incorporate a Bayesian surrogate model, which allows us to select typical samples based on Bayesian uncertainty and interpolate scores from typical samples to other samples, to improve query efficiency. Empirical results demonstrate that our method significantly outperforms existing approaches under a low query budget. Notably, when detecting the text generated by LLaMA family models, our method with just 2 or 3 queries can outperform DetectGPT with 200 queries.\n  Structured Summary:\n    - Inefficiency of probability curvature-based LLM-generated text detection: The paper addresses the computational inefficiency of DetectGPT, which requires hundreds of queries to the source LLM to estimate local probability curvature for detecting a single text passage. This level of expense is impractical for large models like LLaMA, ChatGPT, and GPT-4.\n    - Practical deployment of LLM-generated text detection: The authors are motivated by the need to prevent serious social problems from LLM misuse, such as fake news creation and academic cheating. Existing detection methods either lack generalization or suffer from prohibitive computational costs, making practical deployment challenging.\n    - Bayesian surrogate model with Gaussian process: The authors propose using a Gaussian process as a surrogate model to map text samples to LLM probabilities. The GP model provides non-parametric flexibility, resistance to overfitting in low-data regimes, and Bayesian uncertainty estimates that guide the selection of typical samples for efficient curvature estimation.\n    - Sequential typical sample selection via Bayesian uncertainty: The method performs sequential selection of typical samples by iteratively choosing the sample with the highest predictive uncertainty from the current GP model, querying the source LLM for its score, and updating the GP. This uncertainty-based selection strategy is inspired by active learning approaches.\n\nPaper 28:\n  Title: Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature\n  Abstract: Large language models (LLMs) have shown the ability to produce fluent and cogent content, presenting both productivity opportunities and societal risks. To build trustworthy AI systems, it is imperative to distinguish between machine-generated and human-authored content. The leading zero-shot detector, DetectGPT, showcases commendable performance but is marred by its intensive computational costs. In this paper, we introduce the concept of conditional probability curvature to elucidate discrepancies in word choices between LLMs and humans within a given context. Utilizing this curvature as a foundational metric, we present **Fast-DetectGPT**, an optimized zero-shot detector, which substitutes DetectGPT's perturbation step with a more efficient sampling step. Our evaluations on various datasets, source models, and test conditions indicate that Fast-DetectGPT not only surpasses DetectGPT by a relative around 75% in both the white-box and black-box settings but also accelerates the detection process by a factor of 340, as detailed in Table 1. See \\url{this https URL} for code, data, and results.\n  Structured Summary:\n    - High computational cost of zero-shot machine-generated text detection: Existing zero-shot detectors like DetectGPT achieve good detection accuracy but require around one hundred model calls to generate perturbation texts, leading to prohibitive computational costs that hinder practical deployment.\n    - Need for trustworthy AI systems to distinguish machine-generated content: The ability of large language models to produce fluent content presents both productivity opportunities and societal risks such as fake news, malicious reviews, and plagiarism, making reliable detection methods essential for trustworthy AI.\n    - Conditional probability curvature for text detection: The authors propose using conditional probability curvature as a detection metric, based on the hypothesis that machines and humans select different words given a context. This curvature is computed by comparing the conditional probability of the original text against alternative token choices.\n    - Fast-DetectGPT detection algorithm: Fast-DetectGPT replaces DetectGPT's costly perturbation-based approach with conditional independent sampling of alternative tokens, requiring only a single model forward pass instead of multiple model calls, thereby achieving significant speedup while improving accuracy.\n\nPaper 29:\n  Title: GPT-who: An Information Density-based Machine-Generated Text Detector\n  Abstract: The Uniform Information Density (UID) principle posits that humans prefer to spread information evenly during language production. We examine if this UID principle can help capture differences between Large Language Models (LLMs)-generated and human-generated texts. We propose GPT-who, the first psycholinguistically-inspired domain-agnostic statistical detector. This detector employs UID-based features to model the unique statistical signature of each LLM and human author for accurate detection. We evaluate our method using 4 large-scale benchmark datasets and find that GPT-who outperforms state-of-the-art detectors (both statistical- & non-statistical) such as GLTR, GPTZero, DetectGPT, OpenAI detector, and ZeroGPT by over $20$% across domains. In addition to better performance, it is computationally inexpensive and utilizes an interpretable representation of text articles. We find that GPT-who can distinguish texts generated by very sophisticated LLMs, even when the overlying text is indiscernible. UID-based measures for all datasets and code are available at this https URL.\n  Structured Summary:\n    - Detecting machine-generated text from sophisticated LLMs: The paper addresses the challenge of distinguishing texts generated by advanced Large Language Models from human-written texts, particularly as LLMs become increasingly capable of producing human-like quality text that can be misused for generating misinformation, plagiarism, and other harmful content.\n    - Mitigating risks of LLM misuse through interpretable detection: The authors are motivated by the need to prevent malicious uses of LLMs, including generation of toxic content, misinformation, and terrorism recruitment. They emphasize the importance of developing interpretable and computationally efficient detection methods that can identify which specific LLMs generated a text, which is crucial for future policymaking.\n    - GPT-who: UID-based statistical detector: The proposed method extracts Uniform Information Density features from text by calculating token surprisals using an off-the-shelf language model (GPT2-XL). These features include mean surprisal, variance of surprisal, local differences in surprisal, and maximum/minimum UID spans. A simple logistic regression classifier then learns to distinguish authors based on these interpretable features without requiring LLM fine-tuning.\n    - Maximum and minimum UID span features: The authors introduce novel features that identify the most and least uniform consecutive token spans (using a sliding window of 20 tokens) within an article. These extremity spans capture the full range of surprisal fluctuations and provide additional discriminative power for authorship detection.\n\nPaper 30:\n  Title: Exploring the Role of Visual Content in Fake News Detection\n  Abstract: The increasing popularity of social media promotes the proliferation of fake news, which has caused significant negative societal effects. Therefore, fake news detection on social media has recently become an emerging research area of great concern. With the development of multimedia technology, fake news attempts to utilize multimedia content with images or videos to attract and mislead consumers for rapid dissemination, which makes visual content an important part of fake news. Despite the importance of visual content, our understanding of the role of visual content in fake news detection is still limited. This chapter presents a comprehensive review of the visual content in fake news, including the basic concepts, effective visual features, representative detection methods and challenging issues of multimedia fake news detection. This chapter can help readers to understand the role of visual content in fake news detection, and effectively utilize visual content to assist in detecting multimedia fake news.\n  Structured Summary:\n    - Limited understanding of visual content's role in fake news detection: The paper addresses the gap in understanding how visual content (images and videos) contributes to detecting fake news on social media, despite its recognized importance in fake news propagation and credibility.\n    - Proliferation of multimedia fake news on social media: The authors are motivated by the increasing use of multimedia content in fake news to attract attention and mislead consumers, causing significant negative societal effects in political, economic, and social domains.\n    - Comprehensive review of visual features and detection methods: The authors systematically review four types of visual features (forensics, semantic, statistical, and context features) and categorize existing detection approaches into content-based and knowledge-based methods for multimedia fake news detection.\n    - Visual content provides multiple cues for fake news detection: The paper concludes that visual content offers diverse signals for detecting fake news through forensics traces, semantic characteristics, statistical patterns, and contextual information, which can be effectively combined for detection.\n\nPaper 31:\n  Title: Release Strategies and the Social Impacts of Language Models\n  Abstract: Large language models have a range of beneficial uses: they can assist in prose, poetry, and programming; analyze dataset biases; and more. However, their flexibility and generative capabilities also raise misuse concerns. This report discusses OpenAI's work related to the release of its GPT-2 language model. It discusses staged release, which allows time between model releases to conduct risk and benefit analyses as model sizes increased. It also discusses ongoing partnership-based research and provides recommendations for better coordination and responsible publication in AI.\n  Structured Summary:\n    - Balancing benefits and risks of releasing powerful language models: The paper addresses the challenge of how to responsibly release increasingly capable language models that offer significant benefits while also posing risks of misuse, requiring careful consideration of release strategies and social impacts.\n    - Need for responsible AI development and deployment practices: The authors are motivated by the need to establish better norms and practices for AI publication and release, particularly as language models become more capable, to ensure responsible development while enabling beneficial applications.\n    - Staged release strategy: The authors implemented a staged release approach where they progressively released larger and more capable versions of their language model over time, allowing for monitoring of impacts and gathering feedback before releasing the most capable version.\n    - Partnership-based access model: The authors established partnerships with external organizations and researchers to explore applications, gather diverse perspectives on impacts, and inform responsible deployment decisions through collaborative engagement.\n\nPaper 32:\n  Title: Detecting Fake News Using Machine Learning : A Systematic Literature Review\n  Abstract: Internet is one of the important inventions and a large number of persons are its users. These persons use this for different purposes. There are different social media platforms that are accessible to these users. Any user can make a post or spread the news through the online platforms. These platforms do not verify the users or their posts. So some of the users try to spread fake news through these platforms. These news can be propaganda against an individual, society, organization or political party. A human being is unable to detect all these fake news. So there is a need for machine learning classifiers that can detect these fake news automatically. Use of machine learning classifiers for detecting fake news is described in this systematic literature review.\n  Structured Summary:\n    - Difficulty in detecting fake news spread through social media platforms: The paper addresses the challenge of detecting fake news on social media platforms, where unverified users can easily spread false information that harms individuals, organizations, or political parties. Human verification is insufficient, necessitating automated detection methods.\n    - Need to prevent harm from fake news and protect reputations: The authors are motivated by the harmful impact of fake news on individuals, organizations, and political entities. Fake news can mislead public opinion, damage reputations, and influence critical decisions such as elections, making automated detection essential.\n    - Systematic literature review of machine learning classifiers for fake news detection: The authors conduct a systematic literature review to examine how various supervised machine learning classifiers (e.g., SVM, Naïve Bayes, Logistic Regression, Neural Networks) are applied to detect fake news. They analyze training methods, feature extraction models (TF-IDF, N-Gram, Bag of Words), and classifier performance.\n    - Machine learning classifiers can automatically detect fake news after proper training: The paper concludes that supervised machine learning classifiers, when properly trained with labeled datasets, can effectively and automatically detect fake news on social media platforms, addressing the challenge of manual verification.\n\nPaper 33:\n  Title: Watermarking Text Generated by Black-Box Language Models\n  Abstract: LLMs now exhibit human-like skills in various fields, leading to worries about misuse. Thus, detecting generated text is crucial. However, passive detection methods are stuck in domain specificity and limited adversarial robustness. To achieve reliable detection, a watermark-based method was proposed for white-box LLMs, allowing them to embed watermarks during text generation. The method involves randomly dividing the model vocabulary to obtain a special list and adjusting the probability distribution to promote the selection of words in the list. A detection algorithm aware of the list can identify the watermarked text. However, this method is not applicable in many real-world scenarios where only black-box language models are available. For instance, third-parties that develop API-based vertical applications cannot watermark text themselves because API providers only supply generated text and withhold probability distributions to shield their commercial interests. To allow third-parties to autonomously inject watermarks into generated text, we develop a watermarking framework for black-box language model usage scenarios. Specifically, we first define a binary encoding function to compute a random binary encoding corresponding to a word. The encodings computed for non-watermarked text conform to a Bernoulli distribution, wherein the probability of a word representing bit-1 being approximately 0.5. To inject a watermark, we alter the distribution by selectively replacing words representing bit-0 with context-based synonyms that represent bit-1. A statistical test is then used to identify the watermark. Experiments demonstrate the effectiveness of our method on both Chinese and English datasets. Furthermore, results under re-translation, polishing, word deletion, and synonym substitution attacks reveal that it is arduous to remove the watermark without compromising the original semantics.\n  Structured Summary:\n    - Watermarking text from black-box language models: Existing watermark-based detection methods require white-box access to language models (i.e., access to output probability distributions), making them inapplicable when only black-box API access is available. Third-party developers using API services cannot autonomously inject watermarks into generated text.\n    - Enabling third-party watermarking for black-box LLM services: The authors are motivated by the need to enable third-party application developers who use black-box language model APIs to independently watermark and authenticate their generated content, especially as policies require labeling generated content for application approval.\n    - Binary encoding function for watermark representation: The method constructs a binary encoding function that computes a random binary value (bit-0 or bit-1) for each word based on the hash of the word and its preceding word. In non-watermarked text, these encodings follow a Bernoulli distribution with equal probabilities.\n    - Context-based synonym substitution for watermark injection: The watermark is injected by selectively replacing words representing bit-0 with context-based synonyms representing bit-1, using BERT to generate synonyms and employing sentence-level and word-level similarity assessments to maintain semantic quality.\n\nPaper 34:\n  Title: Tracing Text Provenance via Context-Aware Lexical Substitution\n  Abstract: Text content created by humans or language models is often stolen or misused by adversaries. Tracing text provenance can help claim the ownership of text content or identify the malicious users who distribute misleading content like machine-generated fake news. There have been some attempts to achieve this, mainly based on watermarking techniques. Specifically, traditional text watermarking methods embed watermarks by slightly altering text format like line spacing and font, which, however, are fragile to cross-media transmissions like OCR. Considering this, natural language watermarking methods represent watermarks by replacing words in original sentences with synonyms from handcrafted lexical resources (e.g., WordNet), but they do not consider the substitution's impact on the overall sentence's meaning. Recently, a transformer-based network was proposed to embed watermarks by modifying the unobtrusive words (e.g., function words), which also impair the sentence's logical and semantic coherence. Besides, one well-trained network fails on other different types of text content. To address the limitations mentioned above, we propose a natural language watermarking scheme based on context-aware lexical substitution (LS). Specifically, we employ BERT to suggest LS candidates by inferring the semantic relatedness between the candidates and the original sentence. Based on this, a selection strategy in terms of synchronicity and substitutability is further designed to test whether a word is exactly suitable for carrying the watermark signal. Extensive experiments demonstrate that, under both objective and subjective metrics, our watermarking scheme can well preserve the semantic integrity of original sentences and has a better transferability than existing methods. Besides, the proposed LS approach outperforms the state-of-the-art approach on the Stanford Word Substitution Benchmark.\n  Structured Summary:\n    - Limitations of existing text watermarking methods: Existing text watermarking approaches suffer from fragility to cross-media transmissions (format-based methods), failure to preserve overall sentence meaning (WordNet-based synonym substitution), impairment of logical and semantic coherence (transformer-based methods modifying function words), and poor transferability across different text types.\n    - Tracing text provenance to prevent misuse: The authors are motivated by the need to protect text content from theft and misuse, including claiming ownership of confidential documents and identifying distributors of misleading machine-generated content such as fake news, which can cause significant losses to individuals and society.\n    - Context-aware lexical substitution using BERT: The authors propose a lexical substitution approach that uses BERT's masked language modeling to generate candidate words and a separate BERT model to measure semantic relatedness between candidates and the original sentence. This ensures substitutions preserve overall sentence meaning by considering context.\n    - Synchronicity and substitutability tests: To address the challenge that context-aware substitution generates different candidates in original versus watermarked sentences, the authors design synchronicity tests (ensuring identical candidate sets at embedding and extraction) and substitutability tests (verifying words are suitable for watermark signals) to enable reliable message encoding and decoding.\n\nPaper 35:\n  Title: Natural Language Adversarial Defense through Synonym Encoding\n  Abstract: In the area of natural language processing, deep learning models are recently known to be vulnerable to various types of adversarial perturbations, but relatively few works are done on the defense side. Especially, there exists few effective defense method against the successful synonym substitution based attacks that preserve the syntactic structure and semantic information of the original text while fooling the deep learning models. We contribute in this direction and propose a novel adversarial defense method called Synonym Encoding Method (SEM). Specifically, SEM inserts an encoder before the input layer of the target model to map each cluster of synonyms to a unique encoding and trains the model to eliminate possible adversarial perturbations without modifying the network architecture or adding extra data. Extensive experiments demonstrate that SEM can effectively defend the current synonym substitution based attacks and block the transferability of adversarial examples. SEM is also easy and efficient to scale to large models and big datasets.\n  Structured Summary:\n    - Vulnerability of NLP models to synonym substitution attacks: Deep learning models in NLP are vulnerable to adversarial attacks, particularly synonym substitution attacks that preserve syntactic structure and semantics while fooling models. Existing defense methods are insufficient, with adversarial training requiring massive examples and certified defenses like IBP being computationally expensive and hard to scale.\n    - Need for scalable defense preserving benign accuracy: Existing certified defense methods face challenges in scaling to large models and datasets due to high computational complexity, and they significantly reduce accuracy on benign data. There is a need for an efficient defense method that maintains both robustness and accuracy without requiring extra data or architectural modifications.\n    - Synonym Encoding Method (SEM): SEM constructs an encoder that maps synonym clusters to unique encodings by clustering synonyms based on Euclidean distance in embedding space. The encoder is inserted before the input layer without modifying the model architecture, forcing all synonymous neighbors of an input to share the same code, thereby smoothing the classification boundary.\n    - Synonym clustering algorithm based on word frequency: The method iterates through the dictionary in descending word frequency order, assigning each word either the code of its closest encoded synonym or itself as a new code, then propagating this code to non-encoded synonyms. This ensures high-frequency words serve as cluster representatives.\n\nPaper 36:\n  Title: Robust Multi-bit Natural Language Watermarking through Invariant Features\n  Abstract: Recent years have witnessed a proliferation of valuable original natural language contents found in subscription-based media outlets, web novel platforms, and outputs of large language models. However, these contents are susceptible to illegal piracy and potential misuse without proper security measures. This calls for a secure watermarking system to guarantee copyright protection through leakage tracing or ownership identification. To effectively combat piracy and protect copyrights, a multi-bit watermarking framework should be able to embed adequate bits of information and extract the watermarks in a robust manner despite possible corruption. In this work, we explore ways to advance both payload and robustness by following a well-known proposition from image watermarking and identify features in natural language that are invariant to minor corruption. Through a systematic analysis of the possible sources of errors, we further propose a corruption-resistant infill model. Our full method improves upon the previous work on robustness by +16.8% point on average on four datasets, three corruption types, and two corruption ratios. Code available at this https URL.\n  Structured Summary:\n    - Lack of robust multi-bit watermarking for natural language: Existing natural language watermarking methods either suffer from high error rates during message extraction when embedding more bits or do not consider robustness under corruption, leading to low accuracy when watermarked texts are modified through deletion, insertion, or substitution attacks.\n    - Protection of valuable natural language content from piracy: The proliferation of valuable text content in subscription media, web novels, and language model outputs creates a need for copyright protection through watermarking that can trace leakage points and identify ownership, especially given concerns about illegal distribution and potential misuse of AI-generated content.\n    - Invariant feature-based watermark embedding framework: The authors propose a two-phase framework where Phase 1 identifies invariant features (semantic keywords via NER and YAKE, or syntactic dependencies via parsing) to determine mask positions, and Phase 2 uses an infill model to generate valid watermarked texts by filling these masks with top-k candidates sorted alphabetically.\n    - Corruption-resistant infill model: A finetuned BERT-based infill model trained to output consistent word distributions when given original and corrupted masked sequences by minimizing reverse KL divergence between their predictions, using a masking strategy aligned with the invariant features rather than random masking.\n\nPaper 37:\n  Title: Tamperproof IoT with Blockchain\n  Abstract: We investigate the tamper-resistant property of Blockchain and its effectiveness for IoT systems. In particular, we implemented an IoT testbed, and built a Blockchain into the testbed. A number of tamper-resistance experiments were conducted and analyzed to corroborate the process of block validation in Blockchain. Our analysis and experimental results demonstrate the tamper-resistant capability of Blockchain in securing trust in IoT systems. The demonstration video is provided at [1].\n  Structured Summary:\n    - Inadequate data security and trust limiting IoT adoption: The paper addresses the challenge that insufficient security and trust mechanisms are preventing widespread adoption of Internet of Things systems, particularly regarding data integrity and tamper protection.\n    - Blockchain's tamper-resistance for ensuring IoT trust: The authors are motivated by the potential of Blockchain's native tamper-resistant properties to address trust issues in IoT systems, enabling secure data collection and sharing without requiring traditional third-party arbitration.\n    - IoT testbed with Ethereum-based Blockchain implementation: The authors constructed a physical testbed combining IoT devices (Raspberry Pi with temperature sensors) and mining workstations, implementing an Ethereum-based Blockchain to validate tamper-resistance through experimental scenarios.\n    - Tamper-resistance experiments on full and light nodes: The authors conducted systematic experiments simulating various hacking scenarios on different node types (mining nodes, non-mining nodes, light nodes) to analyze block validation processes and chain reorganization mechanisms.\n\nPaper 38:\n  Title: ALTER: Auxiliary Text Rewriting Tool for Natural Language Generation\n  Abstract: In this paper, we describe ALTER, an auxiliary text rewriting tool that facilitates the rewriting process for natural language generation tasks, such as paraphrasing, text simplification, fairness-aware text rewriting, and text style transfer. Our tool is characterized by two features, i) recording of word-level revision histories and ii) flexible auxiliary edit support and feedback to annotators. The text rewriting assist and traceable rewriting history are potentially beneficial to the future research of natural language generation.\n  Structured Summary:\n    - Lack of gold references and standard evaluation for generative text editing tasks: The paper identifies that generative modeling of editing text with control attributes (GMETCA) suffers from inconsistent evaluation methods and insufficient human-generated gold references, which makes system comparison difficult and hinders automated error analysis and reproducibility of human-like editing.\n    - Need for task-specific feedback and traceable revision histories in text rewriting: The authors are motivated by the observation that annotators often fail to recognize subtle textual patterns associated with attributes (e.g., gender indicators), and that task-specific feedback can significantly improve both user engagement and annotation quality in text rewriting tasks.\n    - ALTER tool with auxiliary edit support and instant feedback: ALTER is a web-based text rewriting tool that provides annotators with instant task-specific feedback through multiple evaluation models (perplexity, word mover distance, edit distance, and classification scores) and supports flexible auxiliary editing modes including word-level operations and recommendations.\n    - Word-level revision history recording with edit operations: The tool records detailed word-level revision histories by decomposing text transformations into sequences of atomic edit operations (insertions, deletions, replacements, reordering), enabling analysis of human editing behaviors and providing supervision signals for generative models.\n\nPaper 39:\n  Title: Advancing Beyond Identification: Multi-bit Watermark for Large Language Models\n  Abstract: We show the viability of tackling misuses of large language models beyond the identification of machine-generated text. While existing zero-bit watermark methods focus on detection only, some malicious misuses demand tracing the adversary user for counteracting them. To address this, we propose Multi-bit Watermark via Position Allocation, embedding traceable multi-bit information during language model generation. Through allocating tokens onto different parts of the messages, we embed longer messages in high corruption settings without added latency. By independently embedding sub-units of messages, the proposed method outperforms the existing works in terms of robustness and latency. Leveraging the benefits of zero-bit watermarking, our method enables robust extraction of the watermark without any model access, embedding and extraction of long messages ($\\geq$ 32-bit) without finetuning, and maintaining text quality, while allowing zero-bit detection all at the same time. Code is released here: this https URL\n  Structured Summary:\n    - Tracing adversary users beyond machine-text identification: Existing zero-bit watermarking methods only detect machine-generated text but cannot trace the specific user responsible for malicious content generation. For high-stakes misuses like disinformation campaigns, identifying the adversary user is essential for accountability and counteraction.\n    - Enabling accountability for malicious LLM misuse: The authors are motivated by the need to hold adversary users accountable for harmful content generated via language model APIs. Multi-bit watermarking enables tracing malicious users without storing expensive query logs, allowing API providers to ban users and collaborate with law enforcement while preserving ordinary users' privacy.\n    - Multi-bit watermark via Position Allocation (MPAC): MPAC allocates each generated token pseudo-randomly to a sub-unit (position) of the multi-bit message. The message content at that position determines which subset of the vocabulary (colorlist) is biased during generation. This independent encoding of message positions enables embedding longer messages without latency overhead and improves robustness under corruption.\n    - Colorlisting for increased load capacity: Instead of using only a binary greenlist, MPAC partitions the vocabulary into multiple colored lists (determined by greenlist proportion gamma). This allows encoding multiple states per token, increasing the message capacity by converting binary messages to higher radix representations.\n\nPaper 40:\n  Title: Three Bricks to Consolidate Watermarks for Large Language Models\n  Abstract: The task of discerning between generated and natural texts is increasingly challenging. In this context, watermarking emerges as a promising technique for ascribing generated text to a specific model. It alters the sampling generation process so as to leave an invisible trace in the generated output, facilitating later detection. This research consolidates watermarks for large language models based on three theoretical and empirical considerations. First, we introduce new statistical tests that offer robust theoretical guarantees which remain valid even at low false-positive rates (less than 10$^{\\text{-6}}$). Second, we compare the effectiveness of watermarks using classical benchmarks in the field of natural language processing, gaining insights into their real-world applicability. Third, we develop advanced detection schemes for scenarios where access to the LLM is available, as well as multi-bit watermarking.\n  Structured Summary:\n    - Unreliable false positive rates in LLM watermark detection: Existing watermark detection methods for LLMs rely on statistical tests whose theoretical guarantees do not hold in practice, particularly at low false positive rates. The authors find that empirical FPRs are much higher than theoretical predictions, especially when tested at scale with more than 1k negative examples.\n    - Preventing misuse of LLMs through reliable watermarking: The authors are motivated by the need to prevent harmful misuse of LLMs, including generating fake news, impersonation, scams, and academic dishonesty. Watermarking offers a technical means to enforce fair and responsible usage through regulations, especially since passive forensics cannot reliably distinguish generated from natural text.\n    - Non-asymptotic statistical tests for watermark detection: The authors introduce new statistical tests based on binomial and gamma distributions that provide rigorous theoretical guarantees for false positive rates. These tests replace the flawed Z-tests used in prior work and are validated to maintain accurate FPR control even at extremely low rates below 10^-6.\n    - Rectified scoring strategy for watermark detection: To address the independence assumption violations caused by repeated token sequences, the authors propose a revised scoring strategy that only counts tokens whose watermark context plus current token tuple has not been previously encountered during detection. This heuristic mitigates the impact of repetitions on false positive rates.\n\nPaper 41:\n  Title: Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy\n  Abstract: To mitigate potential risks associated with language models, recent AI detection research proposes incorporating watermarks into machine-generated text through random vocabulary restrictions and utilizing this information for detection. While these watermarks only induce a slight deterioration in perplexity, our empirical investigation reveals a significant detriment to the performance of conditional text generation. To address this issue, we introduce a simple yet effective semantic-aware watermarking algorithm that considers the characteristics of conditional text generation and the input context. Experimental results demonstrate that our proposed method yields substantial improvements across various text generation models, including BART and Flan-T5, in tasks such as summarization and data-to-text generation while maintaining detection ability.\n  Structured Summary:\n    - Performance degradation when applying LM watermarks to conditional text generation: Existing watermarking methods designed for language models cause significant performance drops (up to 96.99% with hard watermarks and 27.54% with soft watermarks) when directly applied to conditional text generation tasks like summarization and data-to-text generation, because they randomly partition vocabulary without considering task-specific input context.\n    - Need for effective AI detection in conditional text generation without quality loss: The authors are motivated by the need to detect AI-generated content to mitigate risks such as misinformation and malicious use, while maintaining high-quality outputs in conditional text generation tasks where input-output semantic overlap is substantial.\n    - Semantic-aware watermarking algorithm: The proposed method uses hash functions to embed watermarks while strategically partitioning the vocabulary based on word vector similarity to the input source. Semantically related tokens are included in the green list to reduce adverse impacts, balancing randomness with semantic relevance to the input context.\n    - Vocabulary partitioning using model embeddings and similarity matrix: The method constructs a similarity matrix from model embeddings and uses a hyperparameter k to identify top-k semantically related tokens for each input token. These tokens are added to the green list before randomly partitioning the remaining vocabulary, ensuring coherent and contextually appropriate generation.\n\nPaper 42:\n  Title: A Semantic Invariant Robust Watermark for Large Language Models\n  Abstract: Watermark algorithms for large language models (LLMs) have achieved extremely high accuracy in detecting text generated by LLMs. Such algorithms typically involve adding extra watermark logits to the LLM's logits at each generation step. However, prior algorithms face a trade-off between attack robustness and security robustness. This is because the watermark logits for a token are determined by a certain number of preceding tokens; a small number leads to low security robustness, while a large number results in insufficient attack robustness. In this work, we propose a semantic invariant watermarking method for LLMs that provides both attack robustness and security robustness. The watermark logits in our work are determined by the semantics of all preceding tokens. Specifically, we utilize another embedding LLM to generate semantic embeddings for all preceding tokens, and then these semantic embeddings are transformed into the watermark logits through our trained watermark model. Subsequent analyses and experiments demonstrated the attack robustness of our method in semantically invariant settings: synonym substitution and text paraphrasing settings. Finally, we also show that our watermark possesses adequate security robustness. Our code and data are available at \\href{this https URL}{this https URL\\_Watermark}. Additionally, our algorithm could also be accessed through MarkLLM \\citep{pan2024markllm} \\footnote{this https URL}.\n  Structured Summary:\n    - Trade-off between attack robustness and security robustness in LLM watermarking: Existing watermarking methods for large language models cannot simultaneously achieve strong attack robustness (resistance to text modifications) and security robustness (difficulty of inferring watermarking rules). Methods using fewer preceding tokens for watermark generation are vulnerable to decryption, while those using more tokens lack robustness against semantic-preserving attacks.\n    - Need for watermarks robust to semantic-preserving modifications: The authors aim to create a watermarking scheme where watermark signals depend on semantic content rather than specific token sequences. This approach ensures that semantically equivalent modifications (like paraphrasing or synonym substitution) preserve the watermark, while semantic diversity provides security against rule inference attacks.\n    - Semantic invariant watermarking using embedding LLM and watermark model: The method employs an auxiliary embedding language model (e.g., BERT) to extract semantic embeddings from preceding tokens. These embeddings are then fed into a trained watermark model—a neural network with fully connected layers—that transforms them into watermark logits, which are added to the generative LLM's logits during text generation.\n    - Watermark model training with similarity and normalization losses: The watermark model is trained using two loss functions: a similarity loss that ensures watermark logit similarities correlate with text embedding similarities across a broad range (-1 to 1), and a normalization loss that enforces unbiased token preference and balanced scores by centering means to zero and making absolute values uniform.\n\nPaper 43:\n  Title: REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models\n  Abstract: We present REMARK-LLM, a novel efficient, and robust watermarking framework designed for texts generated by large language models (LLMs). Synthesizing human-like content using LLMs necessitates vast computational resources and extensive datasets, encapsulating critical intellectual property (IP). However, the generated content is prone to malicious exploitation, including spamming and plagiarism. To address the challenges, REMARK-LLM proposes three new components: (i) a learning-based message encoding module to infuse binary signatures into LLM-generated texts; (ii) a reparameterization module to transform the dense distributions from the message encoding to the sparse distribution of the watermarked textual tokens; (iii) a decoding module dedicated for signature extraction; Furthermore, we introduce an optimized beam search algorithm to guarantee the coherence and consistency of the generated content. REMARK-LLM is rigorously trained to encourage the preservation of semantic integrity in watermarked content, while ensuring effective watermark retrieval. Extensive evaluations on multiple unseen datasets highlight REMARK-LLM proficiency and transferability in inserting 2 times more signature bits into the same texts when compared to prior art, all while maintaining semantic integrity. Furthermore, REMARK-LLM exhibits better resilience against a spectrum of watermark detection and removal attacks.\n  Structured Summary:\n    - Malicious exploitation and IP protection of LLM-generated content: The paper addresses the challenge of protecting intellectual property in LLM-generated texts and preventing malicious uses such as spam and plagiarism, while existing watermarking methods either distort semantics or have limited signature capacity.\n    - Need for ownership verification and content tracing in LLMs: The authors are motivated by the need to enable LLM proprietors to monitor content utilization, validate ownership, detect plagiarism, protect copyrights, and track potential misinformation generated by LLMs.\n    - Learning-based message encoding module: A Seq2Seq-based module that encodes LLM-generated texts and binary signatures into latent feature space, producing a watermarked distribution over the vocabulary while preserving semantic integrity.\n    - Reparameterization module with Gumbel-Softmax: A module that uses Gumbel-Softmax to convert dense token distributions from the encoder into sparse, differentiable representations suitable for message decoding, bridging the gap between continuous and discrete token spaces.\n\nPaper 44:\n  Title: What's my role? Modelling responsibility for AI-based safety-critical systems\n  Abstract: AI-Based Safety-Critical Systems (AI-SCS) are being increasingly deployed in the real world. These can pose a risk of harm to people and the environment. Reducing that risk is an overarching priority during development and operation. As more AI-SCS become autonomous, a layer of risk management via human intervention has been removed. Following an accident it will be important to identify causal contributions and the different responsible actors behind those to learn from mistakes and prevent similar future events. Many authors have commented on the \"responsibility gap\" where it is difficult for developers and manufacturers to be held responsible for harmful behaviour of an AI-SCS. This is due to the complex development cycle for AI, uncertainty in AI performance, and dynamic operating environment. A human operator can become a \"liability sink\" absorbing blame for the consequences of AI-SCS outputs they weren't responsible for creating, and may not have understanding of.\nThis cross-disciplinary paper considers different senses of responsibility (role, moral, legal and causal), and how they apply in the context of AI-SCS safety. We use a core concept (Actor(A) is responsible for Occurrence(O)) to create role responsibility models, producing a practical method to capture responsibility relationships and provide clarity on the previously identified responsibility issues. Our paper demonstrates the approach with two examples: a retrospective analysis of the Tempe Arizona fatal collision involving an autonomous vehicle, and a safety focused predictive role-responsibility analysis for an AI-based diabetes co-morbidity predictor. In both examples our primary focus is on safety, aiming to reduce unfair or disproportionate blame being placed on operators or developers. We present a discussion and avenues for future research.\n  Structured Summary:\n    - Responsibility gaps and liability sinks in AI-based safety-critical systems: The paper addresses the challenge that AI-based safety-critical systems create responsibility gaps due to complex development cycles, uncertainty in black-box AI components, and dynamic operating environments. This makes it difficult to attribute responsibility for harm, while human operators may unfairly absorb blame for AI system outputs they did not create.\n    - Need to prevent unfair blame and learn from accidents in AI-SCS: The authors are motivated by the need to identify responsible actors after accidents to learn from mistakes and prevent future incidents. They aim to provide clarity on responsibility issues to avoid unfair blame on operators or developers and support safety assurance processes for AI-based safety-critical systems.\n    - Role responsibility modelling notation with typed relationships: The authors extend prior work on responsibility modelling and task analysis to develop a notation for modelling role responsibility relationships in AI-SCS. The notation uses the formulation 'Actor is responsible for Occurrence' and includes elements for actors (individuals, institutions, AI systems), occurrences (decisions, actions, omissions), resources, and typed relationships (responsibility types, uses, subordinate to, association, acts as).\n    - HAZOP-style analysis method with responsibility-specific guidewords: The authors propose an analysis method that uses guidewords (insufficient/partial, misassigned, overloaded, duplicated/conflict, missing/never/omission, early/late/ordering, incorrect/value) to systematically review responsibility models for safety impacts. This method helps identify issues such as missing responsibilities, conflicts, and burden of risk distribution.\n\nPaper 45:\n  Title: Undetectable Watermarks for Language Models\n  Abstract: Recent advances in the capabilities of large language models such as GPT-4 have spurred increasing concern about our ability to detect AI-generated text. Prior works have suggested methods of embedding watermarks in model outputs, by noticeably altering the output distribution. We ask: Is it possible to introduce a watermark without incurring any detectable change to the output distribution?\nTo this end we introduce a cryptographically-inspired notion of undetectable watermarks for language models. That is, watermarks can be detected only with the knowledge of a secret key; without the secret key, it is computationally intractable to distinguish watermarked outputs from those of the original model. In particular, it is impossible for a user to observe any degradation in the quality of the text. Crucially, watermarks should remain undetectable even when the user is allowed to adaptively query the model with arbitrarily chosen prompts. We construct undetectable watermarks based on the existence of one-way functions, a standard assumption in cryptography.\n  Structured Summary:\n    - Detecting AI-generated text without degrading output quality: Existing watermarking schemes for language models noticeably alter the output distribution, making it possible for users to detect the watermark and potentially degrading text quality. The paper addresses whether watermarks can be embedded without any observable change to the model's output distribution.\n    - Preventing misuse while maintaining model quality: The authors are motivated by the need to distinguish AI-generated text from human-written text to prevent misuse (e.g., academic dishonesty), while ensuring that watermarking does not degrade the quality or utility of state-of-the-art language models, which would be unacceptable to model creators and users.\n    - Cryptographically undetectable watermarking scheme: The authors propose a watermarking scheme that uses pseudorandom functions (PRFs) derived from one-way functions. The scheme embeds watermarks by biasing token selection based on a secret key, ensuring that without the key, the watermarked distribution is computationally indistinguishable from the original model's distribution.\n    - Empirical entropy-based detection: The authors introduce the concept of empirical entropy to quantify the randomness used in generating a specific output. Detection of watermarks is conditioned on outputs having sufficiently high empirical entropy, ensuring that only outputs with enough randomness are watermarked and detectable.\n\nPaper 46:\n  Title: Provable Robust Watermarking for AI-Generated Text\n  Abstract: We study the problem of watermarking large language models (LLMs) generated text -- one of the most promising approaches for addressing the safety challenges of LLM usage. In this paper, we propose a rigorous theoretical framework to quantify the effectiveness and robustness of LLM watermarks. We propose a robust and high-quality watermark method, Unigram-Watermark, by extending an existing approach with a simplified fixed grouping strategy. We prove that our watermark method enjoys guaranteed generation quality, correctness in watermark detection, and is robust against text editing and paraphrasing. Experiments on three varying LLMs and two datasets verify that our Unigram-Watermark achieves superior detection accuracy and comparable generation quality in perplexity, thus promoting the responsible use of LLMs. Code is available at this https URL.\n  Structured Summary:\n    - Lack of provable robustness guarantees for LLM watermarks against post-processing attacks: Existing watermarking methods for large language models lack formal guarantees of robustness when adversaries edit or paraphrase the watermarked text, making it difficult to ensure reliable detection after post-processing attacks.\n    - Addressing safety challenges and misuse of LLMs through provably robust watermarking: The authors are motivated by the need to prevent misuse of LLMs such as generating fake news, academic dishonesty, and contaminating training data, which requires a watermarking approach that can reliably prove text origin even after adversarial modifications.\n    - Unigram-Watermark with fixed Green-Red vocabulary split: The method randomly partitions the vocabulary into a fixed green list and red list, increases logits of green tokens by delta during generation, and detects watermarks by counting green tokens in suspect text using a z-statistic test.\n    - Rigorous theoretical framework for quantifying watermark effectiveness and robustness: The framework formally defines watermarking schemes with quality guarantees measured by Renyi divergence, Type I/II error bounds for detection correctness, and security properties quantifying robustness to adversarial edits via edit distance.\n\nPaper 47:\n  Title: A Watermark for Large Language Models\n  Abstract: Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of \"green\" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.\n  Structured Summary:\n    - Detecting machine-generated text from large language models: The paper addresses the challenge of identifying synthetic text produced by large language models to mitigate potential harms such as social engineering, fake news creation, academic cheating, and contamination of training datasets with synthetic data.\n    - Mitigating harms of large language models through output detection: The authors are motivated by the increasing risks of malicious uses of LLMs, including automated bots for manipulation, fake content generation, and academic dishonesty. They seek to enable third parties to detect and audit machine-generated text as a harm reduction strategy.\n    - Soft red list watermarking with green token promotion: The method partitions the vocabulary into green and red lists using a hash of the previous token, then adds a constant delta to the logits of green tokens before sampling. This soft promotion adaptively enforces the watermark in high-entropy contexts while minimally affecting low-entropy sequences.\n    - Statistical test for watermark detection using z-statistic: The authors develop a one-proportion z-test that evaluates whether the observed number of green tokens significantly exceeds what would be expected by chance under the null hypothesis that text was generated without knowledge of the watermark rule, enabling rigorous confidence measures.\n\nPaper 48:\n  Title: StealthInk: A Multi-bit and Stealthy Watermark for Large Language Models\n  Abstract: Watermarking for large language models (LLMs) offers a promising approach to identifying AI-generated text. Existing approaches, however, either compromise the distribution of original generated text by LLMs or are limited to embedding zero-bit information that only allows for watermark detection but ignores identification. We present StealthInk, a stealthy multi-bit watermarking scheme that preserves the original text distribution while enabling the embedding of provenance data, such as userID, TimeStamp, and modelID, within LLM-generated text. This enhances fast traceability without requiring access to the language model's API or prompts. We derive a lower bound on the number of tokens necessary for watermark detection at a fixed equal error rate, which provides insights on how to enhance the capacity. Comprehensive empirical evaluations across diverse tasks highlight the stealthiness, detectability, and resilience of StealthInk, establishing it as an effective solution for LLM watermarking applications.\n  Structured Summary:\n    - Existing watermarking schemes compromise text distribution or lack multi-bit capacity: Current LLM watermarking methods either distort the original text distribution generated by the model or only support zero-bit watermarks that enable detection without embedding provenance information like userID or TimeStamp.\n    - Need for traceability and prevention of LLM misuse: The authors are motivated by the need to address ethical concerns around LLM misuse, including fake news generation, scams, and plagiarism, by enabling fast traceability of AI-generated text through embedded provenance information.\n    - StealthInk multi-bit watermarking scheme with novel reweighting strategy: StealthInk uses a novel token sampling probability reweighting strategy that doubles probabilities of certain tokens while zeroing others (red tokens), preserving the expected distribution over random permutations while embedding multi-bit messages.\n    - K-shot stealthiness guarantee for multi-bit watermarking: The authors extend the stealthiness concept from zero-bit to multi-bit watermarking, proving that when each token is generated with unbiasedness and ciphers are independent, multiple prompt-response pairs maintain K-shot stealthiness.\n\nPaper 49:\n  Title: Robust Distortion-free Watermarks for Language Models\n  Abstract: We propose a methodology for planting watermarks in text from an autoregressive language model that are robust to perturbations without changing the distribution over text up to a certain maximum generation budget. We generate watermarked text by mapping a sequence of random numbers -- which we compute using a randomized watermark key -- to a sample from the language model. To detect watermarked text, any party who knows the key can align the text to the random number sequence. We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling. We apply these watermarks to three language models -- OPT-1.3B, LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text ($p \\leq 0.01$) from $35$ tokens even after corrupting between $40$-$50\\%$ of the tokens via random edits (i.e., substitutions, insertions or deletions). For the Alpaca-7B model, we conduct a case study on the feasibility of watermarking responses to typical user instructions. Due to the lower entropy of the responses, detection is more difficult: around $25\\%$ of the responses -- whose median length is around $100$ tokens -- are detectable with $p \\leq 0.01$, and the watermark is also less robust to certain automated paraphrasing attacks we implement.\n  Structured Summary:\n    - Lack of robust, distortion-free watermarks for language models: Prior watermarking methods for language models fail to simultaneously preserve the original text distribution and remain detectable after adversarial edits or cropping of the watermarked text.\n    - Need for provenance of language model-generated content: The proliferation of synthetic text from language models necessitates reliable forensic tools to attribute text to specific models, enabling platform moderators, educators, and model providers to enforce policies and track model usage or misuse.\n    - Distortion-free watermarking via inverse transform sampling: The authors propose a watermark using inverse transform sampling, where a decoder maps uniform random variables and permutations to tokens. Detection correlates permuted token indices with the random sequence, ensuring the watermark is distortion-free and robust to edits.\n    - Distortion-free watermarking via exponential minimum sampling: The authors propose a second watermark using exponential minimum sampling, mapping random vectors to tokens via a decoder. This method is also distortion-free and robust, with detection based on alignment costs derived from the sampling mechanism.\n\nPaper 50:\n  Title: SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation\n  Abstract: Existing watermarking algorithms are vulnerable to paraphrase attacks because of their token-level design. To address this issue, we propose SemStamp, a robust sentence-level semantic watermarking algorithm based on locality-sensitive hashing (LSH), which partitions the semantic space of sentences. The algorithm encodes and LSH-hashes a candidate sentence generated by an LLM, and conducts sentence-level rejection sampling until the sampled sentence falls in watermarked partitions in the semantic embedding space. A margin-based constraint is used to enhance its robustness. To show the advantages of our algorithm, we propose a \"bigram\" paraphrase attack using the paraphrase that has the fewest bigram overlaps with the original sentence. This attack is shown to be effective against the existing token-level watermarking method. Experimental results show that our novel semantic watermark algorithm is not only more robust than the previous state-of-the-art method on both common and bigram paraphrase attacks, but also is better at preserving the quality of generation.\n  Structured Summary:\n    - Vulnerability of token-level watermarks to paraphrase attacks: Current watermarking methods for LLM-generated text operate at the token level, making them susceptible to paraphrase attacks that alter surface tokens while preserving semantics. This undermines the detectability of watermarks after text is paraphrased.\n    - Detecting machine-generated text to prevent LLM misuse: The authors are motivated by the urgent need to detect machine-generated text as LLMs produce increasingly realistic outputs, which raises concerns about misinformation, impersonation, and copyright violations.\n    - SEMSTAMP: Sentence-level semantic watermarking with LSH: SEMSTAMP operates on sentence-level semantic embeddings rather than tokens. It uses LSH to partition the semantic space into valid and blocked regions, then applies rejection sampling to generate sentences whose embeddings fall into valid regions, making the watermark robust to paraphrasing.\n    - Paraphrase-robust sentence encoder via contrastive learning: The authors fine-tune a Sentence-BERT encoder using contrastive learning on paraphrased data to ensure that paraphrases of a sentence remain close in the embedding space, enhancing the robustness of LSH signatures under paraphrasing.\n\n\nBefore returning, check that:\n- Every paper title appears exactly once.\n- Only leaf nodes have a \"papers\" field.\n- All intermediate nodes have a \"subtopics\" field.\n- The JSON is strictly valid and parsable.\n\n### Output:", "hierarchy_response": "{\n  \"name\": \"Detection of LLMs-Generated Content\",\n  \"subtopics\": [\n    {\n      \"name\": \"Detection Methods and Techniques\",\n      \"subtopics\": [\n        {\n          \"name\": \"Zero-Shot and Training-Free Detection\",\n          \"subtopics\": [\n            {\n              \"name\": \"Probability Curvature-Based Methods\",\n              \"papers\": [\n                \"DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature\",\n                \"Smaller Language Models are Better Black-box Machine-Generated Text Detectors\",\n                \"Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model\",\n                \"Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature\"\n              ]\n            },\n            {\n              \"name\": \"Statistical and Information-Theoretic Approaches\",\n              \"papers\": [\n                \"GLTR: Statistical Detection and Visualization of Generated Text\",\n                \"DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text\",\n                \"DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text\",\n                \"GPT-who: An Information Density-based Machine-Generated Text Detector\"\n              ]\n            }\n          ]\n        },\n        {\n          \"name\": \"Supervised and Fine-Tuned Detection\",\n          \"subtopics\": [\n            {\n              \"name\": \"Transformer-Based Classifiers\",\n              \"papers\": [\n                \"GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content\",\n                \"Technical Report on the Pangram AI-Generated Text Classifier\",\n                \"Ghostbuster: Detecting Text Ghostwritten by Large Language Models\"\n              ]\n            },\n            {\n              \"name\": \"Feature-Based Detection\",\n              \"papers\": [\n                \"CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Data Limitation With Contrastive Learning\",\n                \"Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts\",\n                \"On the Coherence of Fake News Articles\"\n              ]\n            }\n          ]\n        },\n        {\n          \"name\": \"Cross-Model and Domain Adaptation\",\n          \"papers\": [\n            \"On the Zero-Shot Generalization of Machine-Generated Text Detectors\",\n            \"From Text to Source: Results in Detecting Large Language Model-Generated Content\",\n            \"ConDA: Contrastive Domain Adaptation for AI-generated Text Detection\",\n            \"Multiscale Positive-Unlabeled Detection of AI-Generated Texts\"\n          ]\n        },\n        {\n          \"name\": \"Specialized Detection Scenarios\",\n          \"subtopics\": [\n            {\n              \"name\": \"Sentence-Level and Fine-Grained Detection\",\n              \"papers\": [\n                \"SeqXGPT: Sentence-Level AI-Generated Text Detection\"\n              ]\n            },\n            {\n              \"name\": \"Model Attribution and Origin Tracing\",\n              \"papers\": [\n                \"Origin Tracing and Detecting of LLMs\",\n                \"LLMDet: A Third Party Large Language Models Generated Text Detection Tool\",\n                \"Artificial Text Detection with Multiple Training Strategies\"\n              ]\n            },\n            {\n              \"name\": \"Multimodal Detection\",\n              \"papers\": [\n                \"Deepfake Videos in the Wild: Analysis and Detection\",\n                \"Exploring the Role of Visual Content in Fake News Detection\"\n              ]\n            }\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Watermarking Techniques\",\n      \"subtopics\": [\n        {\n          \"name\": \"Zero-Bit Watermarking\",\n          \"papers\": [\n            \"A Watermark for Large Language Models\",\n            \"Provable Robust Watermarking for AI-Generated Text\",\n            \"Three Bricks to Consolidate Watermarks for Large Language Models\",\n            \"Undetectable Watermarks for Language Models\"\n          ]\n        },\n        {\n          \"name\": \"Multi-Bit Watermarking\",\n          \"papers\": [\n            \"Advancing Beyond Identification: Multi-bit Watermark for Large Language Models\",\n            \"StealthInk: A Multi-bit and Stealthy Watermark for Large Language Models\",\n            \"REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models\"\n          ]\n        },\n        {\n          \"name\": \"Semantic and Robust Watermarking\",\n          \"papers\": [\n            \"SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation\",\n            \"A Semantic Invariant Robust Watermark for Large Language Models\",\n            \"Robust Multi-bit Natural Language Watermarking through Invariant Features\",\n            \"Robust Distortion-free Watermarks for Language Models\"\n          ]\n        },\n        {\n          \"name\": \"Black-Box and Context-Aware Watermarking\",\n          \"papers\": [\n            \"Watermarking Text Generated by Black-Box Language Models\",\n            \"Tracing Text Provenance via Context-Aware Lexical Substitution\",\n            \"Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Robustness and Adversarial Attacks\",\n      \"subtopics\": [\n        {\n          \"name\": \"Paraphrasing and Text Perturbation Attacks\",\n          \"papers\": [\n            \"Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense\",\n            \"RADAR: Robust AI-Text Detection via Adversarial Learning\"\n          ]\n        },\n        {\n          \"name\": \"Defense Mechanisms\",\n          \"papers\": [\n            \"Natural Language Adversarial Defense through Synonym Encoding\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Evaluation and Benchmarking\",\n      \"subtopics\": [\n        {\n          \"name\": \"Human vs. Machine Detection Performance\",\n          \"papers\": [\n            \"Automatic Detection of Generated Text is Easiest when Humans are Fooled\",\n            \"Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays\"\n          ]\n        },\n        {\n          \"name\": \"Fake News Detection\",\n          \"papers\": [\n            \"Detecting Fake News Using Machine Learning : A Systematic Literature Review\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Related Applications and Tools\",\n      \"subtopics\": [\n        {\n          \"name\": \"Text Generation and Quality Assessment\",\n          \"papers\": [\n            \"RefGPT: Dialogue Generation of GPT, by GPT, and for GPT\",\n            \"Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization\"\n          ]\n        },\n        {\n          \"name\": \"Text Rewriting and Annotation Tools\",\n          \"papers\": [\n            \"ALTER: Auxiliary Text Rewriting Tool for Natural Language Generation\"\n          ]\n        },\n        {\n          \"name\": \"Policy and Responsibility\",\n          \"papers\": [\n            \"Release Strategies and the Social Impacts of Language Models\",\n            \"What's my role? Modelling responsibility for AI-based safety-critical systems\"\n          ]\n        },\n        {\n          \"name\": \"Security and Integrity\",\n          \"papers\": [\n            \"Tamperproof IoT with Blockchain\"\n          ]\n        }\n      ]\n    }\n  ]\n}\n", "hierarchy_tree": {"name": "Detection of LLMs-Generated Content", "subtopics": [{"name": "Detection Methods and Techniques", "subtopics": [{"name": "Zero-Shot and Training-Free Detection", "subtopics": [{"name": "Probability Curvature-Based Methods", "papers": ["DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature", "Smaller Language Models are Better Black-box Machine-Generated Text Detectors", "Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model", "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature"]}, {"name": "Statistical and Information-Theoretic Approaches", "papers": ["GLTR: Statistical Detection and Visualization of Generated Text", "DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text", "DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text", "GPT-who: An Information Density-based Machine-Generated Text Detector"]}]}, {"name": "Supervised and Fine-Tuned Detection", "subtopics": [{"name": "Transformer-Based Classifiers", "papers": ["GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content", "Technical Report on the Pangram AI-Generated Text Classifier", "Ghostbuster: Detecting Text Ghostwritten by Large Language Models"]}, {"name": "Feature-Based Detection", "papers": ["CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Data Limitation With Contrastive Learning", "Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts", "On the Coherence of Fake News Articles"]}]}, {"name": "Cross-Model and Domain Adaptation", "papers": ["On the Zero-Shot Generalization of Machine-Generated Text Detectors", "From Text to Source: Results in Detecting Large Language Model-Generated Content", "ConDA: Contrastive Domain Adaptation for AI-generated Text Detection", "Multiscale Positive-Unlabeled Detection of AI-Generated Texts"]}, {"name": "Specialized Detection Scenarios", "subtopics": [{"name": "Sentence-Level and Fine-Grained Detection", "papers": ["SeqXGPT: Sentence-Level AI-Generated Text Detection"]}, {"name": "Model Attribution and Origin Tracing", "papers": ["Origin Tracing and Detecting of LLMs", "LLMDet: A Third Party Large Language Models Generated Text Detection Tool", "Artificial Text Detection with Multiple Training Strategies"]}, {"name": "Multimodal Detection", "papers": ["Deepfake Videos in the Wild: Analysis and Detection", "Exploring the Role of Visual Content in Fake News Detection"]}]}]}, {"name": "Watermarking Techniques", "subtopics": [{"name": "Zero-Bit Watermarking", "papers": ["A Watermark for Large Language Models", "Provable Robust Watermarking for AI-Generated Text", "Three Bricks to Consolidate Watermarks for Large Language Models", "Undetectable Watermarks for Language Models"]}, {"name": "Multi-Bit Watermarking", "papers": ["Advancing Beyond Identification: Multi-bit Watermark for Large Language Models", "StealthInk: A Multi-bit and Stealthy Watermark for Large Language Models", "REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models"]}, {"name": "Semantic and Robust Watermarking", "papers": ["SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation", "A Semantic Invariant Robust Watermark for Large Language Models", "Robust Multi-bit Natural Language Watermarking through Invariant Features", "Robust Distortion-free Watermarks for Language Models"]}, {"name": "Black-Box and Context-Aware Watermarking", "papers": ["Watermarking Text Generated by Black-Box Language Models", "Tracing Text Provenance via Context-Aware Lexical Substitution", "Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy"]}]}, {"name": "Robustness and Adversarial Attacks", "subtopics": [{"name": "Paraphrasing and Text Perturbation Attacks", "papers": ["Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense", "RADAR: Robust AI-Text Detection via Adversarial Learning"]}, {"name": "Defense Mechanisms", "papers": ["Natural Language Adversarial Defense through Synonym Encoding"]}]}, {"name": "Evaluation and Benchmarking", "subtopics": [{"name": "Human vs. Machine Detection Performance", "papers": ["Automatic Detection of Generated Text is Easiest when Humans are Fooled", "Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays"]}, {"name": "Fake News Detection", "papers": ["Detecting Fake News Using Machine Learning : A Systematic Literature Review"]}]}, {"name": "Related Applications and Tools", "subtopics": [{"name": "Text Generation and Quality Assessment", "papers": ["RefGPT: Dialogue Generation of GPT, by GPT, and for GPT", "Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization"]}, {"name": "Text Rewriting and Annotation Tools", "papers": ["ALTER: Auxiliary Text Rewriting Tool for Natural Language Generation"]}, {"name": "Policy and Responsibility", "papers": ["Release Strategies and the Social Impacts of Language Models", "What's my role? Modelling responsibility for AI-based safety-critical systems"]}, {"name": "Security and Integrity", "papers": ["Tamperproof IoT with Blockchain"]}]}]}}
{"id": 64, "survey": "Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects", "pdfs": [{"title": "CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Actions of LLM-based Agents/Tool Creation/CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets.pdf", "summary": {"paper_id": "survey-64_paper-0_CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets", "core_task": {"text": "Customizing large language models through tool creation and retrieval", "query_variants": ["Customizing large language models through tool creation and retrieval", "Adapting LLMs via tool generation and selection mechanisms", "Tailoring large language models using tool construction and fetching"]}, "contributions": [{"name": "Limited flexibility of general-purpose APIs for task-specific LLM augmentation", "description": "Existing methods for augmenting large language models with external tools rely on general-purpose APIs that cannot be easily customized for specific downstream tasks, limiting their effectiveness in specialized domains.", "author_claim_text": "most existing approaches to augment LLMs with tools are constrained by general-purpose APIs and lack the flexibility for tailoring them to specific tasks", "source_hint": "Abstract"}, {"name": "Need for customizable toolsets to adapt LLMs to specialized domains", "description": "The authors are motivated by the observation that many real-world applications require specialized APIs not covered by general-purpose tool collections, necessitating a framework that can create and retrieve task-specific tools to customize LLMs for diverse domains without extensive fine-tuning.", "author_claim_text": "adapting LLMs to many domains and evolving applications involves working with more specialized APIs tailored to address specific challenges, which are often inadequately represented in general-purpose toolsets", "source_hint": "Introduction §1"}, {"name": "CRAFT framework for tool creation and retrieval", "description": "CRAFT is a plug-and-play framework that constructs task-specific toolsets by generating, abstracting, validating, and deduplicating code solutions from training examples, then retrieves relevant tools at inference time using multi-view matching based on problem descriptions, API names, and docstrings.", "author_claim_text": "we present CRAFT, a general tool creation and retrieval framework for LLMs. It creates toolsets specifically curated for the tasks and equips LLMs with a component that retrieves tools from these sets", "source_hint": "Abstract"}, {"name": "Four-step tool creation pipeline", "description": "The tool creation process involves iteratively sampling diverse problems, generating specific code solutions with GPT-4, abstracting them into reusable snippets with generic variable names and docstrings, validating correctness on original problems, and deduplicating to reduce redundancy.", "author_claim_text": "CRAFT constructs the toolset through four steps: Generation, Abstraction, Verification, and Deduplication", "source_hint": "Section 2.1"}]}, "abs": "Large language models (LLMs) are often augmented with tools to solve complex tasks. By generating code snippets and executing them through task-specific Application Programming Interfaces (APIs), they can offload certain functions to dedicated external modules, such as image encoding and performing calculations. However, most existing approaches to augment LLMs with tools are constrained by general-purpose APIs and lack the flexibility for tailoring them to specific tasks. In this work, we present CRAFT, a general tool creation and retrieval framework for LLMs. It creates toolsets specifically curated for the tasks and equips LLMs with a component that retrieves tools from these sets to enhance their capability to solve complex tasks. For each task, we collect specific code solutions by prompting GPT-4 to solve the training examples. Following a validation step ensuring the correctness, these solutions are abstracted into code snippets to enhance reusability, and deduplicated for higher quality. At inference time, the language model retrieves snippets from the toolsets and then executes them or generates the output conditioning on the retrieved snippets. Our method is designed to be flexible and offers a plug-and-play approach to adapt off-the-shelf LLMs to unseen domains and modalities, without any finetuning. Experiments on vision-language, tabular processing, and mathematical reasoning tasks show that our approach achieves substantial improvements compared to strong baselines. In addition, our in-depth analysis reveals that: (1) consistent performance improvement can be achieved by scaling up the number of tools and the capability of the backbone models; (2) each component of our approach contributes to the performance gains; (3) the created tools are well-structured and reliable with low complexity and atomicity. The code is available at this https URL."}, {"title": "Large Language Models as Tool Makers", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Actions of LLM-based Agents/Tool Creation/Large Language Models as Tool Makers.pdf", "summary": {"paper_id": "survey-64_paper-1_Large Language Models as Tool Makers", "core_task": {"text": "large language models creating reusable tools for problem-solving", "query_variants": ["large language models creating reusable tools for problem-solving", "LLMs generating generalizable tools for task completion", "language models building reusable functions for solving problems"]}, "contributions": [{"name": "Limited applicability of tool-using methods due to tool availability", "description": "Existing methods for augmenting LLMs with external tools are constrained by whether appropriate tools already exist for the tasks at hand, limiting their broader application.", "author_claim_text": "the applicability of these tool-using methods is largely contingent on the availability of suitable tools", "source_hint": "Introduction paragraph 2"}, {"name": "Enabling LLMs to fabricate their own tools for emerging challenges", "description": "The authors are motivated by the evolutionary milestone where humans learned to create their own tools. They aim to replicate this capability in LLMs, allowing models to generate reusable tools for new tasks rather than relying solely on pre-existing tools.", "author_claim_text": "a crucial turning point was that humans got the ability to fabricate their own tools to address emerging challenges. Inspired by the importance of tool-making for humans, in this work, we embark on an initial exploration to apply this evolutionary concept to the realm of LLMs", "source_hint": "Introduction paragraph 2"}, {"name": "LATM closed-loop framework with tool making and tool using stages", "description": "LATM is a two-stage framework where a powerful LLM (tool maker) creates reusable Python function tools from a few demonstrations, and a lightweight LLM (tool user) applies these tools to solve new task instances. This division of labor reduces overall computational cost while maintaining high performance.", "author_claim_text": "We propose a closed-loop framework, which we term as LLMs As Tool Makers (LATM), enables LLMs to generate their own reusable tools to tackle new tasks. Our approach comprises two key stages: 1) tool making: an LLM, known as the tool maker, designs tools (implemented as Python functions) specifically for a given task. 2) tool using: another LLM referred to as the tool user, which can be the same as the tool maker, applies the tools to handle new requests", "source_hint": "Introduction paragraph 3"}, {"name": "Functional cache mechanism for LLM serving", "description": "LATM introduces a functional cache that stores generated tools (functions) rather than textual responses. A dispatcher LLM identifies whether an incoming request can be solved with cached tools or requires new tool creation, thereby reducing serving costs for recurring task types.", "author_claim_text": "our method offers a functional cache through the caching and reuse of tools, which stores the functionality of a class of requests instead of the natural language responses from LLMs, thus extending the applicability of the conventional cache mechanism", "source_hint": "Abstract"}]}, "abs": "Recent research has highlighted the potential of large language models (LLMs) to improve their problem-solving capabilities with the aid of suitable external tools. In our work, we further advance this concept by introducing a closed-loop framework, referred to as LLMs A s Tool Makers (LATM), where LLMs create their own reusable tools for problem-solving. Our approach consists of two phases: 1) tool making: an LLM acts as the tool maker that crafts tools for a set of tasks. 2) tool using: another LLM acts as the tool user, which applies the tool built by the tool maker for problem-solving. On the problem-solving server side, tool-making enables continual tool generation and caching as new requests emerge. This framework enables subsequent requests to access cached tools via their corresponding APIs, enhancing the efficiency of task resolution. Recognizing that tool-making requires more sophisticated capabilities, we assign this task to a powerful, albeit resource-intensive, model. Conversely, the simpler tool-using phase is delegated to a lightweight model. This strategic division of labor allows the once-off cost of tool-making to be spread over multiple instances of tool-using, significantly reducing average costs while maintaining strong performance. Furthermore, our method offers a functional cache through the caching and reuse of tools, which stores the functionality of a class of requests instead of the natural language responses from LLMs, thus extending the applicability of the conventional cache mechanism. We evaluate our approach across various complex reasoning tasks, including Big-Bench tasks. With GPT-4 as the tool maker and GPT-3.5 as the tool user, LATM demonstrates performance equivalent to using GPT-4 for both roles, but with a significantly reduced inference cost."}, {"title": "RestGPT: Connecting Large Language Models with Real-World RESTful APIs", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Actions of LLM-based Agents/Tool Employment/RestGPT: Connecting Large Language Models with Real-World RESTful APIs.pdf", "summary": {"paper_id": "survey-64_paper-2_RestGPT: Connecting Large Language Models with Real-World RESTful APIs", "core_task": {"text": "Connecting large language models with real-world RESTful APIs", "query_variants": ["Connecting large language models with real-world RESTful APIs", "Integrating LLMs with external RESTful application programming interfaces", "Linking large language models to real-world REST API services"]}, "contributions": [{"name": "Limitations of existing tool-augmented LLMs in real-world scenarios", "description": "Current tool-augmented large language models are limited to small numbers of specially designed APIs and cannot effectively handle complex real-world instructions that require decomposing tasks and using multiple APIs in combination.", "author_claim_text": "existing methods are mainly restricted to specifically designed tools and fail to fulfill complex instructions, having great limitations when confronted with real-world scenarios", "source_hint": "Abstract"}, {"name": "Connecting LLMs with real-world RESTful APIs for practical applications", "description": "The authors aim to bridge the gap between LLMs and real-world applications by leveraging RESTful APIs, which are the de facto standard for web services, thereby enabling LLMs to interact with existing applications in a standardized and extensible manner.", "author_claim_text": "we explore a more realistic scenario by connecting LLMs with RESTful APIs, which adhere to the widely adopted REST software architectural style for web service development", "source_hint": "Abstract"}, {"name": "RestGPT framework with coarse-to-fine online planning", "description": "RestGPT employs an iterative planning approach where a Planner generates high-level natural language sub-tasks and an API Selector maps these to specific API calls, forming a flexible coarse-to-fine planning mechanism that adapts dynamically to execution feedback.", "author_claim_text": "we propose RestGPT, which exploits the power of LLMs and conducts a coarse-to-fine online planning mechanism to enhance the abilities of task decomposition and API selection", "source_hint": "Abstract"}, {"name": "Specialized API executor for RESTful APIs", "description": "The executor consists of a Caller that generates correct API parameters using API documentation and a schema-based Parser that leverages OpenAPI Specification response schemas to generate Python code for extracting information from complex JSON responses.", "author_claim_text": "RestGPT also contains an API executor tailored for calling RESTful APIs, which can meticulously formulate parameters and parse API responses", "source_hint": "Abstract and Section 3.3"}]}, "abs": "Tool-augmented large language models (LLMs) have achieved remarkable progress in tackling a broad range of tasks. However, existing methods are mainly restricted to specifically designed tools and fail to fulfill complex instructions, having great limitations when confronted with real-world scenarios. In this paper, we explore a more realistic scenario by connecting LLMs with RESTful APIs, which adhere to the widely adopted REST software architectural style for web service development. To address the practical challenges of tackling complex instructions, we propose RestGPT, which exploits the power of LLMs and conducts a coarse-to-fine online planning mechanism to enhance the abilities of task decomposition and API selection. RestGPT also contains an API executor tailored for calling RESTful APIs, which can meticulously formulate parameters and parse API responses. To fully evaluate the performance of RestGPT, we propose RestBench, a high-quality benchmark which consists of two real-world scenarios and human-annotated instructions with gold solution paths. Experiments show that RestGPT is able to achieve impressive results in complex tasks and has strong robustness, which paves a new way towards AGI. RestGPT and RestBench is publicly available at this https URL."}, {"title": "TALM: Tool Augmented Language Models", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Actions of LLM-based Agents/Tool Employment/TALM: Tool Augmented Language Models.pdf", "summary": {"paper_id": "survey-64_paper-3_TALM: Tool Augmented Language Models", "core_task": {"text": "Augmenting language models with external tools", "query_variants": ["Augmenting language models with external tools", "Language models enhanced with external tool integration", "Integrating external tools into language models"]}, "contributions": [{"name": "Language models cannot access ephemeral, changing, or private data unavailable at training time", "description": "The paper addresses the fundamental limitation that transformer-based language models, despite increasing performance with scale, cannot solve tasks requiring access to data that was not present during training, such as live information, private data, or state-modifying APIs.", "author_claim_text": "Scale alone however cannot enable models to solve tasks that require access to ephemeral, changing, or private data that was unavailable at training time.", "source_hint": "Abstract"}, {"name": "Reducing dependence on model scale while enhancing capabilities", "description": "The authors are motivated by the practical downsides of scaling up language models, including high costs, deployment challenges, and reliance on memorization. They aim to enhance model capabilities through tool augmentation rather than solely through increasing model size.", "author_claim_text": "Our results suggest that Tool Augmented Language Models are a promising direction to enrich LMs' capabilities, with less dependence on scale.", "source_hint": "Abstract and Introduction"}, {"name": "Text-to-text tool interface for language model augmentation", "description": "TALM uses a text-to-text interface where the model generates tool inputs, invokes external tool APIs through delimiter detection, receives tool outputs appended to the text sequence, and then generates final task outputs. This approach enables models to interact with non-differentiable tools in a unified textual format.", "author_claim_text": "We use a Text-to-Text tool interface given its broad applicability and simplicity. TALM first generates a tool input conditioned on the task input text and invokes a tool's API by generating a delimiter.", "source_hint": "Methods §2.1"}, {"name": "Iterative self-play technique for bootstrapping tool-use performance", "description": "The method iteratively improves tool-use performance by fine-tuning the model on a growing dataset of tool interactions. In each round, the model samples tool inputs, calls APIs, generates outputs, and adds successful sequences to the training set if they match target outputs within a threshold.", "author_claim_text": "we present Tool Augmented Language Models (TALM), combining a text-only approach to augment language models with non-differentiable tools, and an iterative 'self-play' technique to bootstrap performance starting from few tool demonstrations.", "source_hint": "Abstract and Methods §2.2"}]}, "abs": "Transformer based language models (LMs) demonstrate increasing performance with scale across a wide variety of tasks. Scale alone however cannot enable models to solve tasks that require access to ephemeral, changing, or private data that was unavailable at training time. Many useful tasks may also benefit from LMs being able to access APIs that read or modify state. In this work, we present Tool Augmented Language Models (TALM), combining a text-only approach to augment language models with non-differentiable tools, and an iterative \"self-play\" technique to bootstrap performance starting from few tool demonstrations. TALM exhibits strong performance on both a knowledge-heavy QA task and a reasoning oriented math task with simple tools. At a given model scale, TALM significantly outperforms non-augmented LMs. We further demonstrate that TALM successfully performs out-of-distribution inferences on both QA and math tasks, where non-augmented LMs fail. Our results suggest that Tool Augmented Language Models are a promising direction to enrich LMs' capabilities, with less dependence on scale."}, {"title": "Gorilla: Large Language Model Connected with Massive APIs", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Actions of LLM-based Agents/Tool Employment/Gorilla: Large Language Model Connected with Massive APIs.pdf", "summary": {"paper_id": "survey-64_paper-4_Gorilla: Large Language Model Connected with Massive APIs", "core_task": {"text": "API call generation for large language models", "query_variants": ["API call generation for large language models", "function calling synthesis in LLMs", "tool invocation creation for large language models"]}, "contributions": [{"name": "LLMs' inability to accurately use tools via API calls", "description": "The paper addresses the challenge that current large language models, including GPT-4, struggle to correctly invoke APIs due to generating inaccurate arguments and hallucinating non-existent API usage, limiting their ability to effectively use external tools.", "author_claim_text": "their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call.", "source_hint": "Abstract"}, {"name": "Enabling LLMs to access dynamic knowledge and computational tools", "description": "The authors are motivated by the potential to transform LLMs into primary interfaces for computing infrastructure by enabling them to access dynamic, large-scale knowledge bases and perform complex tasks through API calls, moving beyond static model weights and limited context.", "author_claim_text": "By empowering LLMs to use tools, we can grant access to vastly larger and changing knowledge bases and accomplish complex computational tasks... This transition from a small set of hand-coded tools, to the ability to invoke a vast space of changing cloud APIs could transform LLMs into the primary interface to computing infrastructure and the web.", "source_hint": "Introduction §1"}, {"name": "Gorilla: retriever-aware finetuned LLaMA model for API calls", "description": "Gorilla is a finetuned LLaMA-7B model trained with retrieval-aware techniques, enabling it to generate accurate API calls and adapt to changing API documentation at test time by incorporating retrieved documents during both training and inference.", "author_claim_text": "We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes", "source_hint": "Abstract"}, {"name": "APIBench: comprehensive dataset of ML API calls", "description": "APIBench is a large-scale dataset containing 1,645 API calls from three major ML model hubs (HuggingFace, TorchHub, TensorHub), with 10 synthetic instruction-API pairs per API generated using self-instruct, totaling over 16,000 training examples.", "author_claim_text": "To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs.", "source_hint": "Abstract and Methodology §3.1"}]}, "abs": "Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at this https URL"}, {"title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Actions of LLM-based Agents/Tool Employment/HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face.pdf", "summary": {"paper_id": "survey-64_paper-5_HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face", "core_task": {"text": "Coordinating large language models with expert models for solving complex AI tasks", "query_variants": ["Coordinating large language models with expert models for solving complex AI tasks", "Integrating expert models with large language models for complex AI tasks", "Combining large language models and expert models to address complex AI problems"]}, "contributions": [{"name": "Handling complex AI tasks across modalities", "description": "The paper addresses the challenge of enabling AI systems to autonomously solve complex tasks that span multiple domains and modalities, which current AI models struggle to achieve.", "author_claim_text": "While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously.", "source_hint": "Abstract"}, {"name": "Advancing artificial general intelligence", "description": "The authors are motivated by the goal of advancing artificial general intelligence by enabling AI systems to handle complex tasks that require coordination across multiple domains and modalities.", "author_claim_text": "Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence.", "source_hint": "Abstract"}, {"name": "HuggingGPT system design", "description": "The authors propose HuggingGPT, a system that uses large language models (LLMs) like ChatGPT as controllers to plan tasks, select models, execute tasks, and generate responses by leveraging AI models from the Hugging Face community.", "author_claim_text": "In this paper, we propose an LLM-powered agent named HuggingGPT to autonomously tackle a wide range of complex AI tasks.", "source_hint": "Introduction §3"}, {"name": "Language as a generic interface", "description": "The authors propose using language as a universal interface to connect LLMs with external AI models, enabling task planning, scheduling, and cooperation through model descriptions.", "author_claim_text": "We introduce a concept: 'Language as a generic interface for LLMs to collaborate with AI models.'", "source_hint": "Introduction §2"}]}, "abs": "Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence."}, {"title": "ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Actions of LLM-based Agents/Tool Employment/ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models.pdf", "summary": {"paper_id": "survey-64_paper-6_ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models", "core_task": {"text": "tool-augmented chain-of-thought reasoning on chat-based large language models", "query_variants": ["tool-augmented chain-of-thought reasoning on chat-based large language models", "chain-of-thought reasoning with external tools in conversational LLMs", "tool-enhanced reasoning chains for dialogue-based large language models"]}, "contributions": [{"name": "Difficulty of LLMs in complex reasoning requiring tools and multi-hop reasoning", "description": "Large language models face challenges when solving complex reasoning tasks that demand specific domain knowledge and multi-step reasoning processes, particularly when external tools are needed for functionalities like arithmetic calculation and information retrieval.", "author_claim_text": "they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning", "source_hint": "Abstract"}, {"name": "Need for unified integration of CoT reasoning and tool manipulation", "description": "Existing approaches either require LLMs to prearrange tool use plans without interaction or frequently switch between reasoning and tool actions, both hurting the continuity of the chain-of-thought reasoning process. A more unified integration is needed for tasks requiring frequent tool invocation.", "author_claim_text": "incorporating external tools would have to interrupt the CoT reasoning process of LLMs. Such an issue would become more intractable on complex reasoning tasks that frequently invoke the use of tools", "source_hint": "Introduction paragraph 3"}, {"name": "ChatCoT framework modeling CoT reasoning as multi-turn conversations", "description": "ChatCoT reformulates chain-of-thought reasoning as a multi-turn conversation where LLMs can freely interact with tools or perform reasoning at each turn, leveraging the multi-turn conversation ability of chat-based LLMs to maintain reasoning continuity while manipulating tools.", "author_claim_text": "we propose ChatCoT, a tool-augmented chain-of-thought reasoning framework for chat-based LLMs. In ChatCoT, we model the chain-of-thought (CoT) reasoning as multi-turn conversations, to utilize tools in a more natural way through chatting", "source_hint": "Abstract"}, {"name": "Conversational knowledge memory initialization", "description": "The approach initializes the conversation with three types of knowledge: tool descriptions, retrieval-augmented task exemplars, and multi-turn reasoning format demonstrations. This conversational memory provides essential context for guiding LLMs in subsequent reasoning steps.", "author_claim_text": "we initialize the early turns of the conversation by the knowledge about tools, tasks, and reasoning format", "source_hint": "Abstract and Section 4.2"}]}, "abs": "Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning. To improve the reasoning abilities, we propose ChatCoT, a tool-augmented chain-of-thought reasoning framework for chat-based LLMs (e.g., ChatGPT). In ChatCoT, we model the chain-of-thought (CoT) reasoning as multi-turn conversations, to utilize tools in a more natural way through chatting. At each turn, LLMs can either interact with tools or perform the reasoning. Our approach can effectively leverage the multi-turn conversation ability of chat-based LLMs, and integrate the thought chain following and tools manipulation in a unified way. Specially, we initialize the early turns of the conversation by the knowledge about tools, tasks, and reasoning format, and propose an iterative tool-augmented reasoning step to perform step-by-step tool-augmented reasoning. The experiment results on two complex reasoning datasets (MATH and HotpotQA) have shown the effectiveness of ChatCoT on complex reasoning tasks, achieving a 7.9% relative improvement over the state-of-the-art baseline. Our code and data are available at: \\url{this https URL}."}, {"title": "Toolformer: Language Models Can Teach Themselves to Use Tools", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Actions of LLM-based Agents/Tool Employment/Toolformer: Language Models Can Teach Themselves to Use Tools.pdf", "summary": {"paper_id": "survey-64_paper-7_Toolformer: Language Models Can Teach Themselves to Use Tools", "core_task": {"text": "teaching language models to use external tools via self-supervised learning", "query_variants": ["teaching language models to use external tools via self-supervised learning", "training LMs to leverage external tools through self-supervised methods", "self-supervised approaches for enabling language models to utilize external tools"]}, "contributions": [{"name": "Language models' inability to perform basic tasks despite strong few-shot abilities", "description": "The paper addresses the paradox that large language models, while excelling at few-shot learning, fail at simple tasks like arithmetic and factual retrieval that smaller specialized models handle easily. These limitations include inability to access recent information, hallucination of facts, difficulties with low-resource languages, lack of mathematical skills, and unawareness of time progression.", "author_claim_text": "Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel.", "source_hint": "Abstract"}, {"name": "Overcoming inherent limitations of language models through external tool use", "description": "The authors are motivated by the need to address fundamental limitations of language models that cannot be solved by scaling alone, such as accessing current information, performing precise calculations, and understanding temporal context. Enabling tool use offers a path to combine the generality of large LMs with the precision of specialized tools.", "author_claim_text": "A simple way to overcome these limitations of today's language models is to give them the ability to use external tools such as search engines, calculators, or calendars.", "source_hint": "Introduction §1"}, {"name": "Self-supervised API call generation and filtering", "description": "The method uses in-context learning to sample potential API calls from a language model, executes these calls, and filters them based on whether they reduce perplexity on future tokens. The model is then finetuned on the filtered API calls, enabling it to learn tool use in a self-supervised manner without human annotations.", "author_claim_text": "We use a self-supervised loss to determine which of these API calls actually help the model in predicting future tokens. Finally, we finetune the LM itself on the API calls that it considers useful.", "source_hint": "Introduction §1 and Approach §2"}, {"name": "Weighted cross-entropy loss for API call filtering", "description": "The filtering mechanism compares the weighted cross-entropy loss when the model receives both API call input and output versus receiving no call or only the input. API calls are retained only if they reduce the loss by at least a threshold, ensuring only genuinely helpful calls are included in training.", "author_claim_text": "We compare two different instantiations of this loss: L+i = Li(e(ci,ri)) and L−i = min(Li(ε),Li(e(ci,ε))). An API call is helpful to M if providing it with both the input and the output of this call makes it easier for the model to predict future tokens.", "source_hint": "Approach §2, Filtering API Calls"}]}, "abs": "Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities."}, {"title": "LLM As DBA", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Actions of LLM-based Agents/Tool Employment/LLM As DBA.pdf", "summary": {"paper_id": "survey-64_paper-8_LLM As DBA", "core_task": {"text": "automated database diagnosis and optimization", "query_variants": ["automated database diagnosis and optimization", "automatic database troubleshooting and tuning", "autonomous database fault detection and performance enhancement"]}, "contributions": [{"name": "Difficulty and tediousness of managing large-scale database instances", "description": "The paper addresses the challenge that human DBAs face significant difficulties in managing massive numbers of database instances, particularly on cloud platforms. Traditional DBAs require years of training, cannot scale to millions of instances, and may not provide timely responses in emergent cases.", "author_claim_text": "it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases)", "source_hint": "Abstract"}, {"name": "Leveraging LLMs to build a human-beyond database administrator", "description": "The authors are motivated by the potential of large language models to understand documents and generate reasonable answers, which can be harnessed to create an automated database administrator that continuously learns from textual sources and provides diagnosis and optimization advice without human limitations.", "author_claim_text": "we aim to build a human-beyond 'DBA' that can tirelessly learn from documents", "source_hint": "Introduction"}, {"name": "Database maintenance knowledge detection from documents and tools", "description": "D-Bot extracts experiential knowledge from database documents by segmenting them into manageable chunks, generating summaries, and using LLM to detect maintenance experience in a structured four-field format (name, content, metrics, steps). It also retrieves and learns to use external tools via matching algorithms.", "author_claim_text": "database maintenance knowledge detection from documents and tools", "source_hint": "Abstract, Section 4, Section 6"}, {"name": "Tree of thought reasoning for root cause analysis", "description": "D-Bot employs a tree search strategy where the LLM iteratively generates and evaluates diagnostic actions. If an action fails, the system can revert to previous steps, using UCT scores to guide node selection and reflection to refine decisions, thereby increasing the likelihood of accurate root cause identification.", "author_claim_text": "tree of thought reasoning for root cause analysis", "source_hint": "Abstract, Section 7"}]}, "abs": "Database administrators (DBAs) play a crucial role in managing, maintaining and optimizing a database system to ensure data availability, performance, and reliability. However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases). Recently large language models (LLMs) have shown great potential to understand valuable documents and accordingly generate reasonable answers. Thus, we propose D-Bot, a LLM-based database administrator that can continuously acquire database maintenance experience from textual sources, and provide reasonable, well-founded, in-time diagnosis and optimization advice for target databases. This paper presents a revolutionary LLM-centric framework for database maintenance, including (i) database maintenance knowledge detection from documents and tools, (ii) tree of thought reasoning for root cause analysis, and (iii) collaborative diagnosis among multiple LLMs. Our preliminary experimental results that D-Bot can efficiently and effectively diagnose the root causes and our code is available at this http URL."}, {"title": "MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Actions of LLM-based Agents/Tool Employment/MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.pdf", "summary": {"paper_id": "survey-64_paper-9_MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning", "core_task": {"text": "Modular neuro-symbolic architecture combining language models with external knowledge and reasoning", "query_variants": ["Modular neuro-symbolic architecture combining language models with external knowledge and reasoning", "Compositional neural-symbolic systems integrating LMs with external knowledge bases and logical reasoning", "Hybrid neuro-symbolic frameworks merging language models with structured knowledge and inference mechanisms"]}, "contributions": [{"name": "Inherent limitations of large language models", "description": "Large language models lack access to current and proprietary information, cannot perform reliable symbolic reasoning (e.g., arithmetic), and suffer from model explosion when fine-tuned for multiple tasks, leading to catastrophic forgetting and infeasibility of repeated retraining.", "author_claim_text": "current LMs have inherent limitations. While versatile and impressive, the output of even huge LMs is in many cases wrong, and often ridiculously so", "source_hint": "Introduction, paragraph 2"}, {"name": "Need for systems approach combining neural and symbolic methods", "description": "The authors aim to retain the versatility and power of large language models while overcoming their shortcomings by integrating external knowledge sources and discrete reasoning modules into a flexible architecture.", "author_claim_text": "the question is how to have our cake and eat it too, enjoying the benefits of self-supervised deep language models without suffering these drawbacks", "source_hint": "Introduction, paragraph 6"}, {"name": "MRKL (Modular Reasoning, Knowledge and Language) system architecture", "description": "A neuro-symbolic architecture consisting of an extendable set of expert modules (both neural and symbolic) and a router that directs natural language inputs to the appropriate module. Modules include general-purpose language models, specialized neural models, and symbolic reasoners like calculators or database APIs.", "author_claim_text": "a flexible architecture dubbed the Modular Reasoning, Knowledge and Language (MRKL, pronounced 'miracle') system", "source_hint": "Abstract and Introduction, paragraph 7"}, {"name": "Data augmentation methodology for training argument extraction", "description": "A systematic approach using template-based generation across multiple dimensions (operand types, number of digits, operation types, question formats) combined with prompt-tuning to train the router to reliably extract discrete parameters from natural language for symbolic modules.", "author_claim_text": "Using a data augmentation methodology, based on generating examples from a structured example space, we show that the static pretrained LM can be made to achieve near perfect performance", "source_hint": "Section 3.2, paragraph 1"}]}, "abs": "Huge language models (LMs) have ushered in a new era for AI, serving as a gateway to natural-language-based knowledge tasks. Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. Conceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we define a flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning modules. We describe this neuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Language (MRKL, pronounced \"miracle\") system, some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs' MRKL system implementation."}, {"title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Actions of LLM-based Agents/Tool Employment/Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models.pdf", "summary": {"paper_id": "survey-64_paper-10_Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models", "core_task": {"text": "compositional reasoning with large language models using external tools", "query_variants": ["compositional reasoning with large language models using external tools", "large language models performing compositional reasoning via external tool integration", "tool-augmented compositional reasoning in large language models"]}, "contributions": [{"name": "Inherent limitations of LLMs in accessing information and using tools", "description": "The paper identifies that large language models cannot access current information from the web or knowledge bases, cannot utilize external tools effectively, and struggle with precise mathematical and logical reasoning tasks.", "author_claim_text": "LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning.", "source_hint": "Abstract"}, {"name": "Need to enhance LLMs with automatic tool composition for real-world tasks", "description": "The authors are motivated by the need to overcome LLM limitations by enabling them to automatically compose and use external tools, which is essential for solving complex real-world problems across diverse domains.", "author_claim_text": "enhancing current LLMs with the capability to automatically compose external tools for real-world task solving is critical to address these drawbacks.", "source_hint": "Introduction §1"}, {"name": "Chameleon: plug-and-play compositional reasoning framework", "description": "Chameleon is a framework that augments LLMs with diverse plug-and-play modules including vision models, web search engines, Python functions, and rule-based modules. It uses an LLM-based planner to generate natural-language-like programs that compose these tools sequentially to solve complex reasoning tasks.", "author_claim_text": "we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools", "source_hint": "Abstract"}, {"name": "LLM-based natural language planner for program synthesis", "description": "The core component is an LLM-based planner that generates natural-language-like programs by selecting and sequencing appropriate tools from the module inventory, prompted by tool descriptions and usage examples without requiring training or domain-specific prompts.", "author_claim_text": "At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response.", "source_hint": "Abstract and Introduction §1"}]}, "abs": "Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks. At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response. We showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54% overall accuracy on ScienceQA, improving the best published few-shot result by 11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%, lifting the state of the art to 98.78%. Our analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner. The project is available at this https URL."}, {"title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Actions of LLM-based Agents/Tool Planning/ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs.pdf", "summary": {"paper_id": "survey-64_paper-11_ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs", "core_task": {"text": "tool use and API mastery for large language models", "query_variants": ["tool use and API mastery for large language models", "API integration and tool utilization in LLMs", "large language model proficiency in tool usage and API calls"]}, "contributions": [{"name": "Limited tool-use capabilities in open-source LLMs", "description": "Open-source large language models like LLaMA remain significantly limited in their ability to use external tools (APIs) to fulfill human instructions, unlike closed-source models such as ChatGPT. Current instruction tuning focuses on basic language tasks and neglects the tool-use domain.", "author_claim_text": "current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs", "source_hint": "Abstract"}, {"name": "Democratizing tool-use capabilities for open-source LLMs", "description": "The authors are motivated by the need to bridge the gap between open-source and closed-source LLMs in tool-use capabilities, enabling democratization of AI technologies and facilitating community-driven innovation by making tool-use abilities accessible in open models.", "author_claim_text": "we deem it urgent to empower open-source LLMs to skillfully master diverse APIs", "source_hint": "Introduction §1"}, {"name": "ToolBench instruction-tuning dataset construction", "description": "The authors construct ToolBench through three stages: collecting 16,464 real-world RESTful APIs from RapidAPI Hub spanning 49 categories, prompting ChatGPT to generate diverse instructions covering single-tool and multi-tool scenarios, and annotating solution paths using ChatGPT to search for valid API call chains.", "author_claim_text": "We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT", "source_hint": "Abstract"}, {"name": "Depth-first search-based decision tree (DFSDT)", "description": "A novel reasoning strategy that constructs a decision tree allowing LLMs to assess different reasoning paths and choose to either proceed along promising paths or abandon existing nodes and expand new ones. This approach expands the search space beyond conventional ReACT or CoT methods.", "author_claim_text": "we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space", "source_hint": "Abstract"}]}, "abs": "Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench."}, {"title": "TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Actions of LLM-based Agents/Tool Planning/TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage.pdf", "summary": {"paper_id": "survey-64_paper-12_TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage", "core_task": {"text": "Task planning and tool usage with large language models", "query_variants": ["Task planning and tool usage with large language models", "Large language models for task decomposition and tool utilization", "Tool selection and planning using LLMs"]}, "contributions": [{"name": "Insufficient intrinsic generative abilities of LLMs for complex tasks", "description": "Large Language Models alone cannot effectively handle complex real-world tasks that require both strategic task decomposition and the ability to interact with external tools and resources beyond their internal knowledge.", "author_claim_text": "the intrinsic generative abilities of LLMs may prove insufficient for handling complex tasks, which necessitate a combination of task planning and the usage of external tools", "source_hint": "Abstract"}, {"name": "Need for structured evaluation of LLM-based AI agents", "description": "The authors aim to create a systematic framework and evaluation methodology to help researchers and practitioners understand how to effectively deploy LLMs as AI agents that can plan tasks and use tools in practical applications.", "author_claim_text": "our goal is to provide a helpful resource for researchers and practitioners to leverage the power of LLMs in their AI applications", "source_hint": "Abstract"}, {"name": "Structured framework for LLM-based AI Agents with six components", "description": "The framework consists of task instruction, designed prompt, tool set, LLM core, intermediate output, and final answer. It systematically organizes how LLMs perceive tasks, plan solutions, select and use tools, and generate responses.", "author_claim_text": "we design our AI Agent framework with six components", "source_hint": "Method §2.1"}, {"name": "Two agent types: One-step Agent (TPTU-OA) and Sequential Agent (TPTU-SA)", "description": "TPTU-OA adopts a global perspective to break down problems into sub-tasks all at once, while TPTU-SA tackles sub-tasks sequentially with continuous feedback. These represent two different problem-solving strategies for task planning and tool usage.", "author_claim_text": "we have devised two distinct types of AI agents", "source_hint": "Method §2.3"}]}, "abs": "With recent advancements in natural language processing, Large Language Models (LLMs) have emerged as powerful tools for various real-world applications. Despite their prowess, the intrinsic generative abilities of LLMs may prove insufficient for handling complex tasks which necessitate a combination of task planning and the usage of external tools. In this paper, we first propose a structured framework tailored for LLM-based AI Agents and discuss the crucial capabilities necessary for tackling intricate problems. Within this framework, we design two distinct types of agents (i.e., one-step agent and sequential agent) to execute the inference process. Subsequently, we instantiate the framework using various LLMs and evaluate their Task Planning and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings and challenges, our goal is to provide a helpful resource for researchers and practitioners to leverage the power of LLMs in their AI applications. Our study emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement."}, {"title": "Gentopia: A Collaborative Platform for Tool-Augmented LLMs", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Actions of LLM-based Agents/Tool Planning/Gentopia: A Collaborative Platform for Tool-Augmented LLMs.pdf", "summary": {"paper_id": "survey-64_paper-13_Gentopia: A Collaborative Platform for Tool-Augmented LLMs", "core_task": {"text": "Building and evaluating tool-augmented large language model agents", "query_variants": ["Building and evaluating tool-augmented large language model agents", "Constructing and assessing LLM agents enhanced with external tools", "Development and evaluation of tool-integrated large language model agents"]}, "contributions": [{"name": "Deficiencies in existing ALM frameworks", "description": "Current frameworks for augmented language models lack three key capabilities: the ability to flexibly customize agents without redundant implementation, a platform for sharing and collaborating user-customized agents, and comprehensive benchmarks for evaluating agent quality across multiple dimensions like safety and robustness.", "author_claim_text": "most existing frameworks for ALMs, to varying degrees, are deficient in the following critical features: flexible customization, collaborative democratization, and holistic evaluation", "source_hint": "Abstract"}, {"name": "Enabling collective growth through agent specialization and collaboration", "description": "The authors are motivated by the vision that specialized agents, when shared and composed together, can democratize AI development. Improvements to individual agents benefit the entire ecosystem, enabling collaborative advancement toward more capable and reliable intelligent systems.", "author_claim_text": "we believe the collaboration between user-customized agents can contribute to the democratization of AI... A single step to improve agent capability and efficiency gives plural contributions to interacted agents, fostering collective growth toward greater intelligence", "source_hint": "Introduction and Section 3.1"}, {"name": "Gentopia framework for agent assembly via YAML configuration", "description": "Gentopia is a lightweight framework that enables users to define agents through simple YAML configuration files. It unifies various language models, task formats, prompting modules, and plugins into a single paradigm, supporting flexible customization without boilerplate code.", "author_claim_text": "Gentopia allows practitioners to customize an agent with a single configuration file, greatly simplifying the process of building, tuning, sharing, and evaluating agents", "source_hint": "Abstract and Section 3.2"}, {"name": "GentPool platform for agent registration and sharing", "description": "GentPool is a public platform where users can register, share, and discover specialized agents. Registered agents are composable, allowing hierarchical assembly for multi-agent collaboration, and each agent is accompanied by a descriptive Wiki page.", "author_claim_text": "we establish GentPool, a public platform enabling the registration and sharing of user-customized agents. Agents registered in GentPool are composable such that they can be assembled together for agent collaboration", "source_hint": "Abstract and Section 3.1"}]}, "abs": "Augmented Language Models (ALMs) empower large language models with the ability to use tools, transforming them into intelligent agents for real-world interactions. However, most existing frameworks for ALMs, to varying degrees, are deficient in the following critical features: flexible customization, collaborative democratization, and holistic evaluation. We present gentopia, an ALM framework enabling flexible customization of agents through simple configurations, seamlessly integrating various language models, task formats, prompting modules, and plugins into a unified paradigm. Furthermore, we establish gentpool, a public platform enabling the registration and sharing of user-customized agents. Agents registered in gentpool are composable such that they can be assembled together for agent collaboration, advancing the democratization of artificial intelligence. To ensure high-quality agents, gentbench, an integral component of gentpool, is designed to thoroughly evaluate user-customized agents across diverse aspects such as safety, robustness, efficiency, etc. We release gentopia on Github and will continuously move forward."}, {"title": "A Survey of Knowledge Graph Embedding and Their Applications", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Memory Capability of LLM-based Agents/Long-term Memory/A Survey of Knowledge Graph Embedding and Their Applications.pdf", "summary": {"paper_id": "survey-64_paper-14_A Survey of Knowledge Graph Embedding and Their Applications", "core_task": {"text": "knowledge graph embedding and representation learning", "query_variants": ["knowledge graph embedding and representation learning", "representation learning for knowledge graphs", "embedding methods for knowledge graph representations"]}, "contributions": [{"name": "Difficulty of consuming structured knowledge graph information in real-world applications", "description": "Knowledge graphs represent structured relational information as entities and relations, but this representation is difficult to directly utilize in practical applications. The challenge lies in converting the structured triple format into a form that can be easily manipulated and consumed by real-world systems.", "author_claim_text": "The information embedded in Knowledge graph though being structured is challenging to consume in a real-world application.", "source_hint": "Abstract"}, {"name": "Enabling real-world applications through numerical representation of knowledge", "description": "The authors are motivated by the need to make knowledge graph information accessible to real-world applications such as recommender systems, question answering, and information extraction. Converting structured knowledge into continuous vector representations enables easier manipulation and integration into these applications.", "author_claim_text": "The consumption of information enshrined in the knowledge graph will be very easy if it can be converted to numerical representation. Knowledge graph embedding is a solution to incorporate the knowledge from the knowledge graph in a real-world application.", "source_hint": "Introduction paragraph 2"}, {"name": "Translation-based embedding models", "description": "Translation models represent entities and relations in low-dimensional vector spaces where relationships are modeled as translations. Starting with TransE, these models evolved through TransH, TransR, TransD, RotatE, and HakE to address limitations in handling different relation types (one-to-many, symmetric, hierarchical) and semantic properties.", "author_claim_text": "The translation-based model uses distance-based measures to generate the similarity score for a pair of entities and their relationships. The translation-based model aims to find a vector representation of entities with relation to the translation of the entities.", "source_hint": "Section 2.1"}, {"name": "Semantic matching embedding models", "description": "Semantic matching models employ similarity-based scoring functions rather than distance-based measures. These include tensor factorization approaches (RESCAL, TATEC), simplified bilinear models (DistMult), compositional representations (HolE), complex embeddings (ComplEx), and analogy-based models (ANALOGY) that capture different types of relational patterns.", "author_claim_text": "Semantic Matching Models use similarity-based scoring function.", "source_hint": "Section 2.2"}]}, "abs": "Knowledge Graph embedding provides a versatile technique for representing knowledge. These techniques can be used in a variety of applications such as completion of knowledge graph to predict missing information, recommender systems, question answering, query expansion, etc. The information embedded in Knowledge graph though being structured is challenging to consume in a real-world application. Knowledge graph embedding enables the real-world application to consume information to improve performance. Knowledge graph embedding is an active research area. Most of the embedding methods focus on structure-based information. Recent research has extended the boundary to include text-based information and image-based information in entity embedding. Efforts have been made to enhance the representation with context information. This paper introduces growth in the field of KG embedding from simple translation-based models to enrichment-based models. This paper includes the utility of the Knowledge graph in real-world applications."}, {"title": "Dynamic Planning with a LLM", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Planning Capability of LLM-based Agents/External Methods/Dynamic Planning with a LLM.pdf", "summary": {"paper_id": "survey-64_paper-15_Dynamic Planning with a LLM", "core_task": {"text": "Embodied task planning with language models and symbolic planners", "query_variants": ["Embodied task planning with language models and symbolic planners", "Language model integration with symbolic planning for embodied tasks", "Combining LLMs and symbolic reasoning for embodied agent planning"]}, "contributions": [{"name": "LLM limitations in embodied multi-step planning tasks", "description": "The paper addresses the challenge that while LLMs excel at many NLP tasks, they struggle with embodied agent applications requiring complex multi-step reasoning due to context window limitations, high computational costs, hallucinations, and brittleness to prompt phrasing.", "author_claim_text": "applications involving embodied agents remain problematic. In particular, complex plans that require multi-step reasoning become difficult and too costly as the context window grows.", "source_hint": "Abstract"}, {"name": "Combining LLM flexibility with symbolic planner efficiency", "description": "The authors are motivated by the complementary strengths of symbolic planners (which find optimal solutions efficiently but require complete information) and LLMs (which handle noisy observations and uncertainty but are costly and limited by context windows), seeking to integrate both approaches.", "author_claim_text": "While symbolic planners find optimal solutions quickly, they require a complete and accurate representation of the planning problem, severely limiting their use in practical scenarios. In contrast, modern LLMs cope with noisy observations and high levels of uncertainty when reasoning about a task.", "source_hint": "Abstract"}, {"name": "LLM Dynamic Planner (LLM-DP) neuro-symbolic framework", "description": "LLM-DP is a closed-loop agent framework that integrates an LLM with a symbolic planner. The LLM translates natural language tasks into PDDL goals and samples plausible world states from beliefs about unknown predicates, while a symbolic planner (BFS(f)) generates optimal action sequences from these sampled states.", "author_claim_text": "Our work presents LLM Dynamic Planner (LLM-DP): a neuro-symbolic framework where an LLM works hand-in-hand with a traditional planner to solve an embodied task.", "source_hint": "Abstract and Introduction"}, {"name": "Belief sampling for incomplete world states", "description": "The method maintains a world state W (known information) and beliefs B (unknown predicates). It uses the LLM to sample plausible instantiations of unknown predicates, generating multiple likely world states that are then passed to the symbolic planner to find valid plans.", "author_claim_text": "Since B includes possible predicates which are unknown, we sample from B using an LLM to obtain wbelief. This sampling leverages the LLM to complete a world state and is extendable to any unknown predicate from which a set of beliefs can be deduced.", "source_hint": "Section 4.3"}]}, "abs": "While Large Language Models (LLMs) can solve many NLP tasks in zero-shot settings, applications involving embodied agents remain problematic. In particular, complex plans that require multi-step reasoning become difficult and too costly as the context window grows. Planning requires understanding the likely effects of one's actions and identifying whether the current environment satisfies the goal state. While symbolic planners find optimal solutions quickly, they require a complete and accurate representation of the planning problem, severely limiting their use in practical scenarios. In contrast, modern LLMs cope with noisy observations and high levels of uncertainty when reasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a neuro-symbolic framework where an LLM works hand-in-hand with a traditional planner to solve an embodied task. Given action-descriptions, LLM-DP solves Alfworld faster and more efficiently than a naive LLM ReAct baseline."}, {"title": "Reasoning with Language Model is Planning with World Model", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Planning Capability of LLM-based Agents/External Methods/Reasoning with Language Model is Planning with World Model.pdf", "summary": {"paper_id": "survey-64_paper-16_Reasoning with Language Model is Planning with World Model", "core_task": {"text": "reasoning with language models through strategic planning", "query_variants": ["reasoning with language models through strategic planning", "strategic planning approaches for reasoning in language models", "language model reasoning via strategic planning methods"]}, "contributions": [{"name": "LLMs lack internal world model for deliberate planning", "description": "Current large language models cannot perform deliberate planning similar to human reasoning because they lack an internal world model to predict future states and simulate action outcomes, preventing them from exploring alternative reasoning paths and refining steps iteratively.", "author_claim_text": "The deficiency stems from the key fact that LLMs lack an internal world model to predict the world state (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions.", "source_hint": "Abstract"}, {"name": "Enable human-like strategic planning in LLMs", "description": "The authors are motivated by how humans use internal world models for deliberate planning—exploring alternatives, assessing outcomes, and refining plans—which current LLMs cannot replicate, limiting their reasoning on tasks that are easy for humans.", "author_claim_text": "Humans possess an internal world model, a mental representation of the environment, which enables humans to simulate actions and their effects on the world's state for deliberate planning for complex tasks of motor control, imagery, inference, and decision making.", "source_hint": "Introduction §1"}, {"name": "Reasoning via Planning (RAP) framework", "description": "RAP is a framework that repurposes the same LLM to serve as both a world model (predicting state transitions) and a reasoning agent, integrating Monte Carlo Tree Search to balance exploration and exploitation in the reasoning space, enabling strategic planning.", "author_claim_text": "To overcome the limitations, we propose a new LLM reasoning framework, Reasoning via Planning (RAP). RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm based on Monte Carlo Tree Search for strategic exploration in the vast reasoning space.", "source_hint": "Abstract"}, {"name": "World model via LLM repurposing with state-action formulation", "description": "The authors repurpose the LLM with prompts to act as a world model that predicts next states given current states and actions, enabling the LLM to simulate reasoning traces as sequences of interleaved states and actions rather than actions alone.", "author_claim_text": "We acquire the world model by repurposing the LLM itself with appropriate prompts. During the reasoning, the LLM strategically builds a reasoning tree by iteratively considering the most promising reasoning steps (actions) and using the world model (the same, repurposed LLM) to look ahead for future outcomes.", "source_hint": "Introduction §1, Section 3.1"}]}, "abs": "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting."}, {"title": "LLM+P: Empowering Large Language Models with Optimal Planning Proficiency", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Planning Capability of LLM-based Agents/External Methods/LLM+P: Empowering Large Language Models with Optimal Planning Proficiency.pdf", "summary": {"paper_id": "survey-64_paper-17_LLM+P: Empowering Large Language Models with Optimal Planning Proficiency", "core_task": {"text": "integrating large language models with classical planners for robot task planning", "query_variants": ["integrating large language models with classical planners for robot task planning", "combining LLMs and traditional planning algorithms for robotic task execution", "fusion of large language models with symbolic planners in robot planning"]}, "contributions": [{"name": "LLMs cannot reliably solve long-horizon robot planning problems", "description": "The paper identifies that while large language models demonstrate impressive zero-shot generalization for common questions, they fail to produce correct solutions for complex planning problems that require long-horizon reasoning and understanding of preconditions and world dynamics.", "author_claim_text": "so far, LLMs cannot reliably solve long-horizon robot planning problems", "source_hint": "Abstract"}, {"name": "Combining strengths of LLMs and classical planners", "description": "The authors are motivated by the observation that classical planners can efficiently find correct or optimal solutions once a problem is properly formatted, while LLMs excel at natural language understanding. They aim to leverage both capabilities without modifying the LLMs themselves.", "author_claim_text": "In an effort to get the best of both worlds, this paper introduces LLM+P, the first framework that incorporates the strengths of classical planners into LLMs", "source_hint": "Abstract"}, {"name": "LLM+P framework for planning", "description": "The proposed LLM+P method uses an LLM to translate natural language problem descriptions into PDDL format, invokes a classical planner to find optimal solutions, and then translates the PDDL plan back to natural language. The approach assumes a domain PDDL file is provided and uses in-context learning with a simple example problem-PDDL pair.", "author_claim_text": "LLM+P takes in a natural language description of a planning problem, then returns a correct (or optimal) plan for solving that problem in natural language. LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL), then leveraging classical planners to quickly find a solution, and then translating the found solution back into natural language", "source_hint": "Abstract and Section III-C"}, {"name": "In-context learning for PDDL generation", "description": "The method leverages in-context learning by providing the LLM with an example pair of a natural language problem description and its corresponding PDDL file. This enables the LLM to generalize and correctly translate new problem descriptions into valid PDDL format without fine-tuning.", "author_claim_text": "By in-context learning, we mean LLMs' ability to perform unseen downstream tasks by simply conditioning on a few input-label pairs (demonstrations)", "source_hint": "Section III-B"}]}, "abs": "Large language models (LLMs) have demonstrated remarkable zero-shot generalization abilities: state-of-the-art chatbots can provide plausible answers to many common questions that arise in daily life. However, so far, LLMs cannot reliably solve long-horizon planning problems. By contrast, classical planners, once a problem is given in a formatted way, can use efficient search algorithms to quickly identify correct, or even optimal, plans. In an effort to get the best of both worlds, this paper introduces LLM+P, the first framework that incorporates the strengths of classical planners into LLMs. LLM+P takes in a natural language description of a planning problem, then returns a correct (or optimal) plan for solving that problem in natural language. LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL), then leveraging classical planners to quickly find a solution, and then translating the found solution back into natural language. Along with LLM+P, we define a diverse set of different benchmark problems taken from common planning scenarios. Via a comprehensive set of experiments on these benchmark problems, we find that LLM+P is able to provide optimal solutions for most problems, while LLMs fail to provide even feasible plans for most problems.\\footnote{The code and results are publicly available at this https URL."}, {"title": "Context-Aware Composition of Agent Policies by Markov Decision Process Entity Embeddings and Agent Ensembles", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Planning Capability of LLM-based Agents/External Methods/Context-Aware Composition of Agent Policies by Markov Decision Process Entity Embeddings and Agent Ensembles.pdf", "summary": {"paper_id": "survey-64_paper-18_Context-Aware Composition of Agent Policies by Markov Decision Process Entity Embeddings and Agent Ensembles", "core_task": {"text": "context-aware composition of agent policies by entity embeddings and agent ensembles", "query_variants": ["context-aware composition of agent policies by entity embeddings and agent ensembles", "entity embedding and ensemble methods for context-sensitive agent policy composition", "composing agent policies using entity representations and multi-agent ensembles with contextual awareness"]}, "contributions": [{"name": "Time-consuming policy learning for agents in heterogeneous contexts", "description": "Computational agents in dynamically changing environments with large state and action spaces require lengthy training procedures to learn context-dependent policies via reinforcement learning, which prevents on-the-fly integration and immediate performance of activities across multiple contexts.", "author_claim_text": "training policies for all possible contexts using reinforcement learning is time-consuming. A requirement and challenge for agents is to learn strategies quickly and respond immediately in cross-context environments", "source_hint": "Abstract"}, {"name": "Enable immediate cross-context agent operation without lengthy training", "description": "The authors aim to allow agents to seamlessly switch between different contexts and perform activities without time-consuming training episodes, enabling rapid deployment in heterogeneous environments such as service robotics and cyber-physical systems.", "author_claim_text": "To enable agents to operate immediately in individual, multi-context environments, we propose a simulation-based approach", "source_hint": "Introduction §1"}, {"name": "MDP knowledge graph generation from activity datasets", "description": "The approach introduces an ontology defining concepts (Activity, State, Action, Transition, Effect) and properties for automatically deriving Markov Decision Process knowledge graphs from operational datasets, capturing state transitions, actions, and observation features.", "author_claim_text": "A light-weight ontology and method that allows the automated generation of MDP knowledge graphs from activity datasets that reproduce MDPs", "source_hint": "Abstract (C1), Section 5.2-5.3"}, {"name": "MDP entity embeddings for context representation", "description": "Entity embedding vectors are trained using deep neural networks to numerically represent activities, states, and actions in an n-dimensional space, capturing semantic relationships and enabling fast identification of contextually appropriate actions through spatial proximity.", "author_claim_text": "An approach for training MDP-related entity embeddings in order to constrain the different contexts and uncover semantic relatedness as well as similarities between MDP entities", "source_hint": "Abstract (C2), Section 5.4"}]}, "abs": "Computational agents support humans in many areas of life and are therefore found in heterogeneous contexts. This means they operate in rapidly changing environments and can be confronted with huge state and action spaces. In order to perform services and carry out activities in a goal-oriented manner, agents require prior knowledge and therefore have to develop and pursue context-dependent policies. However, prescribing policies in advance is limited and inflexible, especially in dynamically changing environments. Moreover, the context of an agent determines its choice of actions. Since the environments can be stochastic and complex in terms of the number of states and feasible actions, activities are usually modelled in a simplified way by Markov decision processes so that, e.g., agents with reinforcement learning are able to learn policies, that help to capture the context and act accordingly to optimally perform activities. However, training policies for all possible contexts using reinforcement learning is time-consuming. A requirement and challenge for agents is to learn strategies quickly and respond immediately in cross-context environments and applications, e.g., the Internet, service robotics, cyber-physical systems. In this work, we propose a novel simulation-based approach that enables a) the representation of heterogeneous contexts through knowledge graphs and entity embeddings and b) the context-aware composition of policies on demand by ensembles of agents running in parallel. The evaluation we conducted with the \"Virtual Home\" dataset indicates that agents with a need to switch seamlessly between different contexts, can request on-demand composed policies that lead to the successful completion of context-appropriate activities without having to learn these policies in lengthy training steps and episodes, in contrast to agents that use reinforcement learning."}, {"title": "Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Planning Capability of LLM-based Agents/External Methods/Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis.pdf", "summary": {"paper_id": "survey-64_paper-19_Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis", "core_task": {"text": "integrating large language models and cognitive architectures for robust AI systems", "query_variants": ["integrating large language models and cognitive architectures for robust AI systems", "combining LLMs with cognitive architectures to build reliable artificial intelligence systems", "merging large language models and cognitive frameworks for dependable AI agents"]}, "contributions": [{"name": "Limitations of LLMs and CAs in isolation", "description": "The paper identifies that LLMs suffer from interpretability, consistency, scalability, hallucinations, and biases, while CAs struggle with limited and domain-specific knowledge representation, inability to handle complex knowledge, and reliance on manually curated procedural knowledge.", "author_claim_text": "LLMs face interpretability, consistency, and scalability issues partly due to limitations in context window size and sensitivity to prompt structure. CAs face challenges in knowledge representation and scalability. Their encoded information is limited in size and homogeneous typology.", "source_hint": "Introduction paragraphs 1-2"}, {"name": "Synergistic integration to harness complementary strengths", "description": "The authors are motivated by the observation that LLMs and CAs have complementary strengths and weaknesses. Integrating them synergistically could enable more robust AI systems by mitigating each approach's limitations while leveraging their respective advantages.", "author_claim_text": "Both LLMs and CAs have made valuable and sound contributions to the construction of complex autonomous AI agents; however, each approach has its strengths and weaknesses. Thus, the main contribution of this work lies in characterizing the plausible approaches to integrating CAs and LLMs, viewing them through a hybrid and synergetic lens.", "source_hint": "Introduction paragraph 3"}, {"name": "Modular approach with four integration models", "description": "The modular approach proposes four integration models: (a) cognitively-augmented LLM using chain-of-thought prompting with CA-injected reasoning traces, (b) CA with LLM-powered perception and motor modules, (c) CA with multiple LLM-driven modules including memories, and (d) CA using LLMs for internal simulation and anticipation.", "author_claim_text": "The modular approach, which introduces four models with varying degrees of integration, makes use of chain-of-thought prompting, and draws inspiration from augmented LLMs, the Common Model of Cognition, and the simulation theory of cognition.", "source_hint": "Abstract; Modular Approach section"}, {"name": "Agency approach at micro and macro levels", "description": "The agency approach operates at two levels: micro-level where specialized agents (LLM-based or symbolic) compete for resources and form coalitions within a CA, and macro-level where cognitive agents interact with other agents and humans for collaborative goal achievement.", "author_claim_text": "The agency approach, motivated by the Society of Mind theory and the LIDA cognitive architecture, proposes the formation of agent collections that interact at micro and macro cognitive levels, driven by either LLMs or symbolic components.", "source_hint": "Abstract; Agency Approach section"}]}, "abs": "This paper explores the integration of two AI subdisciplines employed in the development of artificial agents that exhibit intelligent behavior: Large Language Models (LLMs) and Cognitive Architectures (CAs). We present three integration approaches, each grounded in theoretical models and supported by preliminary empirical evidence. The modular approach, which introduces four models with varying degrees of integration, makes use of chain-of-thought prompting, and draws inspiration from augmented LLMs, the Common Model of Cognition, and the simulation theory of cognition. The agency approach, motivated by the Society of Mind theory and the LIDA cognitive architecture, proposes the formation of agent collections that interact at micro and macro cognitive levels, driven by either LLMs or symbolic components. The neuro-symbolic approach, which takes inspiration from the CLARION cognitive architecture, proposes a model where bottom-up learning extracts symbolic representations from an LLM layer and top-down guidance utilizes symbolic representations to direct prompt engineering in the LLM layer. These approaches aim to harness the strengths of both LLMs and CAs, while mitigating their weaknesses, thereby advancing the development of more robust AI systems. We discuss the tradeoffs and challenges associated with each approach."}, {"title": "Self-Refine: Iterative Refinement with Self-Feedback", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Planning Capability of LLM-based Agents/In-Context Learning Methods/Self-Refine: Iterative Refinement with Self-Feedback.pdf", "summary": {"paper_id": "survey-64_paper-20_Self-Refine: Iterative Refinement with Self-Feedback", "core_task": {"text": "iterative refinement with self-feedback for language model outputs", "query_variants": ["iterative refinement with self-feedback for language model outputs", "self-critique and iterative improvement of LM generation", "language model output refinement through self-evaluation loops"]}, "contributions": [{"name": "Suboptimal initial outputs from large language models", "description": "Large language models often produce intelligible initial outputs but fall short in addressing intricate requirements or multifaceted objectives, particularly in tasks with hard-to-define goals such as dialogue generation or code readability improvement.", "author_claim_text": "large language models (LLMs) do not always generate the best output on their first try", "source_hint": "Abstract, sentence 1"}, {"name": "Need for refinement without extensive supervision", "description": "Existing refinement methods require domain-specific training data, external supervision, or expensive human annotations. The authors are motivated to develop an approach that can refine LLM outputs across various tasks without these costly requirements, inspired by how humans iteratively refine their own work through self-feedback.", "author_claim_text": "Iterative refinement typically involves training a refinement model that relies on domain-specific data... Other approaches that rely on external supervision or reward models require large training sets or expensive human annotations... which may not always be feasible to obtain. These limitations underscore the need for an effective refinement approach that can be applied to various tasks without requiring extensive supervision.", "source_hint": "Introduction, paragraph 1"}, {"name": "SELF-REFINE iterative feedback and refinement algorithm", "description": "SELF-REFINE uses a single LLM to generate an initial output, then passes it back to the same model to obtain feedback on that output, and finally uses the feedback to refine the output. This process iterates until a stopping condition is met, using few-shot prompting to guide the model through feedback generation and refinement steps.", "author_claim_text": "We present SELF-REFINE: an iterative self-refinement algorithm that alternates between two generative steps–FEEDBACK and REFINE. These steps work in tandem to generate high-quality outputs.", "source_hint": "Introduction, paragraph 4; Section 2"}, {"name": "Actionable and specific feedback generation", "description": "The feedback mechanism is designed to produce actionable suggestions (concrete actions for improvement) and specific critiques (identifying exact phrases or elements to change), rather than generic comments, enabling effective refinement in subsequent iterations.", "author_claim_text": "We prompt the model to write feedback that is actionable and specific via fb(k). By 'actionable', we mean the feedback should contain a concrete action that would likely improve the output. By 'specific', we mean the feedback should identify concrete phrases in the output to change.", "source_hint": "Section 2, FEEDBACK paragraph"}]}, "abs": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach."}, {"title": "Complexity-Based Prompting for Multi-Step Reasoning", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Planning Capability of LLM-based Agents/In-Context Learning Methods/Complexity-Based Prompting for Multi-Step Reasoning.pdf", "summary": {"paper_id": "survey-64_paper-21_Complexity-Based Prompting for Multi-Step Reasoning", "core_task": {"text": "prompting large language models for multi-step reasoning", "query_variants": ["prompting large language models for multi-step reasoning", "eliciting multi-turn inference from large language models through prompts", "prompt-based multi-step logical reasoning in large language models"]}, "contributions": [{"name": "Example selection for chain-of-thoughts multi-step reasoning", "description": "The paper addresses the challenge of identifying which reasoning examples make the most effective prompts for multi-step reasoning tasks when using chain-of-thoughts prompting with large language models.", "author_claim_text": "This work studies example selection in chain-of-thoughts multi-step reasoning. Example selection is a central problem in the prompting literature. It asks what instances make the best prompts for solving the tasks of interest.", "source_hint": "Introduction paragraph 2"}, {"name": "Annotation efficiency and performance improvement in CoT prompting", "description": "The authors are motivated by the need to reduce annotation costs while improving reasoning performance, since chain-of-thoughts prompting requires manually annotated reasoning chains and identifying the best examples to annotate can save resources.", "author_claim_text": "For CoT prompting, example selection is further related to annotation efficiency, as CoT requires manually-annotated reasoning chains. For datasets where reasoning annotations are easy to obtain, one may want to know which annotated chains make the best prompt; if the annotations are hard to obtain, one may identify the best cases to annotate, rather than annotating the entire dataset.", "source_hint": "Introduction paragraph 2"}, {"name": "Complexity-based prompting", "description": "The method selects examples with more complex reasoning chains (more reasoning steps) as prompts for the language model, hypothesizing that complex instances subsume simpler ones and lead to better reasoning performance.", "author_claim_text": "We propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multi-step reasoning tasks over strong baselines.", "source_hint": "Abstract"}, {"name": "Complexity-based consistency", "description": "This method extends complexity-based selection to the output space by sampling multiple reasoning chains and taking the majority vote among the top K most complex generated chains rather than all chains, promoting outputs with more complex reasoning.", "author_claim_text": "We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains).", "source_hint": "Abstract and Section 3.2"}]}, "abs": "We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multi-step reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift."}, {"title": "Automatic Chain of Thought Prompting in Large Language Models", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Planning Capability of LLM-based Agents/In-Context Learning Methods/Automatic Chain of Thought Prompting in Large Language Models.pdf", "summary": {"paper_id": "survey-64_paper-22_Automatic Chain of Thought Prompting in Large Language Models", "core_task": {"text": "Automatic construction of chain-of-thought prompting demonstrations for large language models", "query_variants": ["Automatic construction of chain-of-thought prompting demonstrations for large language models", "Automated generation of reasoning chain examples for LLM prompting", "Constructing step-by-step reasoning demonstrations automatically for large language models"]}, "contributions": [{"name": "Manual effort required for designing task-specific CoT demonstrations", "description": "Existing Manual-CoT methods require nontrivial human effort to hand-craft effective demonstrations, including designing both questions and their reasoning chains. Different tasks require different demonstration designs, making this process labor-intensive and task-specific.", "author_claim_text": "The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated", "source_hint": "Abstract"}, {"name": "Eliminating manual demonstration design while maintaining performance", "description": "The authors aim to remove the burden of manually designing demonstrations for CoT prompting while achieving competitive performance. This would make CoT reasoning more practical and scalable across different tasks without requiring human annotation efforts.", "author_claim_text": "To eliminate such manual designs, we advocate another Auto-CoT paradigm to automatically construct demonstrations with questions and reasoning chains", "source_hint": "Introduction §1"}, {"name": "Auto-CoT: Automatic Chain-of-Thought prompting with diversity-based sampling", "description": "Auto-CoT consists of two main stages: (i) partitioning questions into clusters using k-means to ensure diversity, and (ii) selecting representative questions from each cluster and generating reasoning chains using Zero-Shot-CoT with simple heuristics (e.g., preferring shorter questions and rationales).", "author_claim_text": "We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations", "source_hint": "Abstract and Section 4"}, {"name": "Diversity-based question clustering to mitigate misleading by similarity", "description": "The method uses clustering to sample diverse demonstration questions rather than similarity-based retrieval. This diversity helps avoid the 'misleading by similarity' problem where similar questions with Zero-Shot-CoT errors would propagate mistakes to test questions.", "author_claim_text": "To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations", "source_hint": "Abstract and Section 3.3"}]}, "abs": "Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like \"Let's think step by step\" to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the \"Let's think step by step\" prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at this https URL"}, {"title": "Chain-of-Thought in Large Language Models: Decoding, Projection, and Activation", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Planning Capability of LLM-based Agents/In-Context Learning Methods/Chain-of-Thought in Large Language Models: Decoding, Projection, and Activation.pdf", "summary": {"paper_id": "survey-64_paper-23_Chain-of-Thought in Large Language Models: Decoding, Projection, and Activation", "core_task": {"text": "chain-of-thought prompting mechanisms in large language models", "query_variants": ["chain-of-thought prompting mechanisms in large language models", "step-by-step reasoning prompts for LLMs", "intermediate reasoning elicitation methods in large language models"]}, "contributions": [{"name": "Limited understanding of Chain-of-Thought operational mechanisms in LLMs", "description": "Despite Chain-of-Thought prompting significantly enhancing reasoning capabilities of large language models, the specific internal and external changes occurring within models when employing CoT remain unclear, creating a knowledge gap in understanding how CoT actually functions.", "author_claim_text": "However, the underlying mechanisms remain poorly understood. To further demystify the operational principles, this work examines three key aspects: decoding, projection, and activation", "source_hint": "Abstract"}, {"name": "Need to understand internal model changes with CoT prompting", "description": "The authors are motivated to fill the gap in understanding what specific changes occur both internally and externally within models when using CoT, as existing hypotheses lack comprehensive experimental validation and mechanistic insights.", "author_claim_text": "This paper aims to address this knowledge gap by examining CoT mechanisms from three perspectives: the decoding phase, the projection phase, and neuron activation", "source_hint": "Introduction"}, {"name": "Multi-faceted analysis framework examining decoding, projection, and activation", "description": "The authors propose a comprehensive analytical framework that investigates CoT mechanisms from three distinct perspectives: analyzing generated text through test point matching and transfer tests in the decoding phase, examining logits values and probability distributions in the projection phase, and comparing neuron activation patterns in FFN layers.", "author_claim_text": "To answer these questions, we employ a multi-faceted approach: In the decoding phase, we analyze CoT-generated text, focusing on four test points: time, action, location, and number. In the projection phase, we examine horizontal and vertical changes in projected logits. We compare the range, intensity, and dynamic evolution of neuron activations", "source_hint": "Introduction"}, {"name": "Test points matching analysis for fine-grained behavior assessment", "description": "A novel fine-grained analysis method that uses predefined test points (time, action, location/people, number) to statistically measure whether model-generated content aligns more with exemplar formats or pre-trained knowledge, enabling quantification of imitation versus understanding.", "author_claim_text": "Inspired by computer vision methods for comparing actions using key points, we applied a similar approach with 'test points' to assess whether the model's generated content aligns more with the exemplar or pre-trained knowledge. We predefined four types of test points: time test points, action test points, loc&peo test points, number test points", "source_hint": "Methodology §3.3"}]}, "abs": "Chain-of-Thought prompting has significantly enhanced the reasoning capabilities of large language models, with numerous studies exploring factors influencing its performance. However, the underlying mechanisms remain poorly understood. To further demystify the operational principles, this work examines three key aspects: decoding, projection, and activation, aiming to elucidate the changes that occur within models when employing Chainof-Thought. Our findings reveal that LLMs effectively imitate exemplar formats while integrating them with their understanding of the question, exhibiting fluctuations in token logits during generation but ultimately producing a more concentrated logits distribution, and activating a broader set of neurons in the final layers, indicating more extensive knowledge retrieval compared to standard prompts. Our code and data will be publicly avialable when the paper is accepted."}, {"title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Planning Capability of LLM-based Agents/In-Context Learning Methods/Self-Consistency Improves Chain of Thought Reasoning in Language Models.pdf", "summary": {"paper_id": "survey-64_paper-24_Self-Consistency Improves Chain of Thought Reasoning in Language Models", "core_task": {"text": "improving chain of thought reasoning through diverse reasoning path sampling and answer aggregation", "query_variants": ["improving chain of thought reasoning through diverse reasoning path sampling and answer aggregation", "enhancing multi-step reasoning via varied inference trajectory generation and response ensemble methods", "boosting step-by-step inference using multiple reasoning pathway exploration and output combination"]}, "contributions": [{"name": "Limited reasoning ability in language models despite chain-of-thought prompting", "description": "Language models struggle with complex reasoning tasks even when using chain-of-thought prompting with greedy decoding, which represents a fundamental limitation in their ability to perform multi-step reasoning across arithmetic and commonsense domains.", "author_claim_text": "their ability to demonstrate reasoning is often seen as a limitation, which cannot be overcome solely by increasing model scale", "source_hint": "Introduction paragraph 1"}, {"name": "Leveraging multiple reasoning paths to improve answer accuracy", "description": "The authors are motivated by the observation that humans can solve complex problems through diverse reasoning approaches that converge on the same answer, suggesting that sampling multiple reasoning paths from language models could improve reliability and accuracy compared to single-path greedy decoding.", "author_claim_text": "Self-consistency leverages the intuition that complex reasoning tasks typically admit multiple reasoning paths that reach a correct answer", "source_hint": "Introduction paragraph 3"}, {"name": "Self-consistency decoding strategy", "description": "A three-step approach that prompts language models with chain-of-thought examples, samples diverse reasoning paths from the decoder instead of greedy decoding, and aggregates answers by marginalizing over reasoning paths through majority voting to select the most consistent answer.", "author_claim_text": "we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting", "source_hint": "Abstract"}, {"name": "Sample-and-marginalize decoding procedure", "description": "The method samples multiple candidate reasoning paths using temperature sampling or other sampling strategies, where each path may lead to different answers, then determines the optimal answer by taking a majority vote over the final answer set, effectively marginalizing out the sampled reasoning paths.", "author_claim_text": "we propose a 'sample-and-marginalize' decoding procedure: we first sample from the language model's decoder to generate a diverse set of reasoning paths", "source_hint": "Introduction paragraph 4"}]}, "abs": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%)."}, {"title": "Large Language Models are Zero-Shot Reasoners", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Planning Capability of LLM-based Agents/In-Context Learning Methods/Large Language Models are Zero-Shot Reasoners.pdf", "summary": {"paper_id": "survey-64_paper-25_Large Language Models are Zero-Shot Reasoners", "core_task": {"text": "zero-shot chain of thought reasoning in large language models", "query_variants": ["zero-shot chain of thought reasoning in large language models", "chain of thought prompting without examples in LLMs", "step-by-step reasoning without demonstrations in large language models"]}, "contributions": [{"name": "Limited zero-shot reasoning ability of large language models on multi-step tasks", "description": "Large language models struggle with complex multi-step reasoning tasks (system-2 tasks) in zero-shot settings, exhibiting flat scaling curves and poor performance on arithmetic and logical reasoning benchmarks without task-specific few-shot examples.", "author_claim_text": "pre-trained models usually are not good at reasoning, even language models at the scale of 100B or more parameters had struggled on system-2 tasks requiring slow and multi-step reasoning", "source_hint": "Introduction paragraph 2"}, {"name": "Uncovering untapped zero-shot reasoning capabilities in LLMs", "description": "The authors aim to demonstrate that LLMs possess inherent zero-shot reasoning abilities that have been overlooked, and that these broad cognitive capabilities can be elicited through simple prompting rather than requiring expensive few-shot example engineering or fine-tuning.", "author_claim_text": "the versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting", "source_hint": "Abstract"}, {"name": "Zero-shot Chain of Thought prompting with 'Let's think step by step'", "description": "The method adds a simple trigger phrase ('Let's think step by step') to prompts, which elicits step-by-step reasoning from LLMs in a zero-shot manner without requiring hand-crafted task-specific examples, using a two-stage prompting process for reasoning extraction and answer extraction.", "author_claim_text": "we show that LLMs are decent zero-shot reasoners by simply adding 'Let's think step by step' before each answer", "source_hint": "Abstract and Section 3"}, {"name": "Two-stage prompting pipeline", "description": "The approach employs a two-stage process: first prompting the model to generate a reasoning path using the trigger phrase, then using a second prompt with the generated reasoning to extract the final answer in the correct format.", "author_claim_text": "While Zero-shot-CoT is conceptually simple, it uses prompting twice to extract both reasoning and answer", "source_hint": "Section 3.1"}]}, "abs": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding \"Let's think step by step\" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars."}, {"title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Planning Capability of LLM-based Agents/In-Context Learning Methods/Graph of Thoughts: Solving Elaborate Problems with Large Language Models.pdf", "summary": {"paper_id": "survey-64_paper-26_Graph of Thoughts: Solving Elaborate Problems with Large Language Models", "core_task": {"text": "Reasoning with large language models using graph-structured thought transformations", "query_variants": ["Reasoning with large language models using graph-structured thought transformations", "Graph-based thought transformation approaches for reasoning in LLMs", "Large language model reasoning via structured graph thought processes"]}, "contributions": [{"name": "Rigid tree structure limits LLM reasoning in existing prompting schemes", "description": "Existing prompting paradigms like Chain-of-Thought and Tree of Thoughts constrain LLM reasoning by forcing thoughts into linear chains or tree structures, preventing more complex networked reasoning patterns that could combine, merge, or refine thoughts in arbitrary ways.", "author_claim_text": "the ToT approaches still fundamentally limit the reasoning abilities within a prompt by imposing the rigid tree structure on the thought process", "source_hint": "Introduction paragraph 3"}, {"name": "Human reasoning and brain structure form complex networks", "description": "The authors are motivated by the observation that human reasoning, brain structure, and algorithmic execution naturally form complex networked patterns rather than simple chains or trees. Humans combine ideas from different reasoning paths, backtrack, and merge solutions, which cannot be naturally expressed in existing prompting schemes.", "author_claim_text": "When working on a novel idea, a human would not only follow a chain of thoughts (as in CoT) or try different separate ones (as in ToT), but would actually form a more complex network of thoughts", "source_hint": "Introduction paragraph 4"}, {"name": "Graph of Thoughts framework with arbitrary graph-based reasoning", "description": "GoT models LLM reasoning as an arbitrary directed graph where vertices represent thoughts (units of information) and edges represent dependencies between thoughts. This enables novel transformations like aggregating multiple thoughts into synergistic outcomes, refining thoughts through feedback loops, and distilling essence from networks of thoughts.", "author_claim_text": "We propose Graph of Thoughts (GoT), an approach that enhances LLMs' capabilities through networked reasoning", "source_hint": "Introduction paragraph 5"}, {"name": "Modular architecture with fine-grained thought control", "description": "The architecture consists of interacting modules (Prompter, Parser, Scoring, Controller with Graph of Operations and Graph Reasoning State) that enable fine-grained control over individual thoughts and seamless extension with novel thought transformations, reasoning patterns, and LLM models.", "author_claim_text": "we carefully design a modular architecture for implementing GoT (contribution #2), coming with two design highlights. First, we enable a fine-grained control over individual thoughts", "source_hint": "Introduction paragraph 7"}]}, "abs": "We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information (\"LLM thoughts\") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by >31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks."}, {"title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Planning Capability of LLM-based Agents/In-Context Learning Methods/Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.pdf", "summary": {"paper_id": "survey-64_paper-27_Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "core_task": {"text": "eliciting reasoning in large language models through prompting", "query_variants": ["eliciting reasoning in large language models through prompting", "prompting strategies to extract reasoning from large language models", "using prompts to induce reasoning capabilities in large language models"]}, "contributions": [{"name": "Limited reasoning ability of large language models on complex tasks", "description": "Despite improvements from scaling language models, they still struggle with tasks requiring complex reasoning such as arithmetic, commonsense, and symbolic reasoning. Standard prompting methods do not enable models to perform well on these challenging reasoning tasks.", "author_claim_text": "scaling up model size alone has not proved sufficient for achieving high performance on challenging tasks such as arithmetic, commonsense, and symbolic reasoning", "source_hint": "Introduction paragraph 1"}, {"name": "Unlocking reasoning abilities through natural language intermediate steps", "description": "The authors are motivated by combining two insights: (1) generating natural language rationales with intermediate reasoning steps can improve arithmetic reasoning, and (2) large language models enable few-shot in-context learning via prompting, avoiding costly finetuning for each task.", "author_claim_text": "This work explores how the reasoning ability of large language models can be unlocked by a simple method motivated by two ideas", "source_hint": "Introduction paragraph 2"}, {"name": "Chain-of-thought prompting", "description": "The method augments few-shot prompting exemplars with chains of thought—coherent series of intermediate natural language reasoning steps leading to final answers. This allows models to decompose multi-step problems and allocate additional computation to complex reasoning tasks without requiring finetuning.", "author_claim_text": "we explore the ability of language models to perform few-shot prompting for reasoning tasks, given a prompt that consists of triples: ⟨input, chain of thought, output⟩", "source_hint": "Introduction paragraph 3"}, {"name": "Chain-of-thought reasoning emerges with model scale", "description": "Chain-of-thought reasoning is an emergent property that appears only in sufficiently large language models (around 100B parameters). For many reasoning tasks with flat standard prompting scaling curves, chain-of-thought prompting produces dramatically increasing performance with scale.", "author_claim_text": "The emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme", "source_hint": "Discussion paragraph 2"}]}, "abs": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier."}, {"title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Planning Capability of LLM-based Agents/In-Context Learning Methods/Progressive-Hint Prompting Improves Reasoning in Large Language Models.pdf", "summary": {"paper_id": "survey-64_paper-28_Progressive-Hint Prompting Improves Reasoning in Large Language Models", "core_task": {"text": "improving reasoning in large language models through progressive hint prompting", "query_variants": ["improving reasoning in large language models through progressive hint prompting", "enhancing logical inference in LLMs via incremental cue-based prompting", "boosting reasoning capabilities of large language models using stepwise hint strategies"]}, "contributions": [{"name": "Underutilization of LLM-generated answers in reasoning", "description": "Existing prompting methods like Chain-of-Thought and self-consistency fail to leverage previously generated answers from the LLM to iteratively refine and improve reasoning paths toward correct solutions.", "author_claim_text": "these methods do not fully exploit the answers generated by the LLM to guide subsequent responses", "source_hint": "Abstract"}, {"name": "Simulating human double-checking in LLMs", "description": "The authors are motivated by the human cognitive process of re-evaluating answers. They aim to enable LLMs to iteratively refine reasoning by using prior outputs as hints, mimicking how humans reconsider and correct their thinking.", "author_claim_text": "One salient aspect of humanity is our ability to not only think once, but to also double-check our answers. In this paper, we propose that this process can be simulated in language models by sequentially employing previous answers.", "source_hint": "Introduction §3"}, {"name": "Progressive-Hint Prompting (PHP)", "description": "PHP is a two-stage pipeline: first generating a base answer using standard prompts (e.g., CoT), then iteratively re-querying the LLM with the question augmented by previous answers as hints until two consecutive answers match, signaling convergence to a stable solution.", "author_claim_text": "This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers.", "source_hint": "Abstract and §3"}, {"name": "PHP prompt design with dual hint scenarios", "description": "The PHP prompt structure includes a phrase indicating answer proximity in the question and a rehearsal sentence in the answer part. It is designed to handle both correct and incorrect hints, ensuring the model can either confirm correct answers or escape incorrect ones.", "author_claim_text": "we consider the following two potential situations: 1) The hints are the same as the correct answer: to be sure that the model can still get the correct answer when the hint is correct; 2) hints are not the same as the correct answer: to be sure that the model can jump out of the incorrect answer.", "source_hint": "§3 PHP Design Principle"}]}, "abs": "The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experiments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sample paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% -> 91.9%), GSM8K (92% -> 95.5%), AQuA (76.4% -> 79.9%) and MATH (50.3% -> 53.9%)."}, {"title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Planning Capability of LLM-based Agents/In-Context Learning Methods/Tree of Thoughts: Deliberate Problem Solving with Large Language Models.pdf", "summary": {"paper_id": "survey-64_paper-29_Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "core_task": {"text": "deliberate problem solving with large language models", "query_variants": ["deliberate problem solving with large language models", "systematic reasoning using large language models", "intentional problem-solving approaches with LLMs"]}, "contributions": [{"name": "Limitations of token-level left-to-right decision-making in language models", "description": "The paper identifies that current language models perform token-by-token autoregressive generation without exploration or planning, which limits their effectiveness on tasks requiring search, lookahead, or backtracking to correct early mistakes.", "author_claim_text": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role.", "source_hint": "Abstract"}, {"name": "Need for deliberate System 2 reasoning in language models", "description": "The authors are motivated by cognitive science insights about dual-process reasoning, arguing that language models currently operate like System 1 (fast, associative) and would benefit from System 2 (deliberate, planning-based) capabilities to handle complex problem-solving tasks.", "author_claim_text": "Research on 'dual process' models suggests that people have two modes in which they engage with decisions – a fast, automatic, unconscious mode ('System 1') and a slow, deliberate, conscious mode ('System 2'). The simple associative token-level choices of LMs are also reminiscent of 'System 1', and thus might benefit from augmentation by a more deliberate 'System 2' planning process.", "source_hint": "Introduction §1"}, {"name": "Tree of Thoughts (ToT) framework", "description": "ToT is a framework that structures problem-solving as search over a tree where nodes represent partial solutions (thoughts). It allows language models to generate multiple candidate thoughts, evaluate them via self-assessment, and use search algorithms (BFS or DFS) to explore, backtrack, and make global decisions.", "author_claim_text": "We introduce a new framework for language model inference, 'Tree of Thoughts' (ToT), which generalizes over the popular 'Chain of Thought' approach to prompting language models, and enables exploration over coherent units of text ('thoughts') that serve as intermediate steps toward problem solving.", "source_hint": "Abstract"}, {"name": "Language-based thought generation and evaluation", "description": "The method uses the language model itself to generate diverse candidate thoughts at each step and to evaluate their promise toward solving the problem through deliberate reasoning expressed in natural language, replacing traditional programmed or learned heuristics.", "author_claim_text": "ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices.", "source_hint": "Abstract and Introduction §3"}]}, "abs": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: this https URL."}, {"title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Planning Capability of LLM-based Agents/In-Context Learning Methods/Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.pdf", "summary": {"paper_id": "survey-64_paper-30_Least-to-Most Prompting Enables Complex Reasoning in Large Language Models", "core_task": {"text": "complex problem decomposition and sequential reasoning in large language models", "query_variants": ["complex problem decomposition and sequential reasoning in large language models", "multi-step reasoning and task breakdown in LLMs", "hierarchical problem solving and chain-of-thought in large language models"]}, "contributions": [{"name": "Easy-to-hard generalization limitation in chain-of-thought prompting", "description": "Chain-of-thought prompting struggles when test problems are more difficult than the exemplars shown in prompts, particularly failing on tasks requiring compositional generalization where problems exceed the complexity of demonstration examples.", "author_claim_text": "chain-of-thought prompting has a key limitation—it often performs poorly on tasks that require generalization of solving problems harder than the demonstration examples, such as compositional generalization", "source_hint": "Introduction paragraph 3"}, {"name": "Bridging gap between human and machine intelligence in problem-solving", "description": "The authors aim to narrow the gap between human and machine intelligence by enabling models to solve harder problems than those seen during training, mimicking humans' ability to tackle novel challenges of greater complexity.", "author_claim_text": "there still remain huge differences between human intelligence and machine learning: ... (3) Humans can solve problems more difficult than any they have seen before, while for machine learning, examples in training and testing are typically at the same level of difficulty", "source_hint": "Introduction paragraph 1"}, {"name": "Least-to-most prompting strategy", "description": "A two-stage prompting approach that first decomposes complex problems into simpler subproblems, then sequentially solves these subproblems where each solution builds on previously solved subproblems. Both stages use few-shot prompting without training or finetuning.", "author_claim_text": "we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence", "source_hint": "Abstract and Introduction paragraph 4"}, {"name": "Two-stage decomposition and solving framework", "description": "The method implements decomposition through prompts with constant exemplars demonstrating problem breakdown, followed by subproblem solving prompts containing demonstration examples, previously answered subquestions with solutions, and the next question to answer.", "author_claim_text": "It consists of two sequential stages: 1. Decomposition. The prompt in this stage contains constant examples that demonstrate the decomposition ... 2. Subproblem solving. The prompt in this stage consists of three parts", "source_hint": "Section 2"}]}, "abs": "Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix."}, {"title": "Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Planning Capability of LLM-based Agents/Multi-stage Methods/Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling.pdf", "summary": {"paper_id": "survey-64_paper-31_Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling", "core_task": {"text": "Using large language models to guide exploration in reinforcement learning agents", "query_variants": ["Using large language models to guide exploration in reinforcement learning agents", "Leveraging large language models for exploration in RL agents", "Guiding RL agent exploration with large language models"]}, "contributions": [{"name": "Sample inefficiency in RL agents", "description": "The paper addresses the challenge of improving sample efficiency in reinforcement learning agents, which traditionally learn without leveraging external knowledge or pretraining.", "author_claim_text": "Reinforcement learning (RL) agents typically learn tabula rasa, without prior knowledge of the world.", "source_hint": "Abstract"}, {"name": "Utilizing LLMs for RL exploration", "description": "The authors aim to leverage large language models (LLMs) to improve exploration in reinforcement learning, enabling agents to utilize noisy, large-scale knowledge sources effectively.", "author_claim_text": "We hypothesize that if LLMs are instead applied to improving exploration during training, resulting policies will not be constrained by the accuracy of an LLM.", "source_hint": "Introduction §1"}, {"name": "DECKARD agent with Dream and Wake phases", "description": "The DECKARD agent alternates between a Dream phase, where an LLM hypothesizes an Abstract World Model (AWM) of subgoals, and a Wake phase, where the agent verifies and corrects the AWM through grounded experience.", "author_claim_text": "Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase... and (2) the Wake phase...", "source_hint": "Abstract"}, {"name": "Modular RL policies for subgoals", "description": "The agent learns modular policies for individual subgoals, enabling compositional training and efficient navigation of complex tasks like crafting items in Minecraft.", "author_claim_text": "DECKARD operates in two phases: (1) the Dream phase... and (2) the Wake phase where it learns a modular policy of subgoals...", "source_hint": "Introduction §1"}]}, "abs": "Reinforcement learning (RL) agents typically learn tabula rasa, without prior knowledge of the world. However, if initialized with knowledge of high-level subgoals and transitions between subgoals, RL agents could utilize this Abstract World Model (AWM) for planning and exploration. We propose using few-shot large language models (LLMs) to hypothesize an AWM, that will be verified through world experience, to improve sample efficiency of RL agents. Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase where the agent uses an LLM to decompose a task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase where the agent learns a modular policy for each subgoal and verifies or corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience not only increases sample efficiency over contemporary methods by an order of magnitude but is also robust to and corrects errors in the LLM, successfully blending noisy internet-scale information from LLMs with knowledge grounded in environment dynamics."}, {"title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Planning Capability of LLM-based Agents/Multi-stage Methods/SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks.pdf", "summary": {"paper_id": "survey-64_paper-32_SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks", "core_task": {"text": "Action planning for complex interactive reasoning tasks", "query_variants": ["Action planning for complex interactive reasoning tasks", "Planning actions for intricate reasoning in interactive tasks", "Developing action plans for complex reasoning interactions"]}, "contributions": [{"name": "Limitations in complex interactive reasoning tasks", "description": "The paper addresses the challenge of developing agents capable of solving complex interactive reasoning tasks, which require long-horizon planning, subgoal decomposition, exception handling, and commonsense knowledge, areas where existing methods struggle.", "author_claim_text": "Although the aforementioned methods exhibit remarkable performance in relatively simple tasks, their ability to generalize to more complex and demanding tasks is limited.", "source_hint": "Introduction §1"}, {"name": "Advancing artificial general intelligence through complex task-solving agents", "description": "The authors are motivated by the need to create agents that can perform human-like problem-solving in dynamic, open-world environments, which is essential for advancing artificial general intelligence.", "author_claim_text": "The advancement of artificial general intelligence is largely dependent on the development of agents that are proficient in complex interactive reasoning tasks.", "source_hint": "Introduction §1"}, {"name": "SWIFT module for fast thinking", "description": "The SWIFT module represents fast, intuitive thinking by using a fine-tuned encoder-decoder language model to handle short-term memory and generate immediate actions based on the current environment and task state.", "author_claim_text": "The SWIFT module is a small encoder-decoder LM, fine-tuned on a T5-large (770m) checkpoint using the searched oracle trajectories of training tasks.", "source_hint": "Abstract"}, {"name": "SAGE module for slow thinking", "description": "The SAGE module emulates deliberate, analytical thinking by leveraging large language models for subgoal planning, exception handling, and converting plans into executable action sequences.", "author_claim_text": "The SAGE module, representing the deliberate thinking of System 2, utilizes LLMs, such as GPT-4, and is structured around two prompting stages: planning and grounding.", "source_hint": "Abstract"}]}, "abs": "We introduce SwiftSage, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks. SwiftSage integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance. The framework comprises two primary modules: the Swift module, representing fast and intuitive thinking, and the Sage module, emulating deliberate thought processes. The Swift module is a small encoder-decoder LM fine-tuned on the oracle agent's action trajectories, while the Sage module employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a heuristic method to harmoniously integrate the two modules, resulting in a more efficient and robust problem-solving process. In 30 tasks from the ScienceWorld benchmark, SwiftSage significantly outperforms other methods such as SayCan, ReAct, and Reflexion, demonstrating its effectiveness in solving complex interactive tasks."}, {"title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Relationship of Multi-Agent/Competitive/ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate.pdf", "summary": {"paper_id": "survey-64_paper-33_ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate", "core_task": {"text": "multi-agent debate for text quality evaluation", "query_variants": ["multi-agent debate for text quality evaluation", "collaborative agent discussion for assessing text quality", "multiple agent deliberation to evaluate textual quality"]}, "contributions": [{"name": "Limitations of single-agent LLM-based text evaluation", "description": "The paper identifies that existing single-agent LLM-based evaluation methods do not yet achieve human-level quality in text evaluation, particularly for open-ended questions and traditional NLG tasks, indicating a gap that needs to be addressed.", "author_claim_text": "While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality.", "source_hint": "Abstract"}, {"name": "Mimicking collaborative human evaluation processes", "description": "The authors are motivated by the observation that human evaluation typically involves multiple annotators working together, which reduces bias and improves reliability. They aim to replicate this collaborative approach using multiple LLM agents to enhance evaluation quality.", "author_claim_text": "Recognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies.", "source_hint": "Abstract"}, {"name": "ChatEval multi-agent referee team with debate framework", "description": "ChatEval is a multi-agent system where multiple LLM agents, each with distinct personas, engage in autonomous debate to evaluate text quality. The framework enables agents to discuss and reach judgments collaboratively, mimicking human group evaluation processes.", "author_claim_text": "In this paper, we construct a multi-agent referee team called ChatEval to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks.", "source_hint": "Abstract"}, {"name": "Diverse role prompts for agents", "description": "The method assigns different role descriptions (personas) to each agent in the multi-agent system, ensuring that agents evaluate from varied perspectives. This diversity is shown to be essential for effective debate and improved evaluation outcomes.", "author_claim_text": "Furthermore, to enrich the evaluation dynamics, every agent within ChatEval is endowed with a unique persona. This deliberate design ensures that each agent focuses on distinct perspectives or brings specific expertise to the table.", "source_hint": "Introduction §1"}]}, "abs": "Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality. Recognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies. The multi-agent-based approach enables a group of LLMs to synergize with an array of intelligent counterparts, harnessing their distinct capabilities and expertise to enhance efficiency and effectiveness in handling intricate tasks. In this paper, we construct a multi-agent referee team called ChatEval to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks. Our analysis shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments. Our code is available at this https URL."}, {"title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Relationship of Multi-Agent/Competitive/Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate.pdf", "summary": {"paper_id": "survey-64_paper-34_Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate", "core_task": {"text": "encouraging divergent thinking in large language models through debate", "query_variants": ["encouraging divergent thinking in large language models through debate", "promoting diverse reasoning in LLMs via multi-agent discussion", "fostering alternative perspectives in large language models using argumentative dialogue"]}, "contributions": [{"name": "Degeneration-of-Thought (DoT) problem in self-reflection", "description": "The paper identifies that self-reflection methods in LLMs suffer from a DoT problem where LLMs become unable to generate novel thoughts through iterative self-reflection once they establish confidence in their initial answers, even when those answers are incorrect. This limits the effectiveness of self-reflection for complex reasoning tasks.", "author_claim_text": "our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect.", "source_hint": "Abstract"}, {"name": "Addressing limitations of self-reflection for complex reasoning", "description": "The authors are motivated by the need to improve LLMs' performance on complex reasoning tasks that require deep contemplation. They aim to explore human-like problem-solving strategies beyond self-reflection to overcome the DoT problem and enable divergent thinking in LLMs.", "author_claim_text": "Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies.", "source_hint": "Abstract"}, {"name": "Multi-Agent Debate (MAD) framework", "description": "The MAD framework involves multiple LLM-based agents (debaters) that engage in adversarial debate in a 'tit for tat' manner, with a judge agent that monitors the debate process and determines when to conclude. This design encourages divergent thinking by allowing agents to challenge each other's biases and provide external feedback.", "author_claim_text": "To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of 'tit for tat' and a judge manages the debate process to obtain a final solution.", "source_hint": "Abstract"}, {"name": "Adaptive break mechanism for debate termination", "description": "The judge agent in MAD can adaptively terminate the debate when it determines that an optimal solution has been reached, rather than forcing a fixed number of debate rounds. This mechanism allows efficient conclusion of debates while maintaining solution quality.", "author_claim_text": "In our experimental setup, we did not implement any additional stopping strategies besides setting the maximum debate iteration to 3. In other words, the judge can take an adaptive break if it believes the optimal answer has already been obtained, efficiently ending the debate early.", "source_hint": "Section 4.3"}]}, "abs": "Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of \"tit for tat\" and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of \"tit for tat\" state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Code is available at this https URL."}, {"title": "Building Cooperative Embodied Agents Modularly with Large Language Models", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Relationship of Multi-Agent/Cooperative/Building Cooperative Embodied Agents Modularly with Large Language Models.pdf", "summary": {"paper_id": "survey-64_paper-35_Building Cooperative Embodied Agents Modularly with Large Language Models", "core_task": {"text": "multi-agent cooperation in embodied environments", "query_variants": ["multi-agent cooperation in embodied environments", "collaborative multi-agent systems in physical simulation environments", "cooperative behavior among multiple agents in embodied tasks"]}, "contributions": [{"name": "Multi-agent cooperation with decentralized control, costly communication, and complex observations", "description": "The paper tackles the challenge of enabling multiple embodied agents to cooperate in environments where agents operate independently (decentralized control), perceive only partial raw sensory data, face time costs for communication, and must accomplish complex multi-objective tasks over long horizons.", "author_claim_text": "we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments", "source_hint": "Abstract, first sentence"}, {"name": "Building agents that cooperate and communicate naturally with humans and other agents", "description": "The authors are motivated by the goal of creating embodied agents capable of assisting humans in real-world tasks, which requires handling perception, planning, and natural language communication in cooperative settings, an area that remains largely unexplored for decentralized multi-agent systems.", "author_claim_text": "Building embodied agents that can also engage in and assist humans in everyday life is a valuable but challenging task, considering the complexity of perception, partial observation, long-horizon planning, natural language communication", "source_hint": "Introduction, paragraph 1"}, {"name": "CoELA: Cooperative Embodied Language Agent with modular cognitive architecture", "description": "CoELA is a modular framework inspired by cognitive architectures that integrates LLMs into five modules: Perception (extracting information from raw observations), Memory (maintaining knowledge about the world and others), Communication (generating natural language messages), Planning (making high-level decisions), and Execution (translating plans into primitive actions).", "author_claim_text": "we present CoELA, a Cooperative Embodied Language Agent, a cognitive architecture with a novel modular framework that utilizes the rich world knowledge, strong reasoning ability and mastery natural language understanding and generation capability of LLMs", "source_hint": "Abstract and Introduction, paragraph 3"}, {"name": "Fine-tuned CoLLAMA using LoRA on agent-collected data", "description": "The authors apply parameter-efficient fine-tuning (LoRA) to the open LLM LLAMA-2 using a small set of high-quality trajectory data collected by their agents in embodied environments, resulting in CoLLAMA, which achieves competitive performance with GPT-4 in some tasks.", "author_claim_text": "we fine-tune a CoLLAMA with data collected with our agents and show how they can achieve promising performance", "source_hint": "Abstract and Implementation Details in Section 5.2"}]}, "abs": "In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA-2 still underperform, we fine-tune a CoELA with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi-agent cooperation. Videos can be found on the project website this https URL."}, {"title": "BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Relationship of Multi-Agent/Cooperative/BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents.pdf", "summary": {"paper_id": "survey-64_paper-36_BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents", "core_task": {"text": "Benchmarking and orchestrating LLM-augmented autonomous agents", "query_variants": ["Benchmarking and orchestrating LLM-augmented autonomous agents", "Evaluation and coordination of large language model enhanced autonomous agents", "Performance assessment and management of LLM-powered autonomous agent systems"]}, "contributions": [{"name": "Limited exploration of LLM-augmented Autonomous Agent architectures and LLM backbones", "description": "The paper addresses the lack of comprehensive understanding regarding optimal agent architectures for LLM-augmented Autonomous Agents (LAAs) and the efficacy of different LLM backbones. Current works have not jointly considered agent architectures with their LLM backbones, and there is insufficient exploration of how to orchestrate multiple agents for complex tasks.", "author_claim_text": "Since the investigation of LAA is still very recent, limited explorations are available. Therefore, we provide a comprehensive comparison of LAA in terms of both agent architectures and LLM backbones.", "source_hint": "Abstract"}, {"name": "Need for quantitative guidance in designing LAA systems", "description": "The authors are motivated to provide empirical evidence and quantitative guidance for researchers and practitioners in selecting appropriate LAA architectures and LLM backbones. They aim to understand which combinations work best for different task complexities and environments, addressing gaps in current LAA research.", "author_claim_text": "Our performance results provide quantitative suggestions for designing LAA architectures and the optimal choice of LLMs, as well as the compatibility of both.", "source_hint": "Abstract"}, {"name": "Six different LAA architectures with varying design principles", "description": "The authors design and implement six LAA architectures: Zeroshot LAA (ZS-LAA), ZeroshotThink LAA (ZST-LAA), ReAct LAA, PlanAct LAA, PlanReAct LAA, and BOLAA. These architectures systematically explore design choices including zero-shot vs few-shot prompting, self-thinking (Chain-of-Thought), and planning capabilities.", "author_claim_text": "We develop 6 different LAA agent architecture. We combine them with various backbone LLMs to justify the designing intuition of LAA from prompting, self-thinking, and planning.", "source_hint": "Introduction, contributions"}, {"name": "BOLAA: Multi-agent orchestration architecture", "description": "BOLAA is a novel architecture that orchestrates multiple specialized LAAs through a controller module. The controller selectively calls labor agents from a pool, where each agent focuses on one type of action. This design divides complex tasks into feasible subtasks handled by specialist agents.", "author_claim_text": "We also develop BOLAA for orchestrating multi-agent strategy, which enhances the action interaction ability of solo agents.", "source_hint": "Introduction, contributions; Section 3.2"}]}, "abs": "The massive successes of large language models (LLMs) encourage the emerging exploration of LLM-augmented Autonomous Agents (LAAs). An LAA is able to generate actions with its core LLM and interact with environments, which facilitates the ability to resolve complex tasks by conditioning on past interactions such as observations and actions. Since the investigation of LAA is still very recent, limited explorations are available. Therefore, we provide a comprehensive comparison of LAA in terms of both agent architectures and LLM backbones. Additionally, we propose a new strategy to orchestrate multiple LAAs such that each labor LAA focuses on one type of action, \\textit{i.e.} BOLAA, where a controller manages the communication among multiple agents. We conduct simulations on both decision-making and multi-step reasoning environments, which comprehensively justify the capacity of LAAs. Our performance results provide quantitative suggestions for designing LAA architectures and the optimal choice of LLMs, as well as the compatibility of both. We release our implementation code of LAAs to the public at \\url{this https URL}."}, {"title": "AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Relationship of Multi-Agent/Cooperative/AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors.pdf", "summary": {"paper_id": "survey-64_paper-37_AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors", "core_task": {"text": "multi-agent collaboration and emergent behaviors in problem-solving", "query_variants": ["multi-agent collaboration and emergent behaviors in problem-solving", "collective agent coordination and spontaneous patterns in task resolution", "cooperative multi-agent systems with emerging dynamics for solving problems"]}, "contributions": [{"name": "Limited cooperation among autonomous agents in complex real-world tasks", "description": "Existing autonomous agents powered by LLMs lack effective mechanisms for collaboration when tackling complex tasks that require cooperation among multiple agents, similar to how human groups work together to solve problems more efficiently.", "author_claim_text": "in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment", "source_hint": "Abstract"}, {"name": "Enhancing agent effectiveness through human-inspired group dynamics", "description": "The authors are motivated by the observation that human groups achieve better outcomes through collaboration, and seek to replicate this dynamic in multi-agent systems to improve task accomplishment efficiency and effectiveness beyond what single agents can achieve.", "author_claim_text": "inspired by human group dynamics, we propose a multi-agent framework AGENT VERSE that can effectively orchestrate a collaborative group of expert agents as a greater-than-the-sum-of-its-parts system", "source_hint": "Abstract"}, {"name": "AGENT VERSE multi-agent framework with four-stage problem-solving process", "description": "AGENT VERSE is a framework that orchestrates multi-agent collaboration through four stages: Expert Recruitment (dynamically determining agent composition), Collaborative Decision-Making (agents discuss strategies using horizontal or vertical communication structures), Action Execution (implementing decided actions), and Evaluation (assessing outcomes and providing feedback for iterative refinement).", "author_claim_text": "we introduce AGENT VERSE. This general multi-agent framework simulates the problem-solving procedures of human groups, and allows for dynamic adjustment of group members based on current progress", "source_hint": "Introduction §1, Section 2"}, {"name": "Automated expert recruitment mechanism", "description": "Instead of manual role assignment, AGENT VERSE uses a recruiter agent that dynamically generates expert descriptions based on the task goal, forming an expert group that can be adjusted based on feedback from the evaluation stage.", "author_claim_text": "AGENT VERSE automates expert recruitment to make agent configuration more scalable. For a given goal g, a particular agent Mr is prompted as the 'recruiter', similar to a human resource manager", "source_hint": "Section 2.1"}]}, "abs": "Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework \\framework that can collaboratively and dynamically adjust its composition as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that \\framework framework can effectively deploy multi-agent groups that outperform a single agent. Furthermore, we delve into the emergence of social behaviors among individual agents within a group during collaborative task accomplishment. In view of these behaviors, we discuss some possible strategies to leverage positive ones and mitigate negative ones for improving the collaborative potential of multi-agent groups. Our codes for \\framework will soon be released at \\url{this https URL}."}, {"title": "CGMI: Configurable General Multi-Agent Interaction Framework", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Relationship of Multi-Agent/Cooperative/CGMI: Configurable General Multi-Agent Interaction Framework.pdf", "summary": {"paper_id": "survey-64_paper-38_CGMI: Configurable General Multi-Agent Interaction Framework", "core_task": {"text": "simulating multi-agent social interactions in domain-specific scenarios", "query_variants": ["simulating multi-agent social interactions in domain-specific scenarios", "modeling interactions among multiple agents in specialized application contexts", "multi-agent behavioral simulation within targeted domain environments"]}, "contributions": [{"name": "Superficial content generation and lack of effective cognitive architecture in LLM-based agents", "description": "LLM-based agents produce shallow outputs and lack the cognitive structures needed to handle complex domain-specific tasks and realistic human-like interactions, particularly in multi-turn dialogues where they forget character settings and fail to execute plans consistently.", "author_claim_text": "the content generated by these agents remains somewhat superficial, owing to their limited domain expertise and the absence of an effective cognitive architecture", "source_hint": "Abstract"}, {"name": "Simulating realistic human interactions in domain-specific scenarios", "description": "The authors aim to create agents that can authentically simulate social phenomena and human behaviors in specific contexts, such as classroom teaching, by addressing limitations in role stability, communication mechanisms, and cognitive depth.", "author_claim_text": "designed to replicate human interactions in real-world scenarios", "source_hint": "Abstract and Introduction"}, {"name": "Tree-structured persona model for personality assignment, detection, and maintenance", "description": "A hierarchical approach based on the Big Five Personality Scale that uses depth-first traversal to assign personalities and efficient random testing to detect and maintain personality consistency, reducing memory pressure and improving agent stability in multi-turn interactions.", "author_claim_text": "we propose a tree-structured methodology for the assignment, detection, and maintenance of agent personality", "source_hint": "Abstract and Method section"}, {"name": "Cognitive architecture equipped with skill library based on ACT* model", "description": "A cognitive framework inspired by the Adaptive Control of Thought model that integrates declarative and procedural memories extracted via Chain of Thought and Chain of Action methods, combined with a domain-specific skill library to enable deeper reflection and specialized planning.", "author_claim_text": "we designed a cognitive architecture equipped with a skill library based on the ACT* model, which contains memory, reflection, and planning modules", "source_hint": "Abstract and Method section"}]}, "abs": "Benefiting from the powerful capabilities of large language models (LLMs), agents based on LLMs have shown the potential to address domain-specific tasks and emulate human behaviors. However, the content generated by these agents remains somewhat superficial, owing to their limited domain expertise and the absence of an effective cognitive architecture. To address this, we present the Configurable General Multi-Agent Interaction (CGMI) framework, designed to replicate human interactions in real-world scenarios. Specifically, we propose a tree-structured methodology for the assignment, detection, and maintenance of agent personality. Additionally, we designed a cognitive architecture equipped with a skill library based on the ACT* model, which contains memory, reflection, and planning modules. We have also integrated general agents to augment the virtual environment's realism. Using the CGMI framework, we simulated numerous classroom interactions between teacher and students. The experiments indicate that aspects such as the teaching methodology, curriculum, and student performance closely mirror real classroom settings. We will open source our work."}, {"title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Relationship of Multi-Agent/Hierarchical/AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation.pdf", "summary": {"paper_id": "survey-64_paper-39_AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation", "core_task": {"text": "multi-agent conversation framework for LLM applications", "query_variants": ["multi-agent conversation framework for LLM applications", "multi-agent dialogue system for large language model tasks", "conversational multi-agent architecture for LLM-based applications"]}, "contributions": [{"name": "Difficulty in building diverse LLM applications via multi-agent cooperation", "description": "The paper addresses the challenge of enabling developers to build LLM applications across diverse domains and complexities using multiple cooperating agents. Existing approaches lack a unified framework that supports flexible agent customization, diverse conversation patterns, and both automated and human-in-the-loop workflows.", "author_claim_text": "how can we facilitate the development of LLM applications that could span a broad spectrum of domains and complexities based on the multi-agent approach?", "source_hint": "Introduction paragraph 1"}, {"name": "Leveraging multi-agent conversations to scale LLM capabilities", "description": "The authors are motivated by the observation that chat-optimized LLMs can incorporate feedback and cooperate through conversations, that differently configured agents can combine broad LLM capabilities in a modular manner, and that multi-agent conversations enable intuitive task decomposition and integration for complex problems.", "author_claim_text": "Our insight is to use multi-agent conversations to achieve it. There are at least three reasons confirming its general feasibility and utility thanks to recent advances in LLMs", "source_hint": "Introduction paragraph 2"}, {"name": "Conversable agents with customizable capabilities", "description": "AutoGen introduces conversable agents that can send and receive messages, maintain internal context, and be configured with various capabilities including LLMs, human inputs, and tools. Agents are customizable through built-in classes like ConversableAgent, AssistantAgent, and UserProxyAgent, enabling developers to create specialized agents by reusing or extending these components.", "author_claim_text": "AutoGen uses a generic design of agents that can leverage LLMs, human inputs, tools, or a combination of them", "source_hint": "Introduction and Section 2.1"}, {"name": "Conversation programming paradigm", "description": "The framework adopts a programming paradigm centered on multi-agent conversations, involving (1) defining conversable agents with specific capabilities and roles, and (2) programming agent interaction behavior via conversation-centric computation and control. This paradigm uses both natural and programming languages to build applications with diverse conversation patterns.", "author_claim_text": "We refer to this paradigm as conversation programming, which streamlines the development of intricate applications via two primary steps", "source_hint": "Introduction and Section 2.2"}]}, "abs": "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc."}, {"title": "AvalonBench: Evaluating LLMs Playing the Game of Avalon", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Relationship of Multi-Agent/Mixed/AvalonBench: Evaluating LLMs Playing the Game of Avalon.pdf", "summary": {"paper_id": "survey-64_paper-40_AvalonBench: Evaluating LLMs Playing the Game of Avalon", "core_task": {"text": "Evaluating large language models playing social deduction games", "query_variants": ["Evaluating large language models playing social deduction games", "Assessment of LLMs in social deduction game environments", "Testing large language models performance in deception-based multiplayer games"]}, "contributions": [{"name": "Lack of comprehensive benchmark for multi-agent LLM game playing", "description": "Existing benchmarks like AgentBench cover single-agent environments, but there is no comprehensive test-bed specifically designed to evaluate multi-agent LLM agents in complex strategic social deduction games requiring deduction, persuasion, negotiation, and deception.", "author_claim_text": "a comprehensive benchmark test-bed still needs to be improved for studying multi-agent game playing of LLM Agents", "source_hint": "Introduction, paragraph 3"}, {"name": "Avalon as test-bed for LLM reasoning and communication capabilities", "description": "The game Resistance Avalon requires players to perform deductive reasoning, coordinate and collaborate with teammates, and employ deception to hide identities. These characteristics make it an ideal environment to test and study the decision-making, language processing, and multi-agent interaction capabilities of LLM agents.", "author_claim_text": "Resistance Avalon and related hidden-identity social deduction language games present a significant challenge to AI... we believe it is a good test-bed for evaluating and studying the language understanding and reasoning capability of AI Agents", "source_hint": "Introduction, paragraphs 2-3"}, {"name": "AVALON BENCH benchmark environment", "description": "The authors develop a comprehensive benchmark consisting of a game environment that records player actions and manages game progression, rule-based baseline bots with deterministic strategies, and ReAct-style LLM agent implementations with carefully designed prompts for each role.", "author_claim_text": "we build a AVALON BENCH, a game engine to benchmark the multi-agent LLM Agents. This includes (1) a game environment for agents to play on Resistance Avalon... (2) several naive AI bots with deterministic rule-based strategies... and (3) some baseline implementations of ReAct-style LLM agents", "source_hint": "Introduction, paragraph 4"}, {"name": "ReAct-style LLM agent implementation with recursive summarization", "description": "The authors implement LLM agents using a ReAct framework where the LLM generates actions, dialogue, and recursive summaries of game history. A separate parser LLM ensures correct action formatting, and recursive summarization handles long context by condensing previous history with current discussion and outcomes.", "author_claim_text": "we leverage a Reason-then-Action (ReAct) paradigm for decision-making, with zero-shot Chain-of-Thought prompting... we also ask the LLM to summarize their history recursively", "source_hint": "Section 3, subsections 3.1 and 3.2"}]}, "abs": "In this paper, we explore the potential of Large Language Models (LLMs) Agents in playing the strategic social deduction game, Resistance Avalon. Players in Avalon are challenged not only to make informed decisions based on dynamically evolving game phases, but also to engage in discussions where they must deceive, deduce, and negotiate with other players. These characteristics make Avalon a compelling test-bed to study the decision-making and language-processing capabilities of LLM Agents. To facilitate research in this line, we introduce AvalonBench - a comprehensive game environment tailored for evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game environment for Avalon, (2) rule-based bots as baseline opponents, and (3) ReAct-style LLM agents with tailored prompts for each role. Notably, our evaluations based on AvalonBench highlight a clear capability gap. For instance, models like ChatGPT playing good-role got a win rate of 22.2% against rule-based bots playing evil, while good-role bot achieves 38.2% win rate in the same setting. We envision AvalonBench could be a good test-bed for developing more advanced LLMs (with self-playing) and agent frameworks that can effectively model the layered complexities of such game environments."}, {"title": "Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Relationship of Multi-Agent/Mixed/Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf.pdf", "summary": {"paper_id": "survey-64_paper-41_Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf", "core_task": {"text": "communication games with large language models", "query_variants": ["communication games with large language models", "language-based interaction tasks using LLMs", "conversational game scenarios involving large language models"]}, "contributions": [{"name": "Engaging LLMs in communication games with natural language", "description": "Existing methods for communication games either impose strict language restrictions or require substantial human-annotated data, making it difficult for AI agents to participate naturally in games like Werewolf that rely heavily on natural language communication.", "author_claim_text": "it is still challenging for AI agents to play communication games in a natural way", "source_hint": "Introduction paragraph 1"}, {"name": "Practical value of communication games for economics and social science", "description": "Communication games serve as valuable proxies for studying fundamental problems in economics and social science, and recent advances in LLMs with sophisticated language and reasoning abilities make them promising candidates for tackling these games naturally.", "author_claim_text": "incomplete information games such as Werewolf and Poker can be used as a good proxy to exploit various fundamental problems in economics and social science", "source_hint": "Introduction paragraph 1"}, {"name": "Tuning-free framework with retrieval and reflection", "description": "The framework keeps LLMs frozen and addresses context length limitations through three perspectives: freshness (recent messages), informativeness (rule-matched critical messages), and completeness (reflection by answering questions). This enables effective gameplay without parameter tuning.", "author_claim_text": "we propose a tuning-free framework. Our approach keeps LLMs frozen, and relies on the retrieval and reflection on past communications and experiences for improvement", "source_hint": "Abstract"}, {"name": "Non-parametric learning from experience pool", "description": "The method collects scored response-reflection pairs from all players into an experience pool, then retrieves relevant experiences based on current reflection and extracts suggestions to guide agent reasoning, allowing learning across rounds without fine-tuning.", "author_claim_text": "we propose a non-parametric learning mechanism, enabling LLMs to take reference from experiences without parameter tuning", "source_hint": "Section 3.4"}]}, "abs": "Communication games, which we refer to as incomplete information games that heavily depend on natural language communication, hold significant research value in fields such as economics, social science, and artificial intelligence. In this work, we explore the problem of how to engage large language models (LLMs) in communication games, and in response, propose a tuning-free framework. Our approach keeps LLMs frozen, and relies on the retrieval and reflection on past communications and experiences for improvement. An empirical study on the representative and widely-studied communication game, ``Werewolf'', demonstrates that our framework can effectively play Werewolf game without tuning the parameters of the LLMs. More importantly, strategic behaviors begin to emerge in our experiments, suggesting that it will be a fruitful journey to engage LLMs in communication games and associated domains."}, {"title": "Reflexion: Language Agents with Verbal Reinforcement Learning", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Rethinking Capability of LLM-based Agents/In-Context Learning Methods/Reflexion: Language Agents with Verbal Reinforcement Learning.pdf", "summary": {"paper_id": "survey-64_paper-42_Reflexion: Language Agents with Verbal Reinforcement Learning", "core_task": {"text": "verbal reinforcement learning for language agents", "query_variants": ["verbal reinforcement learning for language agents", "language-based RL for conversational agents", "reinforcement learning with linguistic feedback for language models"]}, "contributions": [{"name": "Inefficient learning from trial-and-error in language agents", "description": "The paper addresses the challenge that language agents using large language models struggle to learn efficiently from trial-and-error experiences, as traditional RL methods demand extensive training data and costly model fine-tuning, limiting their practical deployment.", "author_claim_text": "it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning", "source_hint": "Abstract"}, {"name": "Enabling lightweight learning without model fine-tuning", "description": "The authors are motivated by the need for a learning approach that avoids expensive model fine-tuning while providing richer feedback than scalar rewards, enabling agents to learn from mistakes through interpretable verbal reflections similar to human iterative learning.", "author_claim_text": "Reflexion has several advantages compared to more traditional RL approaches: 1) it is lightweight and doesn't require finetuning the LLM, 2) it allows for more nuanced forms of feedback", "source_hint": "Introduction paragraph 3"}, {"name": "Reflexion framework with verbal reinforcement", "description": "Reflexion is a framework that reinforces language agents through verbal self-reflection rather than weight updates. Agents verbally reflect on task feedback, store reflective text in episodic memory, and use these experiences to improve decision-making in subsequent trials.", "author_claim_text": "We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback", "source_hint": "Abstract"}, {"name": "Three-model architecture with Actor, Evaluator, and Self-Reflection", "description": "The method employs three components: an Actor (LLM) that generates actions, an Evaluator that scores outputs using various feedback types (binary, heuristic, or LLM-based), and a Self-Reflection model that converts feedback into verbal summaries stored in long-term memory.", "author_claim_text": "We develop a modular formulation for Reflexion, utilizing three distinct models: an Actor, denoted as Ma, which generates text and actions; an Evaluator model, represented by Me, that scores the outputs produced by Ma; and a Self-Reflection model, denoted as Msr", "source_hint": "Section 3"}]}, "abs": "Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance."}, {"title": "ReAct: Synergizing Reasoning and Acting in Language Models", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Rethinking Capability of LLM-based Agents/In-Context Learning Methods/ReAct: Synergizing Reasoning and Acting in Language Models.pdf", "summary": {"paper_id": "survey-64_paper-43_ReAct: Synergizing Reasoning and Acting in Language Models", "core_task": {"text": "interleaving reasoning and acting in language models for task solving", "query_variants": ["interleaving reasoning and acting in language models for task solving", "combining thought and action in LLMs for problem completion", "integrating cognitive steps with execution in language models for goal achievement"]}, "contributions": [{"name": "Isolated reasoning and acting in language models", "description": "Large language models have demonstrated strong capabilities in reasoning and action generation, but these abilities have been studied in isolation rather than combined synergistically, limiting their effectiveness in complex task solving.", "author_claim_text": "their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics", "source_hint": "Abstract, sentence 2"}, {"name": "Synergy between reasoning and acting in human cognition", "description": "Humans naturally combine verbal reasoning with actions during problem solving, using reasoning to track progress, handle exceptions, and maintain working memory while acting to gather information. This synergy enables robust decision making and quick learning of new tasks.", "author_claim_text": "A unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with verbal reasoning (or inner speech), which has been theorized to play an important role in human cognition", "source_hint": "Introduction, paragraph 1"}, {"name": "ReAct paradigm for interleaving reasoning and acting", "description": "ReAct prompts large language models to generate reasoning traces (thoughts) and task-specific actions in an interleaved manner. Reasoning helps induce, track, and update action plans while handling exceptions, and actions enable interaction with external sources to gather additional information that supports reasoning.", "author_claim_text": "we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two", "source_hint": "Abstract, sentence 3"}, {"name": "Augmented action space with language thoughts", "description": "The method augments the agent's action space to include language-based thoughts alongside domain-specific actions. These thoughts do not affect the external environment but compose useful information by reasoning over context to support future reasoning or acting.", "author_claim_text": "we augment the agent's action space to A-hat = A union L, where L is the space of language. An action a-hat in the language space, which we will refer to as a thought or a reasoning trace", "source_hint": "Section 2, paragraph 2"}]}, "abs": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: this https URL"}, {"title": "Making Large Language Models Better Reasoners with Alignment", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Rethinking Capability of LLM-based Agents/Modular Coordination Methods/Making Large Language Models Better Reasoners with Alignment.pdf", "summary": {"paper_id": "survey-64_paper-44_Making Large Language Models Better Reasoners with Alignment", "core_task": {"text": "Improving reasoning capabilities of large language models through alignment fine-tuning", "query_variants": ["Improving reasoning capabilities of large language models through alignment fine-tuning", "Enhancing reasoning skills in large language models via alignment fine-tuning", "Refining reasoning abilities of large language models using alignment fine-tuning"]}, "contributions": [{"name": "Assessment Misalignment in fine-tuned LLMs", "description": "The paper identifies that fine-tuned large language models often assign higher scores to low-quality chain-of-thought (COT) reasoning paths, which limits their reasoning capabilities.", "author_claim_text": "we find that the fine-tuned LLMs suffer from an Assessment Misalignment problem, i.e., they frequently assign higher scores to subpar COTs, leading to potential limitations in their reasoning abilities.", "source_hint": "Abstract"}, {"name": "Enhancing reasoning capabilities of LLMs", "description": "The authors aim to improve reasoning capabilities in large language models, which are critical for their role in artificial general intelligence systems.", "author_claim_text": "The reasoning capability is essential for large language models (LLMs) to serve as the brain of the artificial general intelligence agent.", "source_hint": "Abstract"}, {"name": "Alignment Fine-Tuning (AFT) paradigm", "description": "The authors propose a three-step Alignment Fine-Tuning (AFT) paradigm that fine-tunes LLMs with COT data, categorizes responses as positive or negative, and calibrates their scores using a constraint alignment loss.", "author_claim_text": "To address this problem, we introduce an Alignment Fine-Tuning (AFT) paradigm, which involves three steps: 1) fine-tuning LLMs with COT training data; 2) generating multiple COT responses for each question, and categorizing them into positive and negative ones based on whether they achieve the correct answer; 3) calibrating the scores of positive and negative responses given by LLMs with a novel constraint alignment loss.", "source_hint": "Abstract"}, {"name": "Constraint alignment loss", "description": "The constraint alignment loss ensures that positive responses are scored higher than negative ones and keeps negative scores within a reasonable range to maintain model performance.", "author_claim_text": "Specifically, the constraint alignment loss has two objectives: a) Alignment, which guarantees that positive scores surpass negative scores to encourage answers with high-quality COTs; b) Constraint, which keeps the negative scores confined to a reasonable range to prevent the model degradation.", "source_hint": "Abstract"}]}, "abs": "Reasoning is a cognitive process of using evidence to reach a sound conclusion. The reasoning capability is essential for large language models (LLMs) to serve as the brain of the artificial general intelligence agent. Recent studies reveal that fine-tuning LLMs on data with the chain of thought (COT) reasoning process can significantly enhance their reasoning capabilities. However, we find that the fine-tuned LLMs suffer from an \\textit{Assessment Misalignment} problem, i.e., they frequently assign higher scores to subpar COTs, leading to potential limitations in their reasoning abilities. To address this problem, we introduce an \\textit{Alignment Fine-Tuning (AFT)} paradigm, which involves three steps: 1) fine-tuning LLMs with COT training data; 2) generating multiple COT responses for each question, and categorizing them into positive and negative ones based on whether they achieve the correct answer; 3) calibrating the scores of positive and negative responses given by LLMs with a novel constraint alignment loss. Specifically, the constraint alignment loss has two objectives: a) Alignment, which guarantees that positive scores surpass negative scores to encourage answers with high-quality COTs; b) Constraint, which keeps the negative scores confined to a reasonable range to prevent the model degradation. Beyond just the binary positive and negative feedback, the constraint alignment loss can be seamlessly adapted to the ranking situations when ranking feedback is accessible. Furthermore, we also delve deeply into recent ranking-based alignment methods, such as DPO, RRHF, and PRO, and discover that the constraint, which has been overlooked by these approaches, is also crucial for their performance. Extensive experiments on four reasoning benchmarks with both binary and ranking feedback demonstrate the effectiveness of AFT."}, {"title": "Collaborating with language models for embodied reasoning", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Rethinking Capability of LLM-based Agents/Modular Coordination Methods/Collaborating with language models for embodied reasoning.pdf", "summary": {"paper_id": "survey-64_paper-45_Collaborating with language models for embodied reasoning", "core_task": {"text": "Embodied reasoning with language models in interactive environments", "query_variants": ["Embodied reasoning with language models in interactive environments", "Language model reasoning for embodied agents in interactive settings", "Interactive environment reasoning using embodied language models"]}, "contributions": [{"name": "Combining reasoning ability of language models with embodied agent capabilities", "description": "The paper addresses the challenge of integrating the strong reasoning and generalization capabilities of large language models with the embodied interaction abilities of reinforcement learning agents, as neither approach alone can effectively handle complex reasoning in interactive environments.", "author_claim_text": "While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment.", "source_hint": "Abstract"}, {"name": "Leveraging language model reasoning for embodied tasks requiring information gathering", "description": "The authors are motivated by the difficulty of logical reasoning in embodied settings and the potential to leverage recent advances in language model reasoning to address this challenge, particularly for tasks requiring explicit information gathering and multi-step planning.", "author_claim_text": "Achieving complex tasks in embodied environments often requires logical reasoning. Such logical reasoning has been a challenge for machine learning – even more so with embodied agents, where the agent also has to perceive and control in its environment, in addition to reasoning about how to accomplish a complex task. Recent large scale language models (LSLMs), however, have shown great promise for reasoning. Can this complex reasoning ability be used for embodied tasks?", "source_hint": "Introduction paragraph 1"}, {"name": "Planner-Actor-Reporter paradigm", "description": "The authors propose a three-component system where a pre-trained language model (Planner) issues natural language instructions to an embodied RL agent (Actor), while a Reporter module translates environmental observations back to the Planner to enable closed-loop feedback and adaptive planning.", "author_claim_text": "In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command.", "source_hint": "Abstract"}, {"name": "Reinforcement learning training for Reporter module", "description": "The authors demonstrate that the Reporter component can be trained from scratch using reinforcement learning to learn what information is most helpful to communicate to the Planner, rather than relying solely on hand-coded or pre-trained perception modules.", "author_claim_text": "We believe that building an effective Reporter module should combine both approaches: using a pre-trained module to bootstrap perception and grounding, and then using RL to finetune the pre-trained module to communicate with the Planner module. Our investigations show that Reporter training with RL is indeed viable and beneficial.", "source_hint": "Section 4 and Discussion"}]}, "abs": "Reasoning in a complex and ambiguous environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance."}, {"title": "Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Rethinking Capability of LLM-based Agents/Modular Coordination Methods/Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents.pdf", "summary": {"paper_id": "survey-64_paper-46_Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents", "core_task": {"text": "embodied instruction following with language model assistance", "query_variants": ["embodied instruction following with language model assistance", "language model aided embodied task execution from instructions", "grounded instruction execution using language model support"]}, "contributions": [{"name": "Limitations of using LLMs directly as embodied agents", "description": "The paper identifies that while LLMs can generate abstract plans, directly using them as agents faces challenges including limited context windows for long observations, inefficiency in fine-tuning, pre-training biases, and inability to handle non-textual modalities in embodied environments.", "author_claim_text": "the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments", "source_hint": "Abstract"}, {"name": "Leveraging LLM knowledge without replacing trainable actors", "description": "The authors are motivated to exploit procedural and common-sense knowledge encoded in pre-trained LLMs while preserving the trainability and adaptability of low-level control policies, rather than forcing LLMs to act as the agent itself.", "author_claim_text": "To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it", "source_hint": "Abstract"}, {"name": "Plan, Eliminate, and Track (PET) framework", "description": "PET is a three-module framework where Plan uses an LLM to decompose tasks into sub-tasks via in-context prompting, Eliminate uses zero-shot QA to filter irrelevant objects and receptacles from observations, and Track uses zero-shot QA to detect sub-task completion and advance progress.", "author_claim_text": "We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task", "source_hint": "Abstract and Section 3"}, {"name": "Action Attention agent architecture", "description": "A transformer-based agent that handles dynamic action spaces by computing attention-like scores over permissible actions, using separate query and key heads to score each action based on task embedding, observation history, and current state.", "author_claim_text": "the Action Attention agent uses a transformer-based architecture to accommodate for long roll-out and variable length action space", "source_hint": "Introduction and Section 3.4"}]}, "abs": "Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications."}, {"title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Rethinking Capability of LLM-based Agents/Modular Coordination Methods/Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents.pdf", "summary": {"paper_id": "survey-64_paper-47_Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents", "core_task": {"text": "Interactive planning with large language models for open-world multi-task agents", "query_variants": ["Interactive planning with large language models for open-world multi-task agents", "LLM-based interactive planning for multi-task agents in open-world environments", "Large language model driven planning for open-domain multi-objective agent systems"]}, "contributions": [{"name": "Task planning for multi-task agents in open-world environments", "description": "The paper addresses the challenge of enabling multi-task embodied agents to plan and execute tasks in open-world environments like Minecraft. Two core difficulties are identified: the need for accurate multi-step reasoning over long horizons, and the inefficiency or infeasibility of plans that ignore the agent's current state when selecting among parallel sub-goals.", "author_claim_text": "We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible.", "source_hint": "Abstract"}, {"name": "Achieving robust multi-task agents in complex open-world domains", "description": "The authors are motivated by the goal of building generally capable AI systems. They view the ability to handle diverse tasks in complex, open-ended environments as a critical milestone, and existing methods fail to transfer successfully to such domains.", "author_claim_text": "Developing multi-task agents that can accomplish a vast and diverse suite of tasks in complex domains has been viewed as one of the key milestones towards generally capable artificial intelligence.", "source_hint": "Introduction §1"}, {"name": "Describe, Explain, Plan and Select (DEPS) interactive planning framework", "description": "DEPS is an interactive planning method that uses LLMs to iteratively refine plans. It incorporates a descriptor that summarizes execution outcomes, an explainer that identifies errors in previous plans via self-explanation, and a planner that generates updated plans based on this feedback, enabling robust error correction during task execution.", "author_claim_text": "To this end, we propose 'Describe, Explain, Plan and Select' (DEPS), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated plan by integrating description of the plan execution process and providing self-explanation of feedback when encountering failures during the extended planning phases.", "source_hint": "Abstract and Section 3.2"}, {"name": "Horizon-predictive goal selector", "description": "The goal selector is a learned neural network that predicts the remaining time steps (horizon) to complete each candidate sub-goal from the current state. It ranks parallel sub-goals by their proximity and feasibility, selecting the most efficient goal to execute next, thereby improving plan efficiency.", "author_claim_text": "Furthermore, it includes a goal selector, which is a trainable module that ranks parallel candidate sub-goals based on the estimated steps of completion, consequently refining the initial plan.", "source_hint": "Abstract and Section 3.3"}]}, "abs": "We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose \"$\\underline{D}$escribe, $\\underline{E}$xplain, $\\underline{P}$lan and $\\underline{S}$elect\" ($\\textbf{DEPS}$), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated $\\textit{plan}$ by integrating $\\textit{description}$ of the plan execution process and providing self-$\\textit{explanation}$ of feedback when encountering failures during the extended planning phases. Furthermore, it includes a goal $\\textit{selector}$, which is a trainable module that ranks parallel candidate sub-goals based on the estimated steps of completion, consequently refining the initial plan. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method's general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the $\\texttt{ObtainDiamond}$ grand challenge with our approach. The code is released at this https URL."}, {"title": "Dialogue Shaping: Empowering Agents through NPC Interaction", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Rethinking Capability of LLM-based Agents/Reinforcement Learning Methods/Dialogue Shaping: Empowering Agents through NPC Interaction.pdf", "summary": {"paper_id": "survey-64_paper-48_Dialogue Shaping: Empowering Agents through NPC Interaction", "core_task": {"text": "Accelerating reinforcement learning in text-based games through NPC dialogue", "query_variants": ["Accelerating reinforcement learning in text-based games through NPC dialogue", "Speeding up RL in text games using non-player character conversations", "Enhancing reinforcement learning efficiency in textual games via NPC interactions"]}, "contributions": [{"name": "Slow convergence of RL agents in text-based games with extensive action spaces", "description": "The paper addresses the challenge that reinforcement learning agents require extensive trial-and-error steps to converge to optimal policies in text-based games, where action spaces are large and exploration is inefficient without prior knowledge.", "author_claim_text": "One major challenge in reinforcement learning (RL) is the large amount of steps for the RL agent needs to converge in the training process and learn the optimal policy, especially in text-based game environments where the action space is extensive.", "source_hint": "Abstract"}, {"name": "Leveraging NPC knowledge to reduce RL agent's policy search space", "description": "The authors are motivated by the observation that non-player characters hold key game information that can help RL agents avoid wasteful exploration. By extracting and using this information, agents can focus on relevant subgoals rather than exhaustive trial-and-error.", "author_claim_text": "In text-adventure games, NPCs possess crucial information that could spare the agent from extensive trial-and-error. Utilizing this prior knowledge could significantly reduce the agent's policy search space, making it more efficient by breaking down complex tasks into smaller, focused objectives.", "source_hint": "Introduction paragraph 1"}, {"name": "Dialogue Shaping framework combining LLMs, knowledge graphs, and Story Shaping", "description": "The authors propose Dialogue Shaping, a framework where large language models converse with NPCs to extract game information, convert it into knowledge graphs, and then use Story Shaping to provide additional reward signals that guide the RL agent toward optimal policies more quickly.", "author_claim_text": "In this paper, we propose Dialogue Shaping, a framework that is able to extract useful information through conversation with NPCs, and then convert the information into knowledge graphs which are then used to speed up RL agent's convergence to optimal policy by using the Story Shaping technique.", "source_hint": "Introduction paragraph 3"}, {"name": "Using ChatGPT as dialogue module for NPC interaction", "description": "The method employs large language models (specifically ChatGPT) as a dialogue module that interacts with NPCs through prompting, retrieving crucial game information that would otherwise require extensive exploration by the RL agent.", "author_claim_text": "Large Language Models (LLMs) are incredibly capable of conversational tasks and are highly configurable using prompting techniques. Thus, we chose to use them as the dialogue module responsible for talking to the NPC.", "source_hint": "Introduction paragraph 2"}]}, "abs": "One major challenge in reinforcement learning (RL) is the large amount of steps for the RL agent needs to converge in the training process and learn the optimal policy, especially in text-based game environments where the action space is extensive. However, non-player characters (NPCs) sometimes hold some key information about the game, which can potentially help to train RL agents faster. Thus, this paper explores how to interact and converse with NPC agents to get the key information using large language models (LLMs), as well as incorporate this information to speed up RL agent's training using knowledge graphs (KGs) and Story Shaping."}, {"title": "Large Language Models can Implement Policy Iteration", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Rethinking Capability of LLM-based Agents/Reinforcement Learning Methods/Large Language Models can Implement Policy Iteration.pdf", "summary": {"paper_id": "survey-64_paper-49_Large Language Models can Implement Policy Iteration", "core_task": {"text": "policy iteration using large language models", "query_variants": ["policy iteration using large language models", "iterative policy improvement with LLMs", "policy refinement through large language model iteration"]}, "contributions": [{"name": "Limitations of existing foundation model approaches to RL", "description": "Existing approaches to applying foundation models to reinforcement learning suffer from two main limitations: they require labor-intensive expert demonstrations that limit performance to expert level, or they depend on gradient-based training methods that sacrifice the few-shot quality of in-context learning.", "author_claim_text": "Most work in this area either (1) assumes access to expert demonstrations — collected either from human experts or domain-specific pre-trained RL agents — or (2) relies on gradient-based methods — e.g. fine-tuning of the foundation models parameters as a whole or newly training an adapter layer", "source_hint": "Introduction paragraph 2"}, {"name": "Enabling RL without demonstrations or gradients", "description": "The authors aim to develop a method that can perform reinforcement learning using only in-context learning capabilities of large language models, eliminating the need for both expert demonstrations and gradient-based parameter updates while still enabling policy improvement.", "author_claim_text": "Our work demonstrates an approach to in-context learning which relaxes these assumptions. Our method, In-Context Policy Iteration (ICPI), implements policy iteration using the prompt content, instead of the model parameters, as the locus of learning, thereby avoiding gradient methods.", "source_hint": "Introduction paragraph 3"}, {"name": "In-Context Policy Iteration (ICPI)", "description": "ICPI implements model-based policy iteration where the LLM serves as both world model and rollout policy. The method computes Q-values by generating rollout trajectories using the LLM, then selects actions greedily with respect to these Q-values. Policy improvement occurs by storing improved behavior in a replay buffer and using it to condition future prompts.", "author_claim_text": "Our method demonstrates that a large language model can be used to implement policy iteration using the machinery of in-context learning, enabling it to learn to perform RL tasks without expert demonstrations or gradients. Our approach iteratively updates the contents of the prompt from which it derives its policy through trial-and-error interaction with an RL environment.", "source_hint": "Abstract and Method section"}, {"name": "LLM-based Q-value computation via rollouts", "description": "Q-values are estimated by using the LLM to generate complete rollout trajectories from a given state-action pair. The LLM predicts rewards, terminations, next states, and actions by being prompted with relevant transitions from the agent's experience buffer, with careful selection and balancing of prompt content to maximize relevance.", "author_claim_text": "To compute Q(πt)(ot, a) at time step t in the real-world we rollout a simulated trajectory by predicting, at each simulation time step u: reward, termination, observation, action. The prompts contain data sampled from the replay buffer.", "source_hint": "Method section, Computing Q-values"}]}, "abs": "This work presents In-Context Policy Iteration, an algorithm for performing Reinforcement Learning (RL), in-context, using foundation models. While the application of foundation models to RL has received considerable attention, most approaches rely on either (1) the curation of expert demonstrations (either through manual design or task-specific pretraining) or (2) adaptation to the task of interest using gradient methods (either fine-tuning or training of adapter layers). Both of these techniques have drawbacks. Collecting demonstrations is labor-intensive, and algorithms that rely on them do not outperform the experts from which the demonstrations were derived. All gradient techniques are inherently slow, sacrificing the \"few-shot\" quality that made in-context learning attractive to begin with. In this work, we present an algorithm, ICPI, that learns to perform RL tasks without expert demonstrations or gradients. Instead we present a policy-iteration method in which the prompt content is the entire locus of learning. ICPI iteratively updates the contents of the prompt from which it derives its policy through trial-and-error interaction with an RL environment. In order to eliminate the role of in-weights learning (on which approaches like Decision Transformer rely heavily), we demonstrate our algorithm using Codex, a language model with no prior knowledge of the domains on which we evaluate it."}, {"title": "Large Language Models Are Semi-Parametric Reinforcement Learning Agents", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Rethinking Capability of LLM-based Agents/Reinforcement Learning Methods/Large Language Models Are Semi-Parametric Reinforcement Learning Agents.pdf", "summary": {"paper_id": "survey-64_paper-50_Large Language Models Are Semi-Parametric Reinforcement Learning Agents", "core_task": {"text": "Reinforcement learning with experience memory for language model agents", "query_variants": ["Reinforcement learning with experience memory for language model agents", "RL agents using replay buffers for LLM-based systems", "Memory-augmented reinforcement learning for language agent training"]}, "contributions": [{"name": "LLMs struggle to effectively learn from interaction experiences", "description": "Current methods for LLM-based agents face challenges in utilizing past interaction experiences. Fine-tuning LLMs is expensive, and in-context approaches are limited by input length constraints, preventing effective learning from extensive interaction histories.", "author_claim_text": "existing approaches still struggle to enable LLMs to effectively learn from interaction experiences", "source_hint": "Introduction paragraph 2"}, {"name": "Episodic memory is crucial for human decision-making", "description": "Inspired by cognitive science insights about human memory and reasoning, the authors aim to enable agents to learn from both successful and failed experiences across different task goals, similar to how humans use episodic memory for decision-making.", "author_claim_text": "the episodic memory of the experiences from past episodes plays a crucial role in the complex decision-making processes of human", "source_hint": "Introduction paragraph 1"}, {"name": "REMEMBERER agent framework with long-term experience memory", "description": "REMEMBERER is an agent framework that augments an LLM with a persistent external experience memory storing task information, observations, actions, and Q-value estimations. This memory enables the agent to exploit experiences from past episodes even for different task goals, unlike short-term working memory approaches.", "author_claim_text": "a novel evolvable LLM-based (Large Language Model) agent framework is proposed as REMEMBERER. By equipping the LLM with a long-term experience memory", "source_hint": "Abstract"}, {"name": "Reinforcement Learning with Experience Memory (RLEM)", "description": "RLEM is a learning mechanism that updates the experience memory through an RL process rather than fine-tuning LLM parameters. It uses Q-learning with n-step bootstrapping to update Q-value estimations in the memory, enabling the system to learn from both successful and failed experiences.", "author_claim_text": "We further introduce Reinforcement Learning with Experience Memory (RLEM) to update the memory", "source_hint": "Abstract and Section 3.1"}]}, "abs": "Inspired by the insights in cognitive science with respect to human memory and reasoning mechanism, a novel evolvable LLM-based (Large Language Model) agent framework is proposed as REMEMBERER. By equipping the LLM with a long-term experience memory, REMEMBERER is capable of exploiting the experiences from the past episodes even for different task goals, which excels an LLM-based agent with fixed exemplars or equipped with a transient working memory. We further introduce Reinforcement Learning with Experience Memory (RLEM) to update the memory. Thus, the whole system can learn from the experiences of both success and failure, and evolve its capability without fine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER constitutes a semi-parametric RL agent. Extensive experiments are conducted on two RL task sets to evaluate the proposed framework. The average results with different initialization and training sets exceed the prior SOTA by 4% and 2% for the success rate on two task sets and demonstrate the superiority and robustness of REMEMBERER."}, {"title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Rethinking Capability of LLM-based Agents/Reinforcement Learning Methods/Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization.pdf", "summary": {"paper_id": "survey-64_paper-51_Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization", "core_task": {"text": "Reinforcing large language agents with policy gradient optimization", "query_variants": ["Reinforcing large language agents with policy gradient optimization", "Policy gradient methods for optimizing large language model agents", "Training large language model agents using policy gradient techniques"]}, "contributions": [{"name": "Lack of gradient-based optimization in language agents", "description": "Existing large language model agents cannot effectively learn from environment rewards through gradient-based optimization. While some agents use verbal feedback for iterative refinement, they lack mechanisms compatible with differentiable learning from rewards, limiting their ability to improve through reinforcement learning techniques.", "author_claim_text": "Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards.", "source_hint": "Abstract"}, {"name": "Need for principled reward-based learning in language agents", "description": "The authors are motivated by the difficulty of generating useful self-reflections from frozen LLMs, which struggle with credit assignment problems. Without proper fine-tuning for specific environments, verbal reinforcement cannot be optimal, necessitating a framework that enables gradient-based learning from environment rewards.", "author_claim_text": "generating useful reflective feedback from a pre-trained, frozen LLM is challenging, as showcased in Fig. 1, since it requires the LLM to have a good understanding of where the agent made mistakes in a specific environment, i.e., the credit assignment problem", "source_hint": "Introduction paragraph 2"}, {"name": "Retroformer framework with retrospective model learning", "description": "Retroformer is a framework that reinforces language agents by learning a plug-in retrospective model. This model automatically refines agent prompts using environment feedback through policy gradient optimization, enabling learning from arbitrary reward signals across multiple environments and tasks without accessing the actor LLM parameters.", "author_claim_text": "This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient.", "source_hint": "Abstract"}, {"name": "Retrospective agent architecture with actor and retrospective LLMs", "description": "The architecture comprises two components: a frozen actor LLM that generates actions, and a fine-tunable retrospective LLM that produces self-reflections. The retrospective model refines actor prompts by diagnosing failure causes and devising improvement plans, with memory modules for short-term trajectory history and long-term reflection responses.", "author_claim_text": "our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans.", "source_hint": "Abstract and Section 4.1"}]}, "abs": "Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment. This demonstrates that using policy gradient optimization to improve language agents, for which we believe our work is one of the first, seems promising and can be applied to optimize other models in the agent architecture to enhance agent performances over time."}, {"title": "REX: Rapid Exploration and eXploitation for AI Agents", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Rethinking Capability of LLM-based Agents/Reinforcement Learning Methods/REX: Rapid Exploration and eXploitation for AI Agents.pdf", "summary": {"paper_id": "survey-64_paper-52_REX: Rapid Exploration and eXploitation for AI Agents", "core_task": {"text": "Rapid exploration and exploitation for AI agents using large language models", "query_variants": ["Rapid exploration and exploitation for AI agents using large language models", "LLM-based agents balancing exploration and exploitation quickly", "Fast exploration-exploitation trade-off in large language model agents"]}, "contributions": [{"name": "Limitations of LLM-based AI agents in exploration and exploitation", "description": "Current LLM-based AI agents struggle with systematically incorporating reward signals, balancing exploration versus exploitation, and suffer from accuracy and latency issues when solving complex multi-step problems. They lack a principled framework for learning from environmental feedback without expensive model fine-tuning.", "author_claim_text": "Existing LLM-driven agents have inherent limitations, such as a heavy reliance on precise descriptions for decision-making, and the lack of a systematic approach to leverage try-and-fail procedures akin to traditional RL.", "source_hint": "Abstract"}, {"name": "Need for efficient reward integration without fine-tuning", "description": "The authors are motivated by the impracticality of fine-tuning large language models for each new environment or task, which is costly and time-consuming. They aim to develop a method that leverages in-context learning to integrate rewards and improve decision-making without requiring parameter updates.", "author_claim_text": "the process of updating the model parameters in the context of LLM-based AI agents entails substantial data collection, is time-intensive, and incurs significant costs. Furthermore, given that each unique environment may necessitate distinct adjustments, fine-tuning LLMs for every environment could be challenging especially for real-world applications.", "source_hint": "Introduction paragraph 2"}, {"name": "REX framework with UCB-based reward integration", "description": "REX is a novel approach that compresses MCTS steps by having the LLM predict entire solution sequences at once, then backpropagates rewards to intermediate steps. It uses UCB scores to guide action selection, mapping them to HIGH or LOW tokens in prompts to systematically balance exploration and exploitation.", "author_claim_text": "REX introduces an additional layer of rewards and integrates concepts similar to Upper Confidence Bound (UCB) scores, leading to more robust and efficient AI agent performance.", "source_hint": "Abstract and Section 4.1"}, {"name": "REX-UCL: UCB applied to LLM logits", "description": "REX-UCL is a variant that directly modifies the logit values of tokens corresponding to actions using UCL scores derived from UCB. This ensures the LLM consistently selects intended actions by adjusting token probabilities, offering finer control over action generation.", "author_claim_text": "Systematically influencing the logits of LLMs through what we term UCL (UCB applied for Logits of LLM), providing enhanced control over the generation of actions.", "source_hint": "Abstract and Section 4.4"}]}, "abs": "In this paper, we propose an enhanced approach for Rapid Exploration and eXploitation for AI Agents called REX. Existing AutoGPT-style techniques have inherent limitations, such as a heavy reliance on precise descriptions for decision-making, and the lack of a systematic approach to leverage try-and-fail procedures akin to traditional Reinforcement Learning (RL). REX introduces an additional layer of rewards and integrates concepts similar to Upper Confidence Bound (UCB) scores, leading to more robust and efficient AI agent performance. This approach has the advantage of enabling the utilization of offline behaviors from logs and allowing seamless integration with existing foundation models while it does not require any model fine-tuning. Through comparative analysis with existing methods such as Chain-of-Thoughts(CoT) and Reasoning viA Planning(RAP), REX-based methods demonstrate comparable performance and, in certain cases, even surpass the results achieved by these existing techniques. Notably, REX-based methods exhibit remarkable reductions in execution time, enhancing their practical applicability across a diverse set of scenarios."}, {"title": "Let's Verify Step by Step", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Rethinking Capability of LLM-based Agents/Supervised Learning Methods/Let's Verify Step by Step.pdf", "summary": {"paper_id": "survey-64_paper-53_Let's Verify Step by Step", "core_task": {"text": "training reliable reward models for mathematical reasoning using step-level supervision", "query_variants": ["training reliable reward models for mathematical reasoning using step-level supervision", "learning dependable reward functions for math problem solving with step-by-step feedback", "developing trustworthy reward models for mathematical problem solving using stepwise supervision"]}, "contributions": [{"name": "Logical mistakes in multi-step reasoning by large language models", "description": "Despite improvements in complex multi-step reasoning, current large language models frequently make logical errors and produce hallucinations, which is particularly problematic in domains requiring multi-step reasoning where a single error can derail an entire solution.", "author_claim_text": "even state-of-the-art models still regularly produce logical mistakes", "source_hint": "Abstract"}, {"name": "Need for reliable reward models to detect and mitigate hallucinations", "description": "Training more reliable models requires effective reward models that can discriminate between desirable and undesirable outputs. Given the high cost of human feedback and the importance of reliability, it is critical to carefully compare different supervision methods for training these reward models.", "author_claim_text": "Detecting and mitigating hallucinations is essential to improve reasoning capabilities", "source_hint": "Introduction paragraph 1-2"}, {"name": "Process-supervised reward models (PRMs) with step-level human feedback", "description": "PRMs are trained using human feedback labels (positive, negative, or neutral) for each intermediate reasoning step in model-generated solutions. The PRM predicts step-level correctness and scores complete solutions as the product of correctness probabilities across all steps.", "author_claim_text": "process-supervised reward models (PRMs) receive feedback for each step in the chain-of-thought", "source_hint": "Introduction paragraph 3, Methods §2.6"}, {"name": "Active learning strategy for data collection", "description": "An active learning approach that strategically selects which solutions to show human labelers by surfacing 'convincing wrong-answer solutions' - those rated highly by the current best PRM but reaching incorrect final answers - to maximize the value of limited human feedback.", "author_claim_text": "we choose to surface convincing wrong-answer solutions", "source_hint": "Methods §2.4"}]}, "abs": "In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model."}, {"title": "Chain of Hindsight Aligns Language Models with Feedback", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Rethinking Capability of LLM-based Agents/Supervised Learning Methods/Chain of Hindsight Aligns Language Models with Feedback.pdf", "summary": {"paper_id": "survey-64_paper-54_Chain of Hindsight Aligns Language Models with Feedback", "core_task": {"text": "Learning from human feedback to align language models with preferences", "query_variants": ["Learning from human feedback to align language models with preferences", "Aligning language models with human preferences through feedback-based learning", "Training language models using human feedback for preference alignment"]}, "contributions": [{"name": "Limitations of existing human feedback learning methods", "description": "Existing approaches for learning from human feedback suffer from two main issues: supervised finetuning relies on positively-rated data only, making it data-inefficient and unable to learn from errors, while RLHF requires learning potentially misaligned reward functions and involves difficult optimization procedures.", "author_claim_text": "these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers from imperfect reward functions and relies on extremely challenging optimizations", "source_hint": "Abstract"}, {"name": "Aligning language models with human values and preferences", "description": "The authors are motivated by the need to ensure language models align with human values and societal norms. Human feedback is essential for evaluating model performance objectively and subjectively, identifying issues with accuracy, fairness, and bias, and ensuring ethical outputs.", "author_claim_text": "in order to ensure that these technologies have a positive impact on society, it is of paramount importance for them to be aligned with human values. One of the most critical elements in achieving this is the use of human feedback", "source_hint": "Introduction §1"}, {"name": "Chain of Hindsight (CoH)", "description": "CoH converts all human feedback (positive and negative) into natural language sequences paired with model outputs. The model is finetuned to predict outputs conditioned on a sequence of model generations and their corresponding feedback, enabling it to learn from both positive and negative examples without reinforcement learning.", "author_claim_text": "we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. We convert all types of feedback into sequences of sentences, which are then used to fine-tune the model", "source_hint": "Abstract and Section 2"}, {"name": "Feedback-conditioned autoregressive training", "description": "The training procedure conditions the language model on sequences combining multiple model outputs with templated or natural language feedback based on human ratings. During training, the model predicts tokens autoregressively while being conditioned on feedback like 'Bad' and 'Good', learning to generate outputs aligned with positive feedback.", "author_claim_text": "We condition the model on a sequence of model generations paired with feedback. By doing so, the model is trained to generate outputs based on feedback, while learning to identify and correct negative attributes or errors", "source_hint": "Abstract and Section 2"}]}, "abs": "Learning from human preferences is important for language models to match human needs and to align with human and social values. Prior works have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers from imperfect reward functions and relies on extremely challenging optimizations. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sequences of sentences, which are then used to fine-tune the model, allowing us to take advantage of the language comprehension capabilities of language models. We condition the model on a sequence of model generations paired with feedback. By doing so, the model is trained to generate outputs based on feedback, while learning to identify and correct negative attributes or errors. Applying our method to large language models, we observed that Chain of Hindsight significantly surpasses previous methods in aligning language models with human preferences. We report significant improvements on summarization and dialogue benchmarks, with our approach markedly preferred in human evaluations."}, {"title": "Introspective Tips: Large Language Model for In-Context Decision Making", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Rethinking Capability of LLM-based Agents/Supervised Learning Methods/Introspective Tips: Large Language Model for In-Context Decision Making.pdf", "summary": {"paper_id": "survey-64_paper-55_Introspective Tips: Large Language Model for In-Context Decision Making", "core_task": {"text": "in-context decision making with large language models", "query_variants": ["in-context decision making with large language models", "decision making using large language models with in-context learning", "large language models for in-context decision tasks"]}, "contributions": [{"name": "LLM errors and limited generalization in domain-specific decision-making", "description": "The paper addresses the challenge that large language models exhibit errors and hallucinations in domain-specific decision-making scenarios. Existing self-correction methods like reflections focus on individual agent mistakes and lack generalization across different agents and contexts.", "author_claim_text": "LLMs sometimes demonstrate errors or hallucinations, especially in domain-specific scenarios. Recent works focus on designing self-correction mechanisms to enhance the LLM's decision-making performance.", "source_hint": "Introduction paragraph 2"}, {"name": "Enabling LLM self-optimization through human-like introspection", "description": "The authors are motivated by humans' natural ability to reflect on experiences, distill key insights, and extract lessons applicable to new situations. They aim to empower LLMs with similar self-optimizing capabilities that can generalize across different agents and contexts.", "author_claim_text": "Humans possess high generalization and self-evolution capabilities due to their natural ability to summarize tips through introspection. Inspired by the remarkable introspection capabilities of humans, we introduce 'Introspective Tips'", "source_hint": "Introduction paragraph 3"}, {"name": "Introspective Tips paradigm for self-optimizing decision-making", "description": "The method introduces Introspective Tips as compact, high-level guidance extracted from trajectories. Unlike reflections that focus on specific failures, tips provide succinct practical advice that can be shared among different agents and generalized across contexts without requiring LLM fine-tuning.", "author_claim_text": "We introduce 'Introspective Tips' as a novel prompt-based paradigm, empowering an LLM decision-making agent with self-optimizing capabilities via learning tips from trajectories of itself, experts, and other environments.", "source_hint": "Introduction paragraph 3 and Contributions bullet 1"}, {"name": "Multiple learning strategies for different scenarios", "description": "The framework includes three strategies: learning from the agent's own history through replay and tip generation, learning from expert demonstrations by comparing successful and failed actions, and learning from multiple games to generate general tips that work across diverse settings.", "author_claim_text": "We present distinct strategies applicable in different scenarios to prompt LLMs to learn Introspective Tips from their own trajectories, expert demonstrations, and multi-environment trajectories", "source_hint": "Contributions bullet 2 and Method section 3.3"}]}, "abs": "The emergence of large language models (LLMs) has substantially influenced natural language processing, demonstrating exceptional results across various tasks. In this study, we employ ``Introspective Tips\" to facilitate LLMs in self-optimizing their decision-making. By introspectively examining trajectories, LLM refines its policy by generating succinct and valuable tips. Our method enhances the agent's performance in both few-shot and zero-shot learning situations by considering three essential scenarios: learning from the agent's past experiences, integrating expert demonstrations, and generalizing across diverse games. Importantly, we accomplish these improvements without fine-tuning the LLM parameters; rather, we adjust the prompt to generalize insights from the three aforementioned situations. Our framework not only supports but also emphasizes the advantage of employing LLM in in-contxt decision-making. Experiments involving over 100 games in TextWorld illustrate the superior performance of our approach."}, {"title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification", "path": "/mnt/data/ln_survey_data/[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects/Rethinking Capability of LLM-based Agents/Supervised Learning Methods/Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification.pdf", "summary": {"paper_id": "survey-64_paper-56_Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification", "core_task": {"text": "mathematical reasoning with code generation and self-verification", "query_variants": ["mathematical reasoning with code generation and self-verification", "code-based mathematical problem solving with automated verification", "program synthesis for math reasoning with self-checking mechanisms"]}, "contributions": [{"name": "Limited mathematical reasoning capability of LLMs despite code generation", "description": "The paper addresses the challenge that large language models, despite advances in code generation, continue to struggle with mathematical reasoning tasks, producing errors and inaccuracies in complex calculations.", "author_claim_text": "Large language models (LLMs) ... still fall short in mathematical reasoning, often producing nonsensical or inaccurate content and struggling with complex calculations.", "source_hint": "Introduction paragraph 1"}, {"name": "Understanding and enhancing GPT-4 Code Interpreter's math problem-solving abilities", "description": "The authors are motivated by the lack of systematic analysis of GPT-4 Code Interpreter's strong performance on math problems and seek to understand its mechanisms to further improve its capabilities.", "author_claim_text": "While GPT4-Code has demonstrated proficiency in solving math problems, there has been a notable absence of systematic analysis focusing on understanding and further enhancing its mathematical problem-solving abilities.", "source_hint": "Introduction paragraph 3"}, {"name": "Code Usage Frequency analysis with code-constrained prompts", "description": "The authors introduce Code Usage Frequency as a metric and design code-constrained prompts (no code, single code use, unrestricted code) to systematically analyze how code generation and execution frequency impacts GPT-4 Code Interpreter's mathematical reasoning performance.", "author_claim_text": "we introduce the concept of Code Usage Frequency to differentiate these unique prompting strategies to quantitatively analyze the impact of code-constrained prompts on GPT4-Code for mathematical problem-solving.", "source_hint": "Introduction paragraph 4"}, {"name": "Explicit code-based self-verification (CSV) prompting", "description": "The authors propose CSV, a zero-shot prompting technique that guides GPT-4 Code Interpreter to generate verification code for its answers and automatically correct solutions when verification fails, leveraging the model's self-debugging capabilities without external verifiers.", "author_claim_text": "we propose a novel and effective prompting method, explicit code-based self-verification (CSV), to further boost the mathematical reasoning potential of GPT-4 Code Interpreter. This method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to use code to self-verify its answers.", "source_hint": "Abstract and Section 3.2"}]}, "abs": "Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has brought significant advancements in addressing math reasoning problems. In particular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter, shows remarkable performance on challenging math datasets. In this paper, we explore the effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the \\textit{Code Usage Frequency} of GPT-4 Code Interpreter. We found that its success can be largely attributed to its powerful skills in generating and executing code, evaluating the output of code execution, and rectifying its solution when receiving unreasonable outputs. Based on this insight, we propose a novel and effective prompting method, explicit \\uline{c}ode-based \\uline{s}elf-\\uline{v}erification~(CSV), to further boost the mathematical reasoning potential of GPT-4 Code Interpreter. This method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to use code to self-verify its answers. In instances where the verification state registers as ``False'', the model shall automatically amend its solution, analogous to our approach of rectifying errors during a mathematics examination. Furthermore, we recognize that the states of the verification result indicate the confidence of a solution, which can improve the effectiveness of majority voting. With GPT-4 Code Interpreter and CSV, we achieve an impressive zero-shot accuracy on MATH dataset \\textbf{(53.9\\% $\\to$ 84.3\\%)}."}], "survey_topic_path": "[162]Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects", "survey_topic": "Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects", "gt_paper_count": 57, "gt": {"name": "Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects", "subtopics": [{"name": "Actions of LLM-based Agents", "subtopics": [{"name": "Tool Creation", "papers": ["CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets", "Large Language Models as Tool Makers"]}, {"name": "Tool Employment", "papers": ["RestGPT: Connecting Large Language Models with Real-World RESTful APIs", "TALM: Tool Augmented Language Models", "Gorilla: Large Language Model Connected with Massive APIs", "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face", "ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models", "Toolformer: Language Models Can Teach Themselves to Use Tools", "LLM As DBA", "MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning", "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"]}, {"name": "Tool Planning", "papers": ["ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs", "TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage", "Gentopia: A Collaborative Platform for Tool-Augmented LLMs"]}]}, {"name": "Memory Capability of LLM-based Agents", "subtopics": [{"name": "Long-term Memory", "papers": ["A Survey of Knowledge Graph Embedding and Their Applications"]}]}, {"name": "Planning Capability of LLM-based Agents", "subtopics": [{"name": "External Methods", "papers": ["Dynamic Planning with a LLM", "Reasoning with Language Model is Planning with World Model", "LLM+P: Empowering Large Language Models with Optimal Planning Proficiency", "Context-Aware Composition of Agent Policies by Markov Decision Process Entity Embeddings and Agent Ensembles", "Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis"]}, {"name": "In-Context Learning Methods", "papers": ["Self-Refine: Iterative Refinement with Self-Feedback", "Complexity-Based Prompting for Multi-Step Reasoning", "Automatic Chain of Thought Prompting in Large Language Models", "Chain-of-Thought in Large Language Models: Decoding, Projection, and Activation", "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "Large Language Models are Zero-Shot Reasoners", "Graph of Thoughts: Solving Elaborate Problems with Large Language Models", "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "Progressive-Hint Prompting Improves Reasoning in Large Language Models", "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"]}, {"name": "Multi-stage Methods", "papers": ["Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling", "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks"]}]}, {"name": "Relationship of Multi-Agent", "subtopics": [{"name": "Competitive", "papers": ["ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate", "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate"]}, {"name": "Cooperative", "papers": ["Building Cooperative Embodied Agents Modularly with Large Language Models", "BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents", "AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors", "CGMI: Configurable General Multi-Agent Interaction Framework"]}, {"name": "Hierarchical", "papers": ["AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation"]}, {"name": "Mixed", "papers": ["AvalonBench: Evaluating LLMs Playing the Game of Avalon", "Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf"]}]}, {"name": "Rethinking Capability of LLM-based Agents", "subtopics": [{"name": "In-Context Learning Methods", "papers": ["Reflexion: Language Agents with Verbal Reinforcement Learning", "ReAct: Synergizing Reasoning and Acting in Language Models"]}, {"name": "Modular Coordination Methods", "papers": ["Making Large Language Models Better Reasoners with Alignment", "Collaborating with language models for embodied reasoning", "Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents", "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents"]}, {"name": "Reinforcement Learning Methods", "papers": ["Dialogue Shaping: Empowering Agents through NPC Interaction", "Large Language Models can Implement Policy Iteration", "Large Language Models Are Semi-Parametric Reinforcement Learning Agents", "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization", "REX: Rapid Exploration and eXploitation for AI Agents"]}, {"name": "Supervised Learning Methods", "papers": ["Let's Verify Step by Step", "Chain of Hindsight Aligns Language Models with Feedback", "Introspective Tips: Large Language Model for In-Context Decision Making", "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification"]}]}]}, "input_paper_count": 57, "input_content": "SYSTEM PROMPT:\nYou are a senior researcher and survey-author with deep experience in structuring\nhigh-quality academic survey papers.\n\nYour task is to organize a set of research papers into a **hierarchical topic tree**\n(bottom-up), given:\n- a survey topic,\n- for each paper: title, abstract, and an structured summary\n  (which may include research problem, motivation, methodology, and findings).\n\nYour goal is NOT just to cluster by surface similarity, but to produce a taxonomy\nthat would be considered **reasonable, informative, and defensible** in a top-tier survey paper.\n\n### Hard Constraints\n1. Output must be **strictly valid JSON**.\n2. Only leaf nodes may contain `\"papers\"`; all internal nodes must contain `\"subtopics\"`.\n3. **Every paper must appear exactly once** in the entire tree.\n4. NO duplicate papers anywhere.\n5. The tree must eventually merge into **one single root node**.\n\n### CLASSIFICATION RULES\n- Group papers by semantic similarity using both title + abstract.\n- Create meaningful names for leaf-level themes.\n\n### ANTI-DUPLICATION PROCEDURE (MANDATORY)\nBefore constructing the tree:\n1. Produce an internal list of all given paper titles.\n2. Assign each paper to exactly one leaf node.\n3. After assignment, verify that:\n   - the number of assigned papers equals the number of input papers,\n   - no paper appears in more than one group.\n\n### Output Format\nUse a JSON structure like this (replace placeholders with actual paper titles) and\nthe output you produce MUST be wrapped inside a fenced code block:\n\n```json\n{\n  \"name\": \"AI Research\",\n  \"subtopics\": [\n    {\n      \"name\": \"NLP\",\n      \"subtopics\": [\n        {\n          \"name\": \"Text Summarization\",\n          \"papers\": [\"<actual paper titles>\"]\n        },\n        {\n          \"name\": \"Machine Translation\",\n          \"papers\": [\"<actual paper titles>\"]\n        }\n      ]\n    }\n  ]\n}```\n\nUSER PROMPT:\nPerform a bottom-up hierarchical clustering of the following 57 papers and produce a JSON research topic tree.\n\nSurvey Topic: Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects\n\nPaper List:\nPaper 1:\n  Title: CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets\n  Abstract: Large language models (LLMs) are often augmented with tools to solve complex tasks. By generating code snippets and executing them through task-specific Application Programming Interfaces (APIs), they can offload certain functions to dedicated external modules, such as image encoding and performing calculations. However, most existing approaches to augment LLMs with tools are constrained by general-purpose APIs and lack the flexibility for tailoring them to specific tasks. In this work, we present CRAFT, a general tool creation and retrieval framework for LLMs. It creates toolsets specifically curated for the tasks and equips LLMs with a component that retrieves tools from these sets to enhance their capability to solve complex tasks. For each task, we collect specific code solutions by prompting GPT-4 to solve the training examples. Following a validation step ensuring the correctness, these solutions are abstracted into code snippets to enhance reusability, and deduplicated for higher quality. At inference time, the language model retrieves snippets from the toolsets and then executes them or generates the output conditioning on the retrieved snippets. Our method is designed to be flexible and offers a plug-and-play approach to adapt off-the-shelf LLMs to unseen domains and modalities, without any finetuning. Experiments on vision-language, tabular processing, and mathematical reasoning tasks show that our approach achieves substantial improvements compared to strong baselines. In addition, our in-depth analysis reveals that: (1) consistent performance improvement can be achieved by scaling up the number of tools and the capability of the backbone models; (2) each component of our approach contributes to the performance gains; (3) the created tools are well-structured and reliable with low complexity and atomicity. The code is available at this https URL.\n  Structured Summary:\n    - Limited flexibility of general-purpose APIs for task-specific LLM augmentation: Existing methods for augmenting large language models with external tools rely on general-purpose APIs that cannot be easily customized for specific downstream tasks, limiting their effectiveness in specialized domains.\n    - Need for customizable toolsets to adapt LLMs to specialized domains: The authors are motivated by the observation that many real-world applications require specialized APIs not covered by general-purpose tool collections, necessitating a framework that can create and retrieve task-specific tools to customize LLMs for diverse domains without extensive fine-tuning.\n    - CRAFT framework for tool creation and retrieval: CRAFT is a plug-and-play framework that constructs task-specific toolsets by generating, abstracting, validating, and deduplicating code solutions from training examples, then retrieves relevant tools at inference time using multi-view matching based on problem descriptions, API names, and docstrings.\n    - Four-step tool creation pipeline: The tool creation process involves iteratively sampling diverse problems, generating specific code solutions with GPT-4, abstracting them into reusable snippets with generic variable names and docstrings, validating correctness on original problems, and deduplicating to reduce redundancy.\n\nPaper 2:\n  Title: Large Language Models as Tool Makers\n  Abstract: Recent research has highlighted the potential of large language models (LLMs) to improve their problem-solving capabilities with the aid of suitable external tools. In our work, we further advance this concept by introducing a closed-loop framework, referred to as LLMs A s Tool Makers (LATM), where LLMs create their own reusable tools for problem-solving. Our approach consists of two phases: 1) tool making: an LLM acts as the tool maker that crafts tools for a set of tasks. 2) tool using: another LLM acts as the tool user, which applies the tool built by the tool maker for problem-solving. On the problem-solving server side, tool-making enables continual tool generation and caching as new requests emerge. This framework enables subsequent requests to access cached tools via their corresponding APIs, enhancing the efficiency of task resolution. Recognizing that tool-making requires more sophisticated capabilities, we assign this task to a powerful, albeit resource-intensive, model. Conversely, the simpler tool-using phase is delegated to a lightweight model. This strategic division of labor allows the once-off cost of tool-making to be spread over multiple instances of tool-using, significantly reducing average costs while maintaining strong performance. Furthermore, our method offers a functional cache through the caching and reuse of tools, which stores the functionality of a class of requests instead of the natural language responses from LLMs, thus extending the applicability of the conventional cache mechanism. We evaluate our approach across various complex reasoning tasks, including Big-Bench tasks. With GPT-4 as the tool maker and GPT-3.5 as the tool user, LATM demonstrates performance equivalent to using GPT-4 for both roles, but with a significantly reduced inference cost.\n  Structured Summary:\n    - Limited applicability of tool-using methods due to tool availability: Existing methods for augmenting LLMs with external tools are constrained by whether appropriate tools already exist for the tasks at hand, limiting their broader application.\n    - Enabling LLMs to fabricate their own tools for emerging challenges: The authors are motivated by the evolutionary milestone where humans learned to create their own tools. They aim to replicate this capability in LLMs, allowing models to generate reusable tools for new tasks rather than relying solely on pre-existing tools.\n    - LATM closed-loop framework with tool making and tool using stages: LATM is a two-stage framework where a powerful LLM (tool maker) creates reusable Python function tools from a few demonstrations, and a lightweight LLM (tool user) applies these tools to solve new task instances. This division of labor reduces overall computational cost while maintaining high performance.\n    - Functional cache mechanism for LLM serving: LATM introduces a functional cache that stores generated tools (functions) rather than textual responses. A dispatcher LLM identifies whether an incoming request can be solved with cached tools or requires new tool creation, thereby reducing serving costs for recurring task types.\n\nPaper 3:\n  Title: RestGPT: Connecting Large Language Models with Real-World RESTful APIs\n  Abstract: Tool-augmented large language models (LLMs) have achieved remarkable progress in tackling a broad range of tasks. However, existing methods are mainly restricted to specifically designed tools and fail to fulfill complex instructions, having great limitations when confronted with real-world scenarios. In this paper, we explore a more realistic scenario by connecting LLMs with RESTful APIs, which adhere to the widely adopted REST software architectural style for web service development. To address the practical challenges of tackling complex instructions, we propose RestGPT, which exploits the power of LLMs and conducts a coarse-to-fine online planning mechanism to enhance the abilities of task decomposition and API selection. RestGPT also contains an API executor tailored for calling RESTful APIs, which can meticulously formulate parameters and parse API responses. To fully evaluate the performance of RestGPT, we propose RestBench, a high-quality benchmark which consists of two real-world scenarios and human-annotated instructions with gold solution paths. Experiments show that RestGPT is able to achieve impressive results in complex tasks and has strong robustness, which paves a new way towards AGI. RestGPT and RestBench is publicly available at this https URL.\n  Structured Summary:\n    - Limitations of existing tool-augmented LLMs in real-world scenarios: Current tool-augmented large language models are limited to small numbers of specially designed APIs and cannot effectively handle complex real-world instructions that require decomposing tasks and using multiple APIs in combination.\n    - Connecting LLMs with real-world RESTful APIs for practical applications: The authors aim to bridge the gap between LLMs and real-world applications by leveraging RESTful APIs, which are the de facto standard for web services, thereby enabling LLMs to interact with existing applications in a standardized and extensible manner.\n    - RestGPT framework with coarse-to-fine online planning: RestGPT employs an iterative planning approach where a Planner generates high-level natural language sub-tasks and an API Selector maps these to specific API calls, forming a flexible coarse-to-fine planning mechanism that adapts dynamically to execution feedback.\n    - Specialized API executor for RESTful APIs: The executor consists of a Caller that generates correct API parameters using API documentation and a schema-based Parser that leverages OpenAPI Specification response schemas to generate Python code for extracting information from complex JSON responses.\n\nPaper 4:\n  Title: TALM: Tool Augmented Language Models\n  Abstract: Transformer based language models (LMs) demonstrate increasing performance with scale across a wide variety of tasks. Scale alone however cannot enable models to solve tasks that require access to ephemeral, changing, or private data that was unavailable at training time. Many useful tasks may also benefit from LMs being able to access APIs that read or modify state. In this work, we present Tool Augmented Language Models (TALM), combining a text-only approach to augment language models with non-differentiable tools, and an iterative \"self-play\" technique to bootstrap performance starting from few tool demonstrations. TALM exhibits strong performance on both a knowledge-heavy QA task and a reasoning oriented math task with simple tools. At a given model scale, TALM significantly outperforms non-augmented LMs. We further demonstrate that TALM successfully performs out-of-distribution inferences on both QA and math tasks, where non-augmented LMs fail. Our results suggest that Tool Augmented Language Models are a promising direction to enrich LMs' capabilities, with less dependence on scale.\n  Structured Summary:\n    - Language models cannot access ephemeral, changing, or private data unavailable at training time: The paper addresses the fundamental limitation that transformer-based language models, despite increasing performance with scale, cannot solve tasks requiring access to data that was not present during training, such as live information, private data, or state-modifying APIs.\n    - Reducing dependence on model scale while enhancing capabilities: The authors are motivated by the practical downsides of scaling up language models, including high costs, deployment challenges, and reliance on memorization. They aim to enhance model capabilities through tool augmentation rather than solely through increasing model size.\n    - Text-to-text tool interface for language model augmentation: TALM uses a text-to-text interface where the model generates tool inputs, invokes external tool APIs through delimiter detection, receives tool outputs appended to the text sequence, and then generates final task outputs. This approach enables models to interact with non-differentiable tools in a unified textual format.\n    - Iterative self-play technique for bootstrapping tool-use performance: The method iteratively improves tool-use performance by fine-tuning the model on a growing dataset of tool interactions. In each round, the model samples tool inputs, calls APIs, generates outputs, and adds successful sequences to the training set if they match target outputs within a threshold.\n\nPaper 5:\n  Title: Gorilla: Large Language Model Connected with Massive APIs\n  Abstract: Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at this https URL\n  Structured Summary:\n    - LLMs' inability to accurately use tools via API calls: The paper addresses the challenge that current large language models, including GPT-4, struggle to correctly invoke APIs due to generating inaccurate arguments and hallucinating non-existent API usage, limiting their ability to effectively use external tools.\n    - Enabling LLMs to access dynamic knowledge and computational tools: The authors are motivated by the potential to transform LLMs into primary interfaces for computing infrastructure by enabling them to access dynamic, large-scale knowledge bases and perform complex tasks through API calls, moving beyond static model weights and limited context.\n    - Gorilla: retriever-aware finetuned LLaMA model for API calls: Gorilla is a finetuned LLaMA-7B model trained with retrieval-aware techniques, enabling it to generate accurate API calls and adapt to changing API documentation at test time by incorporating retrieved documents during both training and inference.\n    - APIBench: comprehensive dataset of ML API calls: APIBench is a large-scale dataset containing 1,645 API calls from three major ML model hubs (HuggingFace, TorchHub, TensorHub), with 10 synthetic instruction-API pairs per API generated using self-instruct, totaling over 16,000 training examples.\n\nPaper 6:\n  Title: HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face\n  Abstract: Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.\n  Structured Summary:\n    - Handling complex AI tasks across modalities: The paper addresses the challenge of enabling AI systems to autonomously solve complex tasks that span multiple domains and modalities, which current AI models struggle to achieve.\n    - Advancing artificial general intelligence: The authors are motivated by the goal of advancing artificial general intelligence by enabling AI systems to handle complex tasks that require coordination across multiple domains and modalities.\n    - HuggingGPT system design: The authors propose HuggingGPT, a system that uses large language models (LLMs) like ChatGPT as controllers to plan tasks, select models, execute tasks, and generate responses by leveraging AI models from the Hugging Face community.\n    - Language as a generic interface: The authors propose using language as a universal interface to connect LLMs with external AI models, enabling task planning, scheduling, and cooperation through model descriptions.\n\nPaper 7:\n  Title: ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models\n  Abstract: Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning. To improve the reasoning abilities, we propose ChatCoT, a tool-augmented chain-of-thought reasoning framework for chat-based LLMs (e.g., ChatGPT). In ChatCoT, we model the chain-of-thought (CoT) reasoning as multi-turn conversations, to utilize tools in a more natural way through chatting. At each turn, LLMs can either interact with tools or perform the reasoning. Our approach can effectively leverage the multi-turn conversation ability of chat-based LLMs, and integrate the thought chain following and tools manipulation in a unified way. Specially, we initialize the early turns of the conversation by the knowledge about tools, tasks, and reasoning format, and propose an iterative tool-augmented reasoning step to perform step-by-step tool-augmented reasoning. The experiment results on two complex reasoning datasets (MATH and HotpotQA) have shown the effectiveness of ChatCoT on complex reasoning tasks, achieving a 7.9% relative improvement over the state-of-the-art baseline. Our code and data are available at: \\url{this https URL}.\n  Structured Summary:\n    - Difficulty of LLMs in complex reasoning requiring tools and multi-hop reasoning: Large language models face challenges when solving complex reasoning tasks that demand specific domain knowledge and multi-step reasoning processes, particularly when external tools are needed for functionalities like arithmetic calculation and information retrieval.\n    - Need for unified integration of CoT reasoning and tool manipulation: Existing approaches either require LLMs to prearrange tool use plans without interaction or frequently switch between reasoning and tool actions, both hurting the continuity of the chain-of-thought reasoning process. A more unified integration is needed for tasks requiring frequent tool invocation.\n    - ChatCoT framework modeling CoT reasoning as multi-turn conversations: ChatCoT reformulates chain-of-thought reasoning as a multi-turn conversation where LLMs can freely interact with tools or perform reasoning at each turn, leveraging the multi-turn conversation ability of chat-based LLMs to maintain reasoning continuity while manipulating tools.\n    - Conversational knowledge memory initialization: The approach initializes the conversation with three types of knowledge: tool descriptions, retrieval-augmented task exemplars, and multi-turn reasoning format demonstrations. This conversational memory provides essential context for guiding LLMs in subsequent reasoning steps.\n\nPaper 8:\n  Title: Toolformer: Language Models Can Teach Themselves to Use Tools\n  Abstract: Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.\n  Structured Summary:\n    - Language models' inability to perform basic tasks despite strong few-shot abilities: The paper addresses the paradox that large language models, while excelling at few-shot learning, fail at simple tasks like arithmetic and factual retrieval that smaller specialized models handle easily. These limitations include inability to access recent information, hallucination of facts, difficulties with low-resource languages, lack of mathematical skills, and unawareness of time progression.\n    - Overcoming inherent limitations of language models through external tool use: The authors are motivated by the need to address fundamental limitations of language models that cannot be solved by scaling alone, such as accessing current information, performing precise calculations, and understanding temporal context. Enabling tool use offers a path to combine the generality of large LMs with the precision of specialized tools.\n    - Self-supervised API call generation and filtering: The method uses in-context learning to sample potential API calls from a language model, executes these calls, and filters them based on whether they reduce perplexity on future tokens. The model is then finetuned on the filtered API calls, enabling it to learn tool use in a self-supervised manner without human annotations.\n    - Weighted cross-entropy loss for API call filtering: The filtering mechanism compares the weighted cross-entropy loss when the model receives both API call input and output versus receiving no call or only the input. API calls are retained only if they reduce the loss by at least a threshold, ensuring only genuinely helpful calls are included in training.\n\nPaper 9:\n  Title: LLM As DBA\n  Abstract: Database administrators (DBAs) play a crucial role in managing, maintaining and optimizing a database system to ensure data availability, performance, and reliability. However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases). Recently large language models (LLMs) have shown great potential to understand valuable documents and accordingly generate reasonable answers. Thus, we propose D-Bot, a LLM-based database administrator that can continuously acquire database maintenance experience from textual sources, and provide reasonable, well-founded, in-time diagnosis and optimization advice for target databases. This paper presents a revolutionary LLM-centric framework for database maintenance, including (i) database maintenance knowledge detection from documents and tools, (ii) tree of thought reasoning for root cause analysis, and (iii) collaborative diagnosis among multiple LLMs. Our preliminary experimental results that D-Bot can efficiently and effectively diagnose the root causes and our code is available at this http URL.\n  Structured Summary:\n    - Difficulty and tediousness of managing large-scale database instances: The paper addresses the challenge that human DBAs face significant difficulties in managing massive numbers of database instances, particularly on cloud platforms. Traditional DBAs require years of training, cannot scale to millions of instances, and may not provide timely responses in emergent cases.\n    - Leveraging LLMs to build a human-beyond database administrator: The authors are motivated by the potential of large language models to understand documents and generate reasonable answers, which can be harnessed to create an automated database administrator that continuously learns from textual sources and provides diagnosis and optimization advice without human limitations.\n    - Database maintenance knowledge detection from documents and tools: D-Bot extracts experiential knowledge from database documents by segmenting them into manageable chunks, generating summaries, and using LLM to detect maintenance experience in a structured four-field format (name, content, metrics, steps). It also retrieves and learns to use external tools via matching algorithms.\n    - Tree of thought reasoning for root cause analysis: D-Bot employs a tree search strategy where the LLM iteratively generates and evaluates diagnostic actions. If an action fails, the system can revert to previous steps, using UCT scores to guide node selection and reflection to refine decisions, thereby increasing the likelihood of accurate root cause identification.\n\nPaper 10:\n  Title: MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning\n  Abstract: Huge language models (LMs) have ushered in a new era for AI, serving as a gateway to natural-language-based knowledge tasks. Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. Conceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we define a flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning modules. We describe this neuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Language (MRKL, pronounced \"miracle\") system, some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs' MRKL system implementation.\n  Structured Summary:\n    - Inherent limitations of large language models: Large language models lack access to current and proprietary information, cannot perform reliable symbolic reasoning (e.g., arithmetic), and suffer from model explosion when fine-tuned for multiple tasks, leading to catastrophic forgetting and infeasibility of repeated retraining.\n    - Need for systems approach combining neural and symbolic methods: The authors aim to retain the versatility and power of large language models while overcoming their shortcomings by integrating external knowledge sources and discrete reasoning modules into a flexible architecture.\n    - MRKL (Modular Reasoning, Knowledge and Language) system architecture: A neuro-symbolic architecture consisting of an extendable set of expert modules (both neural and symbolic) and a router that directs natural language inputs to the appropriate module. Modules include general-purpose language models, specialized neural models, and symbolic reasoners like calculators or database APIs.\n    - Data augmentation methodology for training argument extraction: A systematic approach using template-based generation across multiple dimensions (operand types, number of digits, operation types, question formats) combined with prompt-tuning to train the router to reliably extract discrete parameters from natural language for symbolic modules.\n\nPaper 11:\n  Title: Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models\n  Abstract: Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks. At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response. We showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54% overall accuracy on ScienceQA, improving the best published few-shot result by 11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%, lifting the state of the art to 98.78%. Our analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner. The project is available at this https URL.\n  Structured Summary:\n    - Inherent limitations of LLMs in accessing information and using tools: The paper identifies that large language models cannot access current information from the web or knowledge bases, cannot utilize external tools effectively, and struggle with precise mathematical and logical reasoning tasks.\n    - Need to enhance LLMs with automatic tool composition for real-world tasks: The authors are motivated by the need to overcome LLM limitations by enabling them to automatically compose and use external tools, which is essential for solving complex real-world problems across diverse domains.\n    - Chameleon: plug-and-play compositional reasoning framework: Chameleon is a framework that augments LLMs with diverse plug-and-play modules including vision models, web search engines, Python functions, and rule-based modules. It uses an LLM-based planner to generate natural-language-like programs that compose these tools sequentially to solve complex reasoning tasks.\n    - LLM-based natural language planner for program synthesis: The core component is an LLM-based planner that generates natural-language-like programs by selecting and sequencing appropriate tools from the module inventory, prompted by tool descriptions and usage examples without requiring training or domain-specific prompts.\n\nPaper 12:\n  Title: ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs\n  Abstract: Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.\n  Structured Summary:\n    - Limited tool-use capabilities in open-source LLMs: Open-source large language models like LLaMA remain significantly limited in their ability to use external tools (APIs) to fulfill human instructions, unlike closed-source models such as ChatGPT. Current instruction tuning focuses on basic language tasks and neglects the tool-use domain.\n    - Democratizing tool-use capabilities for open-source LLMs: The authors are motivated by the need to bridge the gap between open-source and closed-source LLMs in tool-use capabilities, enabling democratization of AI technologies and facilitating community-driven innovation by making tool-use abilities accessible in open models.\n    - ToolBench instruction-tuning dataset construction: The authors construct ToolBench through three stages: collecting 16,464 real-world RESTful APIs from RapidAPI Hub spanning 49 categories, prompting ChatGPT to generate diverse instructions covering single-tool and multi-tool scenarios, and annotating solution paths using ChatGPT to search for valid API call chains.\n    - Depth-first search-based decision tree (DFSDT): A novel reasoning strategy that constructs a decision tree allowing LLMs to assess different reasoning paths and choose to either proceed along promising paths or abandon existing nodes and expand new ones. This approach expands the search space beyond conventional ReACT or CoT methods.\n\nPaper 13:\n  Title: TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage\n  Abstract: With recent advancements in natural language processing, Large Language Models (LLMs) have emerged as powerful tools for various real-world applications. Despite their prowess, the intrinsic generative abilities of LLMs may prove insufficient for handling complex tasks which necessitate a combination of task planning and the usage of external tools. In this paper, we first propose a structured framework tailored for LLM-based AI Agents and discuss the crucial capabilities necessary for tackling intricate problems. Within this framework, we design two distinct types of agents (i.e., one-step agent and sequential agent) to execute the inference process. Subsequently, we instantiate the framework using various LLMs and evaluate their Task Planning and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings and challenges, our goal is to provide a helpful resource for researchers and practitioners to leverage the power of LLMs in their AI applications. Our study emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.\n  Structured Summary:\n    - Insufficient intrinsic generative abilities of LLMs for complex tasks: Large Language Models alone cannot effectively handle complex real-world tasks that require both strategic task decomposition and the ability to interact with external tools and resources beyond their internal knowledge.\n    - Need for structured evaluation of LLM-based AI agents: The authors aim to create a systematic framework and evaluation methodology to help researchers and practitioners understand how to effectively deploy LLMs as AI agents that can plan tasks and use tools in practical applications.\n    - Structured framework for LLM-based AI Agents with six components: The framework consists of task instruction, designed prompt, tool set, LLM core, intermediate output, and final answer. It systematically organizes how LLMs perceive tasks, plan solutions, select and use tools, and generate responses.\n    - Two agent types: One-step Agent (TPTU-OA) and Sequential Agent (TPTU-SA): TPTU-OA adopts a global perspective to break down problems into sub-tasks all at once, while TPTU-SA tackles sub-tasks sequentially with continuous feedback. These represent two different problem-solving strategies for task planning and tool usage.\n\nPaper 14:\n  Title: Gentopia: A Collaborative Platform for Tool-Augmented LLMs\n  Abstract: Augmented Language Models (ALMs) empower large language models with the ability to use tools, transforming them into intelligent agents for real-world interactions. However, most existing frameworks for ALMs, to varying degrees, are deficient in the following critical features: flexible customization, collaborative democratization, and holistic evaluation. We present gentopia, an ALM framework enabling flexible customization of agents through simple configurations, seamlessly integrating various language models, task formats, prompting modules, and plugins into a unified paradigm. Furthermore, we establish gentpool, a public platform enabling the registration and sharing of user-customized agents. Agents registered in gentpool are composable such that they can be assembled together for agent collaboration, advancing the democratization of artificial intelligence. To ensure high-quality agents, gentbench, an integral component of gentpool, is designed to thoroughly evaluate user-customized agents across diverse aspects such as safety, robustness, efficiency, etc. We release gentopia on Github and will continuously move forward.\n  Structured Summary:\n    - Deficiencies in existing ALM frameworks: Current frameworks for augmented language models lack three key capabilities: the ability to flexibly customize agents without redundant implementation, a platform for sharing and collaborating user-customized agents, and comprehensive benchmarks for evaluating agent quality across multiple dimensions like safety and robustness.\n    - Enabling collective growth through agent specialization and collaboration: The authors are motivated by the vision that specialized agents, when shared and composed together, can democratize AI development. Improvements to individual agents benefit the entire ecosystem, enabling collaborative advancement toward more capable and reliable intelligent systems.\n    - Gentopia framework for agent assembly via YAML configuration: Gentopia is a lightweight framework that enables users to define agents through simple YAML configuration files. It unifies various language models, task formats, prompting modules, and plugins into a single paradigm, supporting flexible customization without boilerplate code.\n    - GentPool platform for agent registration and sharing: GentPool is a public platform where users can register, share, and discover specialized agents. Registered agents are composable, allowing hierarchical assembly for multi-agent collaboration, and each agent is accompanied by a descriptive Wiki page.\n\nPaper 15:\n  Title: A Survey of Knowledge Graph Embedding and Their Applications\n  Abstract: Knowledge Graph embedding provides a versatile technique for representing knowledge. These techniques can be used in a variety of applications such as completion of knowledge graph to predict missing information, recommender systems, question answering, query expansion, etc. The information embedded in Knowledge graph though being structured is challenging to consume in a real-world application. Knowledge graph embedding enables the real-world application to consume information to improve performance. Knowledge graph embedding is an active research area. Most of the embedding methods focus on structure-based information. Recent research has extended the boundary to include text-based information and image-based information in entity embedding. Efforts have been made to enhance the representation with context information. This paper introduces growth in the field of KG embedding from simple translation-based models to enrichment-based models. This paper includes the utility of the Knowledge graph in real-world applications.\n  Structured Summary:\n    - Difficulty of consuming structured knowledge graph information in real-world applications: Knowledge graphs represent structured relational information as entities and relations, but this representation is difficult to directly utilize in practical applications. The challenge lies in converting the structured triple format into a form that can be easily manipulated and consumed by real-world systems.\n    - Enabling real-world applications through numerical representation of knowledge: The authors are motivated by the need to make knowledge graph information accessible to real-world applications such as recommender systems, question answering, and information extraction. Converting structured knowledge into continuous vector representations enables easier manipulation and integration into these applications.\n    - Translation-based embedding models: Translation models represent entities and relations in low-dimensional vector spaces where relationships are modeled as translations. Starting with TransE, these models evolved through TransH, TransR, TransD, RotatE, and HakE to address limitations in handling different relation types (one-to-many, symmetric, hierarchical) and semantic properties.\n    - Semantic matching embedding models: Semantic matching models employ similarity-based scoring functions rather than distance-based measures. These include tensor factorization approaches (RESCAL, TATEC), simplified bilinear models (DistMult), compositional representations (HolE), complex embeddings (ComplEx), and analogy-based models (ANALOGY) that capture different types of relational patterns.\n\nPaper 16:\n  Title: Dynamic Planning with a LLM\n  Abstract: While Large Language Models (LLMs) can solve many NLP tasks in zero-shot settings, applications involving embodied agents remain problematic. In particular, complex plans that require multi-step reasoning become difficult and too costly as the context window grows. Planning requires understanding the likely effects of one's actions and identifying whether the current environment satisfies the goal state. While symbolic planners find optimal solutions quickly, they require a complete and accurate representation of the planning problem, severely limiting their use in practical scenarios. In contrast, modern LLMs cope with noisy observations and high levels of uncertainty when reasoning about a task. Our work presents LLM Dynamic Planner (LLM-DP): a neuro-symbolic framework where an LLM works hand-in-hand with a traditional planner to solve an embodied task. Given action-descriptions, LLM-DP solves Alfworld faster and more efficiently than a naive LLM ReAct baseline.\n  Structured Summary:\n    - LLM limitations in embodied multi-step planning tasks: The paper addresses the challenge that while LLMs excel at many NLP tasks, they struggle with embodied agent applications requiring complex multi-step reasoning due to context window limitations, high computational costs, hallucinations, and brittleness to prompt phrasing.\n    - Combining LLM flexibility with symbolic planner efficiency: The authors are motivated by the complementary strengths of symbolic planners (which find optimal solutions efficiently but require complete information) and LLMs (which handle noisy observations and uncertainty but are costly and limited by context windows), seeking to integrate both approaches.\n    - LLM Dynamic Planner (LLM-DP) neuro-symbolic framework: LLM-DP is a closed-loop agent framework that integrates an LLM with a symbolic planner. The LLM translates natural language tasks into PDDL goals and samples plausible world states from beliefs about unknown predicates, while a symbolic planner (BFS(f)) generates optimal action sequences from these sampled states.\n    - Belief sampling for incomplete world states: The method maintains a world state W (known information) and beliefs B (unknown predicates). It uses the LLM to sample plausible instantiations of unknown predicates, generating multiple likely world states that are then passed to the symbolic planner to find valid plans.\n\nPaper 17:\n  Title: Reasoning with Language Model is Planning with World Model\n  Abstract: Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.\n  Structured Summary:\n    - LLMs lack internal world model for deliberate planning: Current large language models cannot perform deliberate planning similar to human reasoning because they lack an internal world model to predict future states and simulate action outcomes, preventing them from exploring alternative reasoning paths and refining steps iteratively.\n    - Enable human-like strategic planning in LLMs: The authors are motivated by how humans use internal world models for deliberate planning—exploring alternatives, assessing outcomes, and refining plans—which current LLMs cannot replicate, limiting their reasoning on tasks that are easy for humans.\n    - Reasoning via Planning (RAP) framework: RAP is a framework that repurposes the same LLM to serve as both a world model (predicting state transitions) and a reasoning agent, integrating Monte Carlo Tree Search to balance exploration and exploitation in the reasoning space, enabling strategic planning.\n    - World model via LLM repurposing with state-action formulation: The authors repurpose the LLM with prompts to act as a world model that predicts next states given current states and actions, enabling the LLM to simulate reasoning traces as sequences of interleaved states and actions rather than actions alone.\n\nPaper 18:\n  Title: LLM+P: Empowering Large Language Models with Optimal Planning Proficiency\n  Abstract: Large language models (LLMs) have demonstrated remarkable zero-shot generalization abilities: state-of-the-art chatbots can provide plausible answers to many common questions that arise in daily life. However, so far, LLMs cannot reliably solve long-horizon planning problems. By contrast, classical planners, once a problem is given in a formatted way, can use efficient search algorithms to quickly identify correct, or even optimal, plans. In an effort to get the best of both worlds, this paper introduces LLM+P, the first framework that incorporates the strengths of classical planners into LLMs. LLM+P takes in a natural language description of a planning problem, then returns a correct (or optimal) plan for solving that problem in natural language. LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL), then leveraging classical planners to quickly find a solution, and then translating the found solution back into natural language. Along with LLM+P, we define a diverse set of different benchmark problems taken from common planning scenarios. Via a comprehensive set of experiments on these benchmark problems, we find that LLM+P is able to provide optimal solutions for most problems, while LLMs fail to provide even feasible plans for most problems.\\footnote{The code and results are publicly available at this https URL.\n  Structured Summary:\n    - LLMs cannot reliably solve long-horizon robot planning problems: The paper identifies that while large language models demonstrate impressive zero-shot generalization for common questions, they fail to produce correct solutions for complex planning problems that require long-horizon reasoning and understanding of preconditions and world dynamics.\n    - Combining strengths of LLMs and classical planners: The authors are motivated by the observation that classical planners can efficiently find correct or optimal solutions once a problem is properly formatted, while LLMs excel at natural language understanding. They aim to leverage both capabilities without modifying the LLMs themselves.\n    - LLM+P framework for planning: The proposed LLM+P method uses an LLM to translate natural language problem descriptions into PDDL format, invokes a classical planner to find optimal solutions, and then translates the PDDL plan back to natural language. The approach assumes a domain PDDL file is provided and uses in-context learning with a simple example problem-PDDL pair.\n    - In-context learning for PDDL generation: The method leverages in-context learning by providing the LLM with an example pair of a natural language problem description and its corresponding PDDL file. This enables the LLM to generalize and correctly translate new problem descriptions into valid PDDL format without fine-tuning.\n\nPaper 19:\n  Title: Context-Aware Composition of Agent Policies by Markov Decision Process Entity Embeddings and Agent Ensembles\n  Abstract: Computational agents support humans in many areas of life and are therefore found in heterogeneous contexts. This means they operate in rapidly changing environments and can be confronted with huge state and action spaces. In order to perform services and carry out activities in a goal-oriented manner, agents require prior knowledge and therefore have to develop and pursue context-dependent policies. However, prescribing policies in advance is limited and inflexible, especially in dynamically changing environments. Moreover, the context of an agent determines its choice of actions. Since the environments can be stochastic and complex in terms of the number of states and feasible actions, activities are usually modelled in a simplified way by Markov decision processes so that, e.g., agents with reinforcement learning are able to learn policies, that help to capture the context and act accordingly to optimally perform activities. However, training policies for all possible contexts using reinforcement learning is time-consuming. A requirement and challenge for agents is to learn strategies quickly and respond immediately in cross-context environments and applications, e.g., the Internet, service robotics, cyber-physical systems. In this work, we propose a novel simulation-based approach that enables a) the representation of heterogeneous contexts through knowledge graphs and entity embeddings and b) the context-aware composition of policies on demand by ensembles of agents running in parallel. The evaluation we conducted with the \"Virtual Home\" dataset indicates that agents with a need to switch seamlessly between different contexts, can request on-demand composed policies that lead to the successful completion of context-appropriate activities without having to learn these policies in lengthy training steps and episodes, in contrast to agents that use reinforcement learning.\n  Structured Summary:\n    - Time-consuming policy learning for agents in heterogeneous contexts: Computational agents in dynamically changing environments with large state and action spaces require lengthy training procedures to learn context-dependent policies via reinforcement learning, which prevents on-the-fly integration and immediate performance of activities across multiple contexts.\n    - Enable immediate cross-context agent operation without lengthy training: The authors aim to allow agents to seamlessly switch between different contexts and perform activities without time-consuming training episodes, enabling rapid deployment in heterogeneous environments such as service robotics and cyber-physical systems.\n    - MDP knowledge graph generation from activity datasets: The approach introduces an ontology defining concepts (Activity, State, Action, Transition, Effect) and properties for automatically deriving Markov Decision Process knowledge graphs from operational datasets, capturing state transitions, actions, and observation features.\n    - MDP entity embeddings for context representation: Entity embedding vectors are trained using deep neural networks to numerically represent activities, states, and actions in an n-dimensional space, capturing semantic relationships and enabling fast identification of contextually appropriate actions through spatial proximity.\n\nPaper 20:\n  Title: Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis\n  Abstract: This paper explores the integration of two AI subdisciplines employed in the development of artificial agents that exhibit intelligent behavior: Large Language Models (LLMs) and Cognitive Architectures (CAs). We present three integration approaches, each grounded in theoretical models and supported by preliminary empirical evidence. The modular approach, which introduces four models with varying degrees of integration, makes use of chain-of-thought prompting, and draws inspiration from augmented LLMs, the Common Model of Cognition, and the simulation theory of cognition. The agency approach, motivated by the Society of Mind theory and the LIDA cognitive architecture, proposes the formation of agent collections that interact at micro and macro cognitive levels, driven by either LLMs or symbolic components. The neuro-symbolic approach, which takes inspiration from the CLARION cognitive architecture, proposes a model where bottom-up learning extracts symbolic representations from an LLM layer and top-down guidance utilizes symbolic representations to direct prompt engineering in the LLM layer. These approaches aim to harness the strengths of both LLMs and CAs, while mitigating their weaknesses, thereby advancing the development of more robust AI systems. We discuss the tradeoffs and challenges associated with each approach.\n  Structured Summary:\n    - Limitations of LLMs and CAs in isolation: The paper identifies that LLMs suffer from interpretability, consistency, scalability, hallucinations, and biases, while CAs struggle with limited and domain-specific knowledge representation, inability to handle complex knowledge, and reliance on manually curated procedural knowledge.\n    - Synergistic integration to harness complementary strengths: The authors are motivated by the observation that LLMs and CAs have complementary strengths and weaknesses. Integrating them synergistically could enable more robust AI systems by mitigating each approach's limitations while leveraging their respective advantages.\n    - Modular approach with four integration models: The modular approach proposes four integration models: (a) cognitively-augmented LLM using chain-of-thought prompting with CA-injected reasoning traces, (b) CA with LLM-powered perception and motor modules, (c) CA with multiple LLM-driven modules including memories, and (d) CA using LLMs for internal simulation and anticipation.\n    - Agency approach at micro and macro levels: The agency approach operates at two levels: micro-level where specialized agents (LLM-based or symbolic) compete for resources and form coalitions within a CA, and macro-level where cognitive agents interact with other agents and humans for collaborative goal achievement.\n\nPaper 21:\n  Title: Self-Refine: Iterative Refinement with Self-Feedback\n  Abstract: Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.\n  Structured Summary:\n    - Suboptimal initial outputs from large language models: Large language models often produce intelligible initial outputs but fall short in addressing intricate requirements or multifaceted objectives, particularly in tasks with hard-to-define goals such as dialogue generation or code readability improvement.\n    - Need for refinement without extensive supervision: Existing refinement methods require domain-specific training data, external supervision, or expensive human annotations. The authors are motivated to develop an approach that can refine LLM outputs across various tasks without these costly requirements, inspired by how humans iteratively refine their own work through self-feedback.\n    - SELF-REFINE iterative feedback and refinement algorithm: SELF-REFINE uses a single LLM to generate an initial output, then passes it back to the same model to obtain feedback on that output, and finally uses the feedback to refine the output. This process iterates until a stopping condition is met, using few-shot prompting to guide the model through feedback generation and refinement steps.\n    - Actionable and specific feedback generation: The feedback mechanism is designed to produce actionable suggestions (concrete actions for improvement) and specific critiques (identifying exact phrases or elements to change), rather than generic comments, enabling effective refinement in subsequent iterations.\n\nPaper 22:\n  Title: Complexity-Based Prompting for Multi-Step Reasoning\n  Abstract: We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multi-step reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.\n  Structured Summary:\n    - Example selection for chain-of-thoughts multi-step reasoning: The paper addresses the challenge of identifying which reasoning examples make the most effective prompts for multi-step reasoning tasks when using chain-of-thoughts prompting with large language models.\n    - Annotation efficiency and performance improvement in CoT prompting: The authors are motivated by the need to reduce annotation costs while improving reasoning performance, since chain-of-thoughts prompting requires manually annotated reasoning chains and identifying the best examples to annotate can save resources.\n    - Complexity-based prompting: The method selects examples with more complex reasoning chains (more reasoning steps) as prompts for the language model, hypothesizing that complex instances subsume simpler ones and lead to better reasoning performance.\n    - Complexity-based consistency: This method extends complexity-based selection to the output space by sampling multiple reasoning chains and taking the majority vote among the top K most complex generated chains rather than all chains, promoting outputs with more complex reasoning.\n\nPaper 23:\n  Title: Automatic Chain of Thought Prompting in Large Language Models\n  Abstract: Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like \"Let's think step by step\" to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the \"Let's think step by step\" prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at this https URL\n  Structured Summary:\n    - Manual effort required for designing task-specific CoT demonstrations: Existing Manual-CoT methods require nontrivial human effort to hand-craft effective demonstrations, including designing both questions and their reasoning chains. Different tasks require different demonstration designs, making this process labor-intensive and task-specific.\n    - Eliminating manual demonstration design while maintaining performance: The authors aim to remove the burden of manually designing demonstrations for CoT prompting while achieving competitive performance. This would make CoT reasoning more practical and scalable across different tasks without requiring human annotation efforts.\n    - Auto-CoT: Automatic Chain-of-Thought prompting with diversity-based sampling: Auto-CoT consists of two main stages: (i) partitioning questions into clusters using k-means to ensure diversity, and (ii) selecting representative questions from each cluster and generating reasoning chains using Zero-Shot-CoT with simple heuristics (e.g., preferring shorter questions and rationales).\n    - Diversity-based question clustering to mitigate misleading by similarity: The method uses clustering to sample diverse demonstration questions rather than similarity-based retrieval. This diversity helps avoid the 'misleading by similarity' problem where similar questions with Zero-Shot-CoT errors would propagate mistakes to test questions.\n\nPaper 24:\n  Title: Chain-of-Thought in Large Language Models: Decoding, Projection, and Activation\n  Abstract: Chain-of-Thought prompting has significantly enhanced the reasoning capabilities of large language models, with numerous studies exploring factors influencing its performance. However, the underlying mechanisms remain poorly understood. To further demystify the operational principles, this work examines three key aspects: decoding, projection, and activation, aiming to elucidate the changes that occur within models when employing Chainof-Thought. Our findings reveal that LLMs effectively imitate exemplar formats while integrating them with their understanding of the question, exhibiting fluctuations in token logits during generation but ultimately producing a more concentrated logits distribution, and activating a broader set of neurons in the final layers, indicating more extensive knowledge retrieval compared to standard prompts. Our code and data will be publicly avialable when the paper is accepted.\n  Structured Summary:\n    - Limited understanding of Chain-of-Thought operational mechanisms in LLMs: Despite Chain-of-Thought prompting significantly enhancing reasoning capabilities of large language models, the specific internal and external changes occurring within models when employing CoT remain unclear, creating a knowledge gap in understanding how CoT actually functions.\n    - Need to understand internal model changes with CoT prompting: The authors are motivated to fill the gap in understanding what specific changes occur both internally and externally within models when using CoT, as existing hypotheses lack comprehensive experimental validation and mechanistic insights.\n    - Multi-faceted analysis framework examining decoding, projection, and activation: The authors propose a comprehensive analytical framework that investigates CoT mechanisms from three distinct perspectives: analyzing generated text through test point matching and transfer tests in the decoding phase, examining logits values and probability distributions in the projection phase, and comparing neuron activation patterns in FFN layers.\n    - Test points matching analysis for fine-grained behavior assessment: A novel fine-grained analysis method that uses predefined test points (time, action, location/people, number) to statistically measure whether model-generated content aligns more with exemplar formats or pre-trained knowledge, enabling quantification of imitation versus understanding.\n\nPaper 25:\n  Title: Self-Consistency Improves Chain of Thought Reasoning in Language Models\n  Abstract: Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).\n  Structured Summary:\n    - Limited reasoning ability in language models despite chain-of-thought prompting: Language models struggle with complex reasoning tasks even when using chain-of-thought prompting with greedy decoding, which represents a fundamental limitation in their ability to perform multi-step reasoning across arithmetic and commonsense domains.\n    - Leveraging multiple reasoning paths to improve answer accuracy: The authors are motivated by the observation that humans can solve complex problems through diverse reasoning approaches that converge on the same answer, suggesting that sampling multiple reasoning paths from language models could improve reliability and accuracy compared to single-path greedy decoding.\n    - Self-consistency decoding strategy: A three-step approach that prompts language models with chain-of-thought examples, samples diverse reasoning paths from the decoder instead of greedy decoding, and aggregates answers by marginalizing over reasoning paths through majority voting to select the most consistent answer.\n    - Sample-and-marginalize decoding procedure: The method samples multiple candidate reasoning paths using temperature sampling or other sampling strategies, where each path may lead to different answers, then determines the optimal answer by taking a majority vote over the final answer set, effectively marginalizing out the sampled reasoning paths.\n\nPaper 26:\n  Title: Large Language Models are Zero-Shot Reasoners\n  Abstract: Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding \"Let's think step by step\" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.\n  Structured Summary:\n    - Limited zero-shot reasoning ability of large language models on multi-step tasks: Large language models struggle with complex multi-step reasoning tasks (system-2 tasks) in zero-shot settings, exhibiting flat scaling curves and poor performance on arithmetic and logical reasoning benchmarks without task-specific few-shot examples.\n    - Uncovering untapped zero-shot reasoning capabilities in LLMs: The authors aim to demonstrate that LLMs possess inherent zero-shot reasoning abilities that have been overlooked, and that these broad cognitive capabilities can be elicited through simple prompting rather than requiring expensive few-shot example engineering or fine-tuning.\n    - Zero-shot Chain of Thought prompting with 'Let's think step by step': The method adds a simple trigger phrase ('Let's think step by step') to prompts, which elicits step-by-step reasoning from LLMs in a zero-shot manner without requiring hand-crafted task-specific examples, using a two-stage prompting process for reasoning extraction and answer extraction.\n    - Two-stage prompting pipeline: The approach employs a two-stage process: first prompting the model to generate a reasoning path using the trigger phrase, then using a second prompt with the generated reasoning to extract the final answer in the correct format.\n\nPaper 27:\n  Title: Graph of Thoughts: Solving Elaborate Problems with Large Language Models\n  Abstract: We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information (\"LLM thoughts\") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by >31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.\n  Structured Summary:\n    - Rigid tree structure limits LLM reasoning in existing prompting schemes: Existing prompting paradigms like Chain-of-Thought and Tree of Thoughts constrain LLM reasoning by forcing thoughts into linear chains or tree structures, preventing more complex networked reasoning patterns that could combine, merge, or refine thoughts in arbitrary ways.\n    - Human reasoning and brain structure form complex networks: The authors are motivated by the observation that human reasoning, brain structure, and algorithmic execution naturally form complex networked patterns rather than simple chains or trees. Humans combine ideas from different reasoning paths, backtrack, and merge solutions, which cannot be naturally expressed in existing prompting schemes.\n    - Graph of Thoughts framework with arbitrary graph-based reasoning: GoT models LLM reasoning as an arbitrary directed graph where vertices represent thoughts (units of information) and edges represent dependencies between thoughts. This enables novel transformations like aggregating multiple thoughts into synergistic outcomes, refining thoughts through feedback loops, and distilling essence from networks of thoughts.\n    - Modular architecture with fine-grained thought control: The architecture consists of interacting modules (Prompter, Parser, Scoring, Controller with Graph of Operations and Graph Reasoning State) that enable fine-grained control over individual thoughts and seamless extension with novel thought transformations, reasoning patterns, and LLM models.\n\nPaper 28:\n  Title: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n  Abstract: We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.\n  Structured Summary:\n    - Limited reasoning ability of large language models on complex tasks: Despite improvements from scaling language models, they still struggle with tasks requiring complex reasoning such as arithmetic, commonsense, and symbolic reasoning. Standard prompting methods do not enable models to perform well on these challenging reasoning tasks.\n    - Unlocking reasoning abilities through natural language intermediate steps: The authors are motivated by combining two insights: (1) generating natural language rationales with intermediate reasoning steps can improve arithmetic reasoning, and (2) large language models enable few-shot in-context learning via prompting, avoiding costly finetuning for each task.\n    - Chain-of-thought prompting: The method augments few-shot prompting exemplars with chains of thought—coherent series of intermediate natural language reasoning steps leading to final answers. This allows models to decompose multi-step problems and allocate additional computation to complex reasoning tasks without requiring finetuning.\n    - Chain-of-thought reasoning emerges with model scale: Chain-of-thought reasoning is an emergent property that appears only in sufficiently large language models (around 100B parameters). For many reasoning tasks with flat standard prompting scaling curves, chain-of-thought prompting produces dramatically increasing performance with scale.\n\nPaper 29:\n  Title: Progressive-Hint Prompting Improves Reasoning in Large Language Models\n  Abstract: The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experiments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sample paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% -> 91.9%), GSM8K (92% -> 95.5%), AQuA (76.4% -> 79.9%) and MATH (50.3% -> 53.9%).\n  Structured Summary:\n    - Underutilization of LLM-generated answers in reasoning: Existing prompting methods like Chain-of-Thought and self-consistency fail to leverage previously generated answers from the LLM to iteratively refine and improve reasoning paths toward correct solutions.\n    - Simulating human double-checking in LLMs: The authors are motivated by the human cognitive process of re-evaluating answers. They aim to enable LLMs to iteratively refine reasoning by using prior outputs as hints, mimicking how humans reconsider and correct their thinking.\n    - Progressive-Hint Prompting (PHP): PHP is a two-stage pipeline: first generating a base answer using standard prompts (e.g., CoT), then iteratively re-querying the LLM with the question augmented by previous answers as hints until two consecutive answers match, signaling convergence to a stable solution.\n    - PHP prompt design with dual hint scenarios: The PHP prompt structure includes a phrase indicating answer proximity in the question and a rehearsal sentence in the answer part. It is designed to handle both correct and incorrect hints, ensuring the model can either confirm correct answers or escape incorrect ones.\n\nPaper 30:\n  Title: Tree of Thoughts: Deliberate Problem Solving with Large Language Models\n  Abstract: Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: this https URL.\n  Structured Summary:\n    - Limitations of token-level left-to-right decision-making in language models: The paper identifies that current language models perform token-by-token autoregressive generation without exploration or planning, which limits their effectiveness on tasks requiring search, lookahead, or backtracking to correct early mistakes.\n    - Need for deliberate System 2 reasoning in language models: The authors are motivated by cognitive science insights about dual-process reasoning, arguing that language models currently operate like System 1 (fast, associative) and would benefit from System 2 (deliberate, planning-based) capabilities to handle complex problem-solving tasks.\n    - Tree of Thoughts (ToT) framework: ToT is a framework that structures problem-solving as search over a tree where nodes represent partial solutions (thoughts). It allows language models to generate multiple candidate thoughts, evaluate them via self-assessment, and use search algorithms (BFS or DFS) to explore, backtrack, and make global decisions.\n    - Language-based thought generation and evaluation: The method uses the language model itself to generate diverse candidate thoughts at each step and to evaluate their promise toward solving the problem through deliberate reasoning expressed in natural language, replacing traditional programmed or learned heuristics.\n\nPaper 31:\n  Title: Least-to-Most Prompting Enables Complex Reasoning in Large Language Models\n  Abstract: Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.\n  Structured Summary:\n    - Easy-to-hard generalization limitation in chain-of-thought prompting: Chain-of-thought prompting struggles when test problems are more difficult than the exemplars shown in prompts, particularly failing on tasks requiring compositional generalization where problems exceed the complexity of demonstration examples.\n    - Bridging gap between human and machine intelligence in problem-solving: The authors aim to narrow the gap between human and machine intelligence by enabling models to solve harder problems than those seen during training, mimicking humans' ability to tackle novel challenges of greater complexity.\n    - Least-to-most prompting strategy: A two-stage prompting approach that first decomposes complex problems into simpler subproblems, then sequentially solves these subproblems where each solution builds on previously solved subproblems. Both stages use few-shot prompting without training or finetuning.\n    - Two-stage decomposition and solving framework: The method implements decomposition through prompts with constant exemplars demonstrating problem breakdown, followed by subproblem solving prompts containing demonstration examples, previously answered subquestions with solutions, and the next question to answer.\n\nPaper 32:\n  Title: Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling\n  Abstract: Reinforcement learning (RL) agents typically learn tabula rasa, without prior knowledge of the world. However, if initialized with knowledge of high-level subgoals and transitions between subgoals, RL agents could utilize this Abstract World Model (AWM) for planning and exploration. We propose using few-shot large language models (LLMs) to hypothesize an AWM, that will be verified through world experience, to improve sample efficiency of RL agents. Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase where the agent uses an LLM to decompose a task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase where the agent learns a modular policy for each subgoal and verifies or corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience not only increases sample efficiency over contemporary methods by an order of magnitude but is also robust to and corrects errors in the LLM, successfully blending noisy internet-scale information from LLMs with knowledge grounded in environment dynamics.\n  Structured Summary:\n    - Sample inefficiency in RL agents: The paper addresses the challenge of improving sample efficiency in reinforcement learning agents, which traditionally learn without leveraging external knowledge or pretraining.\n    - Utilizing LLMs for RL exploration: The authors aim to leverage large language models (LLMs) to improve exploration in reinforcement learning, enabling agents to utilize noisy, large-scale knowledge sources effectively.\n    - DECKARD agent with Dream and Wake phases: The DECKARD agent alternates between a Dream phase, where an LLM hypothesizes an Abstract World Model (AWM) of subgoals, and a Wake phase, where the agent verifies and corrects the AWM through grounded experience.\n    - Modular RL policies for subgoals: The agent learns modular policies for individual subgoals, enabling compositional training and efficient navigation of complex tasks like crafting items in Minecraft.\n\nPaper 33:\n  Title: SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks\n  Abstract: We introduce SwiftSage, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks. SwiftSage integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance. The framework comprises two primary modules: the Swift module, representing fast and intuitive thinking, and the Sage module, emulating deliberate thought processes. The Swift module is a small encoder-decoder LM fine-tuned on the oracle agent's action trajectories, while the Sage module employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a heuristic method to harmoniously integrate the two modules, resulting in a more efficient and robust problem-solving process. In 30 tasks from the ScienceWorld benchmark, SwiftSage significantly outperforms other methods such as SayCan, ReAct, and Reflexion, demonstrating its effectiveness in solving complex interactive tasks.\n  Structured Summary:\n    - Limitations in complex interactive reasoning tasks: The paper addresses the challenge of developing agents capable of solving complex interactive reasoning tasks, which require long-horizon planning, subgoal decomposition, exception handling, and commonsense knowledge, areas where existing methods struggle.\n    - Advancing artificial general intelligence through complex task-solving agents: The authors are motivated by the need to create agents that can perform human-like problem-solving in dynamic, open-world environments, which is essential for advancing artificial general intelligence.\n    - SWIFT module for fast thinking: The SWIFT module represents fast, intuitive thinking by using a fine-tuned encoder-decoder language model to handle short-term memory and generate immediate actions based on the current environment and task state.\n    - SAGE module for slow thinking: The SAGE module emulates deliberate, analytical thinking by leveraging large language models for subgoal planning, exception handling, and converting plans into executable action sequences.\n\nPaper 34:\n  Title: ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate\n  Abstract: Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality. Recognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies. The multi-agent-based approach enables a group of LLMs to synergize with an array of intelligent counterparts, harnessing their distinct capabilities and expertise to enhance efficiency and effectiveness in handling intricate tasks. In this paper, we construct a multi-agent referee team called ChatEval to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks. Our analysis shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments. Our code is available at this https URL.\n  Structured Summary:\n    - Limitations of single-agent LLM-based text evaluation: The paper identifies that existing single-agent LLM-based evaluation methods do not yet achieve human-level quality in text evaluation, particularly for open-ended questions and traditional NLG tasks, indicating a gap that needs to be addressed.\n    - Mimicking collaborative human evaluation processes: The authors are motivated by the observation that human evaluation typically involves multiple annotators working together, which reduces bias and improves reliability. They aim to replicate this collaborative approach using multiple LLM agents to enhance evaluation quality.\n    - ChatEval multi-agent referee team with debate framework: ChatEval is a multi-agent system where multiple LLM agents, each with distinct personas, engage in autonomous debate to evaluate text quality. The framework enables agents to discuss and reach judgments collaboratively, mimicking human group evaluation processes.\n    - Diverse role prompts for agents: The method assigns different role descriptions (personas) to each agent in the multi-agent system, ensuring that agents evaluate from varied perspectives. This diversity is shown to be essential for effective debate and improved evaluation outcomes.\n\nPaper 35:\n  Title: Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate\n  Abstract: Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of \"tit for tat\" and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of \"tit for tat\" state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Code is available at this https URL.\n  Structured Summary:\n    - Degeneration-of-Thought (DoT) problem in self-reflection: The paper identifies that self-reflection methods in LLMs suffer from a DoT problem where LLMs become unable to generate novel thoughts through iterative self-reflection once they establish confidence in their initial answers, even when those answers are incorrect. This limits the effectiveness of self-reflection for complex reasoning tasks.\n    - Addressing limitations of self-reflection for complex reasoning: The authors are motivated by the need to improve LLMs' performance on complex reasoning tasks that require deep contemplation. They aim to explore human-like problem-solving strategies beyond self-reflection to overcome the DoT problem and enable divergent thinking in LLMs.\n    - Multi-Agent Debate (MAD) framework: The MAD framework involves multiple LLM-based agents (debaters) that engage in adversarial debate in a 'tit for tat' manner, with a judge agent that monitors the debate process and determines when to conclude. This design encourages divergent thinking by allowing agents to challenge each other's biases and provide external feedback.\n    - Adaptive break mechanism for debate termination: The judge agent in MAD can adaptively terminate the debate when it determines that an optimal solution has been reached, rather than forcing a fixed number of debate rounds. This mechanism allows efficient conclusion of debates while maintaining solution quality.\n\nPaper 36:\n  Title: Building Cooperative Embodied Agents Modularly with Large Language Models\n  Abstract: In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA-2 still underperform, we fine-tune a CoELA with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi-agent cooperation. Videos can be found on the project website this https URL.\n  Structured Summary:\n    - Multi-agent cooperation with decentralized control, costly communication, and complex observations: The paper tackles the challenge of enabling multiple embodied agents to cooperate in environments where agents operate independently (decentralized control), perceive only partial raw sensory data, face time costs for communication, and must accomplish complex multi-objective tasks over long horizons.\n    - Building agents that cooperate and communicate naturally with humans and other agents: The authors are motivated by the goal of creating embodied agents capable of assisting humans in real-world tasks, which requires handling perception, planning, and natural language communication in cooperative settings, an area that remains largely unexplored for decentralized multi-agent systems.\n    - CoELA: Cooperative Embodied Language Agent with modular cognitive architecture: CoELA is a modular framework inspired by cognitive architectures that integrates LLMs into five modules: Perception (extracting information from raw observations), Memory (maintaining knowledge about the world and others), Communication (generating natural language messages), Planning (making high-level decisions), and Execution (translating plans into primitive actions).\n    - Fine-tuned CoLLAMA using LoRA on agent-collected data: The authors apply parameter-efficient fine-tuning (LoRA) to the open LLM LLAMA-2 using a small set of high-quality trajectory data collected by their agents in embodied environments, resulting in CoLLAMA, which achieves competitive performance with GPT-4 in some tasks.\n\nPaper 37:\n  Title: BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents\n  Abstract: The massive successes of large language models (LLMs) encourage the emerging exploration of LLM-augmented Autonomous Agents (LAAs). An LAA is able to generate actions with its core LLM and interact with environments, which facilitates the ability to resolve complex tasks by conditioning on past interactions such as observations and actions. Since the investigation of LAA is still very recent, limited explorations are available. Therefore, we provide a comprehensive comparison of LAA in terms of both agent architectures and LLM backbones. Additionally, we propose a new strategy to orchestrate multiple LAAs such that each labor LAA focuses on one type of action, \\textit{i.e.} BOLAA, where a controller manages the communication among multiple agents. We conduct simulations on both decision-making and multi-step reasoning environments, which comprehensively justify the capacity of LAAs. Our performance results provide quantitative suggestions for designing LAA architectures and the optimal choice of LLMs, as well as the compatibility of both. We release our implementation code of LAAs to the public at \\url{this https URL}.\n  Structured Summary:\n    - Limited exploration of LLM-augmented Autonomous Agent architectures and LLM backbones: The paper addresses the lack of comprehensive understanding regarding optimal agent architectures for LLM-augmented Autonomous Agents (LAAs) and the efficacy of different LLM backbones. Current works have not jointly considered agent architectures with their LLM backbones, and there is insufficient exploration of how to orchestrate multiple agents for complex tasks.\n    - Need for quantitative guidance in designing LAA systems: The authors are motivated to provide empirical evidence and quantitative guidance for researchers and practitioners in selecting appropriate LAA architectures and LLM backbones. They aim to understand which combinations work best for different task complexities and environments, addressing gaps in current LAA research.\n    - Six different LAA architectures with varying design principles: The authors design and implement six LAA architectures: Zeroshot LAA (ZS-LAA), ZeroshotThink LAA (ZST-LAA), ReAct LAA, PlanAct LAA, PlanReAct LAA, and BOLAA. These architectures systematically explore design choices including zero-shot vs few-shot prompting, self-thinking (Chain-of-Thought), and planning capabilities.\n    - BOLAA: Multi-agent orchestration architecture: BOLAA is a novel architecture that orchestrates multiple specialized LAAs through a controller module. The controller selectively calls labor agents from a pool, where each agent focuses on one type of action. This design divides complex tasks into feasible subtasks handled by specialist agents.\n\nPaper 38:\n  Title: AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\n  Abstract: Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework \\framework that can collaboratively and dynamically adjust its composition as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that \\framework framework can effectively deploy multi-agent groups that outperform a single agent. Furthermore, we delve into the emergence of social behaviors among individual agents within a group during collaborative task accomplishment. In view of these behaviors, we discuss some possible strategies to leverage positive ones and mitigate negative ones for improving the collaborative potential of multi-agent groups. Our codes for \\framework will soon be released at \\url{this https URL}.\n  Structured Summary:\n    - Limited cooperation among autonomous agents in complex real-world tasks: Existing autonomous agents powered by LLMs lack effective mechanisms for collaboration when tackling complex tasks that require cooperation among multiple agents, similar to how human groups work together to solve problems more efficiently.\n    - Enhancing agent effectiveness through human-inspired group dynamics: The authors are motivated by the observation that human groups achieve better outcomes through collaboration, and seek to replicate this dynamic in multi-agent systems to improve task accomplishment efficiency and effectiveness beyond what single agents can achieve.\n    - AGENT VERSE multi-agent framework with four-stage problem-solving process: AGENT VERSE is a framework that orchestrates multi-agent collaboration through four stages: Expert Recruitment (dynamically determining agent composition), Collaborative Decision-Making (agents discuss strategies using horizontal or vertical communication structures), Action Execution (implementing decided actions), and Evaluation (assessing outcomes and providing feedback for iterative refinement).\n    - Automated expert recruitment mechanism: Instead of manual role assignment, AGENT VERSE uses a recruiter agent that dynamically generates expert descriptions based on the task goal, forming an expert group that can be adjusted based on feedback from the evaluation stage.\n\nPaper 39:\n  Title: CGMI: Configurable General Multi-Agent Interaction Framework\n  Abstract: Benefiting from the powerful capabilities of large language models (LLMs), agents based on LLMs have shown the potential to address domain-specific tasks and emulate human behaviors. However, the content generated by these agents remains somewhat superficial, owing to their limited domain expertise and the absence of an effective cognitive architecture. To address this, we present the Configurable General Multi-Agent Interaction (CGMI) framework, designed to replicate human interactions in real-world scenarios. Specifically, we propose a tree-structured methodology for the assignment, detection, and maintenance of agent personality. Additionally, we designed a cognitive architecture equipped with a skill library based on the ACT* model, which contains memory, reflection, and planning modules. We have also integrated general agents to augment the virtual environment's realism. Using the CGMI framework, we simulated numerous classroom interactions between teacher and students. The experiments indicate that aspects such as the teaching methodology, curriculum, and student performance closely mirror real classroom settings. We will open source our work.\n  Structured Summary:\n    - Superficial content generation and lack of effective cognitive architecture in LLM-based agents: LLM-based agents produce shallow outputs and lack the cognitive structures needed to handle complex domain-specific tasks and realistic human-like interactions, particularly in multi-turn dialogues where they forget character settings and fail to execute plans consistently.\n    - Simulating realistic human interactions in domain-specific scenarios: The authors aim to create agents that can authentically simulate social phenomena and human behaviors in specific contexts, such as classroom teaching, by addressing limitations in role stability, communication mechanisms, and cognitive depth.\n    - Tree-structured persona model for personality assignment, detection, and maintenance: A hierarchical approach based on the Big Five Personality Scale that uses depth-first traversal to assign personalities and efficient random testing to detect and maintain personality consistency, reducing memory pressure and improving agent stability in multi-turn interactions.\n    - Cognitive architecture equipped with skill library based on ACT* model: A cognitive framework inspired by the Adaptive Control of Thought model that integrates declarative and procedural memories extracted via Chain of Thought and Chain of Action methods, combined with a domain-specific skill library to enable deeper reflection and specialized planning.\n\nPaper 40:\n  Title: AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n  Abstract: AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n  Structured Summary:\n    - Difficulty in building diverse LLM applications via multi-agent cooperation: The paper addresses the challenge of enabling developers to build LLM applications across diverse domains and complexities using multiple cooperating agents. Existing approaches lack a unified framework that supports flexible agent customization, diverse conversation patterns, and both automated and human-in-the-loop workflows.\n    - Leveraging multi-agent conversations to scale LLM capabilities: The authors are motivated by the observation that chat-optimized LLMs can incorporate feedback and cooperate through conversations, that differently configured agents can combine broad LLM capabilities in a modular manner, and that multi-agent conversations enable intuitive task decomposition and integration for complex problems.\n    - Conversable agents with customizable capabilities: AutoGen introduces conversable agents that can send and receive messages, maintain internal context, and be configured with various capabilities including LLMs, human inputs, and tools. Agents are customizable through built-in classes like ConversableAgent, AssistantAgent, and UserProxyAgent, enabling developers to create specialized agents by reusing or extending these components.\n    - Conversation programming paradigm: The framework adopts a programming paradigm centered on multi-agent conversations, involving (1) defining conversable agents with specific capabilities and roles, and (2) programming agent interaction behavior via conversation-centric computation and control. This paradigm uses both natural and programming languages to build applications with diverse conversation patterns.\n\nPaper 41:\n  Title: AvalonBench: Evaluating LLMs Playing the Game of Avalon\n  Abstract: In this paper, we explore the potential of Large Language Models (LLMs) Agents in playing the strategic social deduction game, Resistance Avalon. Players in Avalon are challenged not only to make informed decisions based on dynamically evolving game phases, but also to engage in discussions where they must deceive, deduce, and negotiate with other players. These characteristics make Avalon a compelling test-bed to study the decision-making and language-processing capabilities of LLM Agents. To facilitate research in this line, we introduce AvalonBench - a comprehensive game environment tailored for evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game environment for Avalon, (2) rule-based bots as baseline opponents, and (3) ReAct-style LLM agents with tailored prompts for each role. Notably, our evaluations based on AvalonBench highlight a clear capability gap. For instance, models like ChatGPT playing good-role got a win rate of 22.2% against rule-based bots playing evil, while good-role bot achieves 38.2% win rate in the same setting. We envision AvalonBench could be a good test-bed for developing more advanced LLMs (with self-playing) and agent frameworks that can effectively model the layered complexities of such game environments.\n  Structured Summary:\n    - Lack of comprehensive benchmark for multi-agent LLM game playing: Existing benchmarks like AgentBench cover single-agent environments, but there is no comprehensive test-bed specifically designed to evaluate multi-agent LLM agents in complex strategic social deduction games requiring deduction, persuasion, negotiation, and deception.\n    - Avalon as test-bed for LLM reasoning and communication capabilities: The game Resistance Avalon requires players to perform deductive reasoning, coordinate and collaborate with teammates, and employ deception to hide identities. These characteristics make it an ideal environment to test and study the decision-making, language processing, and multi-agent interaction capabilities of LLM agents.\n    - AVALON BENCH benchmark environment: The authors develop a comprehensive benchmark consisting of a game environment that records player actions and manages game progression, rule-based baseline bots with deterministic strategies, and ReAct-style LLM agent implementations with carefully designed prompts for each role.\n    - ReAct-style LLM agent implementation with recursive summarization: The authors implement LLM agents using a ReAct framework where the LLM generates actions, dialogue, and recursive summaries of game history. A separate parser LLM ensures correct action formatting, and recursive summarization handles long context by condensing previous history with current discussion and outcomes.\n\nPaper 42:\n  Title: Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf\n  Abstract: Communication games, which we refer to as incomplete information games that heavily depend on natural language communication, hold significant research value in fields such as economics, social science, and artificial intelligence. In this work, we explore the problem of how to engage large language models (LLMs) in communication games, and in response, propose a tuning-free framework. Our approach keeps LLMs frozen, and relies on the retrieval and reflection on past communications and experiences for improvement. An empirical study on the representative and widely-studied communication game, ``Werewolf'', demonstrates that our framework can effectively play Werewolf game without tuning the parameters of the LLMs. More importantly, strategic behaviors begin to emerge in our experiments, suggesting that it will be a fruitful journey to engage LLMs in communication games and associated domains.\n  Structured Summary:\n    - Engaging LLMs in communication games with natural language: Existing methods for communication games either impose strict language restrictions or require substantial human-annotated data, making it difficult for AI agents to participate naturally in games like Werewolf that rely heavily on natural language communication.\n    - Practical value of communication games for economics and social science: Communication games serve as valuable proxies for studying fundamental problems in economics and social science, and recent advances in LLMs with sophisticated language and reasoning abilities make them promising candidates for tackling these games naturally.\n    - Tuning-free framework with retrieval and reflection: The framework keeps LLMs frozen and addresses context length limitations through three perspectives: freshness (recent messages), informativeness (rule-matched critical messages), and completeness (reflection by answering questions). This enables effective gameplay without parameter tuning.\n    - Non-parametric learning from experience pool: The method collects scored response-reflection pairs from all players into an experience pool, then retrieves relevant experiences based on current reflection and extracts suggestions to guide agent reasoning, allowing learning across rounds without fine-tuning.\n\nPaper 43:\n  Title: Reflexion: Language Agents with Verbal Reinforcement Learning\n  Abstract: Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.\n  Structured Summary:\n    - Inefficient learning from trial-and-error in language agents: The paper addresses the challenge that language agents using large language models struggle to learn efficiently from trial-and-error experiences, as traditional RL methods demand extensive training data and costly model fine-tuning, limiting their practical deployment.\n    - Enabling lightweight learning without model fine-tuning: The authors are motivated by the need for a learning approach that avoids expensive model fine-tuning while providing richer feedback than scalar rewards, enabling agents to learn from mistakes through interpretable verbal reflections similar to human iterative learning.\n    - Reflexion framework with verbal reinforcement: Reflexion is a framework that reinforces language agents through verbal self-reflection rather than weight updates. Agents verbally reflect on task feedback, store reflective text in episodic memory, and use these experiences to improve decision-making in subsequent trials.\n    - Three-model architecture with Actor, Evaluator, and Self-Reflection: The method employs three components: an Actor (LLM) that generates actions, an Evaluator that scores outputs using various feedback types (binary, heuristic, or LLM-based), and a Self-Reflection model that converts feedback into verbal summaries stored in long-term memory.\n\nPaper 44:\n  Title: ReAct: Synergizing Reasoning and Acting in Language Models\n  Abstract: While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: this https URL\n  Structured Summary:\n    - Isolated reasoning and acting in language models: Large language models have demonstrated strong capabilities in reasoning and action generation, but these abilities have been studied in isolation rather than combined synergistically, limiting their effectiveness in complex task solving.\n    - Synergy between reasoning and acting in human cognition: Humans naturally combine verbal reasoning with actions during problem solving, using reasoning to track progress, handle exceptions, and maintain working memory while acting to gather information. This synergy enables robust decision making and quick learning of new tasks.\n    - ReAct paradigm for interleaving reasoning and acting: ReAct prompts large language models to generate reasoning traces (thoughts) and task-specific actions in an interleaved manner. Reasoning helps induce, track, and update action plans while handling exceptions, and actions enable interaction with external sources to gather additional information that supports reasoning.\n    - Augmented action space with language thoughts: The method augments the agent's action space to include language-based thoughts alongside domain-specific actions. These thoughts do not affect the external environment but compose useful information by reasoning over context to support future reasoning or acting.\n\nPaper 45:\n  Title: Making Large Language Models Better Reasoners with Alignment\n  Abstract: Reasoning is a cognitive process of using evidence to reach a sound conclusion. The reasoning capability is essential for large language models (LLMs) to serve as the brain of the artificial general intelligence agent. Recent studies reveal that fine-tuning LLMs on data with the chain of thought (COT) reasoning process can significantly enhance their reasoning capabilities. However, we find that the fine-tuned LLMs suffer from an \\textit{Assessment Misalignment} problem, i.e., they frequently assign higher scores to subpar COTs, leading to potential limitations in their reasoning abilities. To address this problem, we introduce an \\textit{Alignment Fine-Tuning (AFT)} paradigm, which involves three steps: 1) fine-tuning LLMs with COT training data; 2) generating multiple COT responses for each question, and categorizing them into positive and negative ones based on whether they achieve the correct answer; 3) calibrating the scores of positive and negative responses given by LLMs with a novel constraint alignment loss. Specifically, the constraint alignment loss has two objectives: a) Alignment, which guarantees that positive scores surpass negative scores to encourage answers with high-quality COTs; b) Constraint, which keeps the negative scores confined to a reasonable range to prevent the model degradation. Beyond just the binary positive and negative feedback, the constraint alignment loss can be seamlessly adapted to the ranking situations when ranking feedback is accessible. Furthermore, we also delve deeply into recent ranking-based alignment methods, such as DPO, RRHF, and PRO, and discover that the constraint, which has been overlooked by these approaches, is also crucial for their performance. Extensive experiments on four reasoning benchmarks with both binary and ranking feedback demonstrate the effectiveness of AFT.\n  Structured Summary:\n    - Assessment Misalignment in fine-tuned LLMs: The paper identifies that fine-tuned large language models often assign higher scores to low-quality chain-of-thought (COT) reasoning paths, which limits their reasoning capabilities.\n    - Enhancing reasoning capabilities of LLMs: The authors aim to improve reasoning capabilities in large language models, which are critical for their role in artificial general intelligence systems.\n    - Alignment Fine-Tuning (AFT) paradigm: The authors propose a three-step Alignment Fine-Tuning (AFT) paradigm that fine-tunes LLMs with COT data, categorizes responses as positive or negative, and calibrates their scores using a constraint alignment loss.\n    - Constraint alignment loss: The constraint alignment loss ensures that positive responses are scored higher than negative ones and keeps negative scores within a reasonable range to maintain model performance.\n\nPaper 46:\n  Title: Collaborating with language models for embodied reasoning\n  Abstract: Reasoning in a complex and ambiguous environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance.\n  Structured Summary:\n    - Combining reasoning ability of language models with embodied agent capabilities: The paper addresses the challenge of integrating the strong reasoning and generalization capabilities of large language models with the embodied interaction abilities of reinforcement learning agents, as neither approach alone can effectively handle complex reasoning in interactive environments.\n    - Leveraging language model reasoning for embodied tasks requiring information gathering: The authors are motivated by the difficulty of logical reasoning in embodied settings and the potential to leverage recent advances in language model reasoning to address this challenge, particularly for tasks requiring explicit information gathering and multi-step planning.\n    - Planner-Actor-Reporter paradigm: The authors propose a three-component system where a pre-trained language model (Planner) issues natural language instructions to an embodied RL agent (Actor), while a Reporter module translates environmental observations back to the Planner to enable closed-loop feedback and adaptive planning.\n    - Reinforcement learning training for Reporter module: The authors demonstrate that the Reporter component can be trained from scratch using reinforcement learning to learn what information is most helpful to communicate to the Planner, rather than relying solely on hand-coded or pre-trained perception modules.\n\nPaper 47:\n  Title: Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents\n  Abstract: Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.\n  Structured Summary:\n    - Limitations of using LLMs directly as embodied agents: The paper identifies that while LLMs can generate abstract plans, directly using them as agents faces challenges including limited context windows for long observations, inefficiency in fine-tuning, pre-training biases, and inability to handle non-textual modalities in embodied environments.\n    - Leveraging LLM knowledge without replacing trainable actors: The authors are motivated to exploit procedural and common-sense knowledge encoded in pre-trained LLMs while preserving the trainability and adaptability of low-level control policies, rather than forcing LLMs to act as the agent itself.\n    - Plan, Eliminate, and Track (PET) framework: PET is a three-module framework where Plan uses an LLM to decompose tasks into sub-tasks via in-context prompting, Eliminate uses zero-shot QA to filter irrelevant objects and receptacles from observations, and Track uses zero-shot QA to detect sub-task completion and advance progress.\n    - Action Attention agent architecture: A transformer-based agent that handles dynamic action spaces by computing attention-like scores over permissible actions, using separate query and key heads to score each action based on task embedding, observation history, and current state.\n\nPaper 48:\n  Title: Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents\n  Abstract: We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose \"$\\underline{D}$escribe, $\\underline{E}$xplain, $\\underline{P}$lan and $\\underline{S}$elect\" ($\\textbf{DEPS}$), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated $\\textit{plan}$ by integrating $\\textit{description}$ of the plan execution process and providing self-$\\textit{explanation}$ of feedback when encountering failures during the extended planning phases. Furthermore, it includes a goal $\\textit{selector}$, which is a trainable module that ranks parallel candidate sub-goals based on the estimated steps of completion, consequently refining the initial plan. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method's general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the $\\texttt{ObtainDiamond}$ grand challenge with our approach. The code is released at this https URL.\n  Structured Summary:\n    - Task planning for multi-task agents in open-world environments: The paper addresses the challenge of enabling multi-task embodied agents to plan and execute tasks in open-world environments like Minecraft. Two core difficulties are identified: the need for accurate multi-step reasoning over long horizons, and the inefficiency or infeasibility of plans that ignore the agent's current state when selecting among parallel sub-goals.\n    - Achieving robust multi-task agents in complex open-world domains: The authors are motivated by the goal of building generally capable AI systems. They view the ability to handle diverse tasks in complex, open-ended environments as a critical milestone, and existing methods fail to transfer successfully to such domains.\n    - Describe, Explain, Plan and Select (DEPS) interactive planning framework: DEPS is an interactive planning method that uses LLMs to iteratively refine plans. It incorporates a descriptor that summarizes execution outcomes, an explainer that identifies errors in previous plans via self-explanation, and a planner that generates updated plans based on this feedback, enabling robust error correction during task execution.\n    - Horizon-predictive goal selector: The goal selector is a learned neural network that predicts the remaining time steps (horizon) to complete each candidate sub-goal from the current state. It ranks parallel sub-goals by their proximity and feasibility, selecting the most efficient goal to execute next, thereby improving plan efficiency.\n\nPaper 49:\n  Title: Dialogue Shaping: Empowering Agents through NPC Interaction\n  Abstract: One major challenge in reinforcement learning (RL) is the large amount of steps for the RL agent needs to converge in the training process and learn the optimal policy, especially in text-based game environments where the action space is extensive. However, non-player characters (NPCs) sometimes hold some key information about the game, which can potentially help to train RL agents faster. Thus, this paper explores how to interact and converse with NPC agents to get the key information using large language models (LLMs), as well as incorporate this information to speed up RL agent's training using knowledge graphs (KGs) and Story Shaping.\n  Structured Summary:\n    - Slow convergence of RL agents in text-based games with extensive action spaces: The paper addresses the challenge that reinforcement learning agents require extensive trial-and-error steps to converge to optimal policies in text-based games, where action spaces are large and exploration is inefficient without prior knowledge.\n    - Leveraging NPC knowledge to reduce RL agent's policy search space: The authors are motivated by the observation that non-player characters hold key game information that can help RL agents avoid wasteful exploration. By extracting and using this information, agents can focus on relevant subgoals rather than exhaustive trial-and-error.\n    - Dialogue Shaping framework combining LLMs, knowledge graphs, and Story Shaping: The authors propose Dialogue Shaping, a framework where large language models converse with NPCs to extract game information, convert it into knowledge graphs, and then use Story Shaping to provide additional reward signals that guide the RL agent toward optimal policies more quickly.\n    - Using ChatGPT as dialogue module for NPC interaction: The method employs large language models (specifically ChatGPT) as a dialogue module that interacts with NPCs through prompting, retrieving crucial game information that would otherwise require extensive exploration by the RL agent.\n\nPaper 50:\n  Title: Large Language Models can Implement Policy Iteration\n  Abstract: This work presents In-Context Policy Iteration, an algorithm for performing Reinforcement Learning (RL), in-context, using foundation models. While the application of foundation models to RL has received considerable attention, most approaches rely on either (1) the curation of expert demonstrations (either through manual design or task-specific pretraining) or (2) adaptation to the task of interest using gradient methods (either fine-tuning or training of adapter layers). Both of these techniques have drawbacks. Collecting demonstrations is labor-intensive, and algorithms that rely on them do not outperform the experts from which the demonstrations were derived. All gradient techniques are inherently slow, sacrificing the \"few-shot\" quality that made in-context learning attractive to begin with. In this work, we present an algorithm, ICPI, that learns to perform RL tasks without expert demonstrations or gradients. Instead we present a policy-iteration method in which the prompt content is the entire locus of learning. ICPI iteratively updates the contents of the prompt from which it derives its policy through trial-and-error interaction with an RL environment. In order to eliminate the role of in-weights learning (on which approaches like Decision Transformer rely heavily), we demonstrate our algorithm using Codex, a language model with no prior knowledge of the domains on which we evaluate it.\n  Structured Summary:\n    - Limitations of existing foundation model approaches to RL: Existing approaches to applying foundation models to reinforcement learning suffer from two main limitations: they require labor-intensive expert demonstrations that limit performance to expert level, or they depend on gradient-based training methods that sacrifice the few-shot quality of in-context learning.\n    - Enabling RL without demonstrations or gradients: The authors aim to develop a method that can perform reinforcement learning using only in-context learning capabilities of large language models, eliminating the need for both expert demonstrations and gradient-based parameter updates while still enabling policy improvement.\n    - In-Context Policy Iteration (ICPI): ICPI implements model-based policy iteration where the LLM serves as both world model and rollout policy. The method computes Q-values by generating rollout trajectories using the LLM, then selects actions greedily with respect to these Q-values. Policy improvement occurs by storing improved behavior in a replay buffer and using it to condition future prompts.\n    - LLM-based Q-value computation via rollouts: Q-values are estimated by using the LLM to generate complete rollout trajectories from a given state-action pair. The LLM predicts rewards, terminations, next states, and actions by being prompted with relevant transitions from the agent's experience buffer, with careful selection and balancing of prompt content to maximize relevance.\n\nPaper 51:\n  Title: Large Language Models Are Semi-Parametric Reinforcement Learning Agents\n  Abstract: Inspired by the insights in cognitive science with respect to human memory and reasoning mechanism, a novel evolvable LLM-based (Large Language Model) agent framework is proposed as REMEMBERER. By equipping the LLM with a long-term experience memory, REMEMBERER is capable of exploiting the experiences from the past episodes even for different task goals, which excels an LLM-based agent with fixed exemplars or equipped with a transient working memory. We further introduce Reinforcement Learning with Experience Memory (RLEM) to update the memory. Thus, the whole system can learn from the experiences of both success and failure, and evolve its capability without fine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER constitutes a semi-parametric RL agent. Extensive experiments are conducted on two RL task sets to evaluate the proposed framework. The average results with different initialization and training sets exceed the prior SOTA by 4% and 2% for the success rate on two task sets and demonstrate the superiority and robustness of REMEMBERER.\n  Structured Summary:\n    - LLMs struggle to effectively learn from interaction experiences: Current methods for LLM-based agents face challenges in utilizing past interaction experiences. Fine-tuning LLMs is expensive, and in-context approaches are limited by input length constraints, preventing effective learning from extensive interaction histories.\n    - Episodic memory is crucial for human decision-making: Inspired by cognitive science insights about human memory and reasoning, the authors aim to enable agents to learn from both successful and failed experiences across different task goals, similar to how humans use episodic memory for decision-making.\n    - REMEMBERER agent framework with long-term experience memory: REMEMBERER is an agent framework that augments an LLM with a persistent external experience memory storing task information, observations, actions, and Q-value estimations. This memory enables the agent to exploit experiences from past episodes even for different task goals, unlike short-term working memory approaches.\n    - Reinforcement Learning with Experience Memory (RLEM): RLEM is a learning mechanism that updates the experience memory through an RL process rather than fine-tuning LLM parameters. It uses Q-learning with n-step bootstrapping to update Q-value estimations in the memory, enabling the system to learn from both successful and failed experiences.\n\nPaper 52:\n  Title: Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization\n  Abstract: Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment. This demonstrates that using policy gradient optimization to improve language agents, for which we believe our work is one of the first, seems promising and can be applied to optimize other models in the agent architecture to enhance agent performances over time.\n  Structured Summary:\n    - Lack of gradient-based optimization in language agents: Existing large language model agents cannot effectively learn from environment rewards through gradient-based optimization. While some agents use verbal feedback for iterative refinement, they lack mechanisms compatible with differentiable learning from rewards, limiting their ability to improve through reinforcement learning techniques.\n    - Need for principled reward-based learning in language agents: The authors are motivated by the difficulty of generating useful self-reflections from frozen LLMs, which struggle with credit assignment problems. Without proper fine-tuning for specific environments, verbal reinforcement cannot be optimal, necessitating a framework that enables gradient-based learning from environment rewards.\n    - Retroformer framework with retrospective model learning: Retroformer is a framework that reinforces language agents by learning a plug-in retrospective model. This model automatically refines agent prompts using environment feedback through policy gradient optimization, enabling learning from arbitrary reward signals across multiple environments and tasks without accessing the actor LLM parameters.\n    - Retrospective agent architecture with actor and retrospective LLMs: The architecture comprises two components: a frozen actor LLM that generates actions, and a fine-tunable retrospective LLM that produces self-reflections. The retrospective model refines actor prompts by diagnosing failure causes and devising improvement plans, with memory modules for short-term trajectory history and long-term reflection responses.\n\nPaper 53:\n  Title: REX: Rapid Exploration and eXploitation for AI Agents\n  Abstract: In this paper, we propose an enhanced approach for Rapid Exploration and eXploitation for AI Agents called REX. Existing AutoGPT-style techniques have inherent limitations, such as a heavy reliance on precise descriptions for decision-making, and the lack of a systematic approach to leverage try-and-fail procedures akin to traditional Reinforcement Learning (RL). REX introduces an additional layer of rewards and integrates concepts similar to Upper Confidence Bound (UCB) scores, leading to more robust and efficient AI agent performance. This approach has the advantage of enabling the utilization of offline behaviors from logs and allowing seamless integration with existing foundation models while it does not require any model fine-tuning. Through comparative analysis with existing methods such as Chain-of-Thoughts(CoT) and Reasoning viA Planning(RAP), REX-based methods demonstrate comparable performance and, in certain cases, even surpass the results achieved by these existing techniques. Notably, REX-based methods exhibit remarkable reductions in execution time, enhancing their practical applicability across a diverse set of scenarios.\n  Structured Summary:\n    - Limitations of LLM-based AI agents in exploration and exploitation: Current LLM-based AI agents struggle with systematically incorporating reward signals, balancing exploration versus exploitation, and suffer from accuracy and latency issues when solving complex multi-step problems. They lack a principled framework for learning from environmental feedback without expensive model fine-tuning.\n    - Need for efficient reward integration without fine-tuning: The authors are motivated by the impracticality of fine-tuning large language models for each new environment or task, which is costly and time-consuming. They aim to develop a method that leverages in-context learning to integrate rewards and improve decision-making without requiring parameter updates.\n    - REX framework with UCB-based reward integration: REX is a novel approach that compresses MCTS steps by having the LLM predict entire solution sequences at once, then backpropagates rewards to intermediate steps. It uses UCB scores to guide action selection, mapping them to HIGH or LOW tokens in prompts to systematically balance exploration and exploitation.\n    - REX-UCL: UCB applied to LLM logits: REX-UCL is a variant that directly modifies the logit values of tokens corresponding to actions using UCL scores derived from UCB. This ensures the LLM consistently selects intended actions by adjusting token probabilities, offering finer control over action generation.\n\nPaper 54:\n  Title: Let's Verify Step by Step\n  Abstract: In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.\n  Structured Summary:\n    - Logical mistakes in multi-step reasoning by large language models: Despite improvements in complex multi-step reasoning, current large language models frequently make logical errors and produce hallucinations, which is particularly problematic in domains requiring multi-step reasoning where a single error can derail an entire solution.\n    - Need for reliable reward models to detect and mitigate hallucinations: Training more reliable models requires effective reward models that can discriminate between desirable and undesirable outputs. Given the high cost of human feedback and the importance of reliability, it is critical to carefully compare different supervision methods for training these reward models.\n    - Process-supervised reward models (PRMs) with step-level human feedback: PRMs are trained using human feedback labels (positive, negative, or neutral) for each intermediate reasoning step in model-generated solutions. The PRM predicts step-level correctness and scores complete solutions as the product of correctness probabilities across all steps.\n    - Active learning strategy for data collection: An active learning approach that strategically selects which solutions to show human labelers by surfacing 'convincing wrong-answer solutions' - those rated highly by the current best PRM but reaching incorrect final answers - to maximize the value of limited human feedback.\n\nPaper 55:\n  Title: Chain of Hindsight Aligns Language Models with Feedback\n  Abstract: Learning from human preferences is important for language models to match human needs and to align with human and social values. Prior works have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers from imperfect reward functions and relies on extremely challenging optimizations. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sequences of sentences, which are then used to fine-tune the model, allowing us to take advantage of the language comprehension capabilities of language models. We condition the model on a sequence of model generations paired with feedback. By doing so, the model is trained to generate outputs based on feedback, while learning to identify and correct negative attributes or errors. Applying our method to large language models, we observed that Chain of Hindsight significantly surpasses previous methods in aligning language models with human preferences. We report significant improvements on summarization and dialogue benchmarks, with our approach markedly preferred in human evaluations.\n  Structured Summary:\n    - Limitations of existing human feedback learning methods: Existing approaches for learning from human feedback suffer from two main issues: supervised finetuning relies on positively-rated data only, making it data-inefficient and unable to learn from errors, while RLHF requires learning potentially misaligned reward functions and involves difficult optimization procedures.\n    - Aligning language models with human values and preferences: The authors are motivated by the need to ensure language models align with human values and societal norms. Human feedback is essential for evaluating model performance objectively and subjectively, identifying issues with accuracy, fairness, and bias, and ensuring ethical outputs.\n    - Chain of Hindsight (CoH): CoH converts all human feedback (positive and negative) into natural language sequences paired with model outputs. The model is finetuned to predict outputs conditioned on a sequence of model generations and their corresponding feedback, enabling it to learn from both positive and negative examples without reinforcement learning.\n    - Feedback-conditioned autoregressive training: The training procedure conditions the language model on sequences combining multiple model outputs with templated or natural language feedback based on human ratings. During training, the model predicts tokens autoregressively while being conditioned on feedback like 'Bad' and 'Good', learning to generate outputs aligned with positive feedback.\n\nPaper 56:\n  Title: Introspective Tips: Large Language Model for In-Context Decision Making\n  Abstract: The emergence of large language models (LLMs) has substantially influenced natural language processing, demonstrating exceptional results across various tasks. In this study, we employ ``Introspective Tips\" to facilitate LLMs in self-optimizing their decision-making. By introspectively examining trajectories, LLM refines its policy by generating succinct and valuable tips. Our method enhances the agent's performance in both few-shot and zero-shot learning situations by considering three essential scenarios: learning from the agent's past experiences, integrating expert demonstrations, and generalizing across diverse games. Importantly, we accomplish these improvements without fine-tuning the LLM parameters; rather, we adjust the prompt to generalize insights from the three aforementioned situations. Our framework not only supports but also emphasizes the advantage of employing LLM in in-contxt decision-making. Experiments involving over 100 games in TextWorld illustrate the superior performance of our approach.\n  Structured Summary:\n    - LLM errors and limited generalization in domain-specific decision-making: The paper addresses the challenge that large language models exhibit errors and hallucinations in domain-specific decision-making scenarios. Existing self-correction methods like reflections focus on individual agent mistakes and lack generalization across different agents and contexts.\n    - Enabling LLM self-optimization through human-like introspection: The authors are motivated by humans' natural ability to reflect on experiences, distill key insights, and extract lessons applicable to new situations. They aim to empower LLMs with similar self-optimizing capabilities that can generalize across different agents and contexts.\n    - Introspective Tips paradigm for self-optimizing decision-making: The method introduces Introspective Tips as compact, high-level guidance extracted from trajectories. Unlike reflections that focus on specific failures, tips provide succinct practical advice that can be shared among different agents and generalized across contexts without requiring LLM fine-tuning.\n    - Multiple learning strategies for different scenarios: The framework includes three strategies: learning from the agent's own history through replay and tip generation, learning from expert demonstrations by comparing successful and failed actions, and learning from multiple games to generate general tips that work across diverse settings.\n\nPaper 57:\n  Title: Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification\n  Abstract: Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has brought significant advancements in addressing math reasoning problems. In particular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter, shows remarkable performance on challenging math datasets. In this paper, we explore the effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the \\textit{Code Usage Frequency} of GPT-4 Code Interpreter. We found that its success can be largely attributed to its powerful skills in generating and executing code, evaluating the output of code execution, and rectifying its solution when receiving unreasonable outputs. Based on this insight, we propose a novel and effective prompting method, explicit \\uline{c}ode-based \\uline{s}elf-\\uline{v}erification~(CSV), to further boost the mathematical reasoning potential of GPT-4 Code Interpreter. This method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to use code to self-verify its answers. In instances where the verification state registers as ``False'', the model shall automatically amend its solution, analogous to our approach of rectifying errors during a mathematics examination. Furthermore, we recognize that the states of the verification result indicate the confidence of a solution, which can improve the effectiveness of majority voting. With GPT-4 Code Interpreter and CSV, we achieve an impressive zero-shot accuracy on MATH dataset \\textbf{(53.9\\% $\\to$ 84.3\\%)}.\n  Structured Summary:\n    - Limited mathematical reasoning capability of LLMs despite code generation: The paper addresses the challenge that large language models, despite advances in code generation, continue to struggle with mathematical reasoning tasks, producing errors and inaccuracies in complex calculations.\n    - Understanding and enhancing GPT-4 Code Interpreter's math problem-solving abilities: The authors are motivated by the lack of systematic analysis of GPT-4 Code Interpreter's strong performance on math problems and seek to understand its mechanisms to further improve its capabilities.\n    - Code Usage Frequency analysis with code-constrained prompts: The authors introduce Code Usage Frequency as a metric and design code-constrained prompts (no code, single code use, unrestricted code) to systematically analyze how code generation and execution frequency impacts GPT-4 Code Interpreter's mathematical reasoning performance.\n    - Explicit code-based self-verification (CSV) prompting: The authors propose CSV, a zero-shot prompting technique that guides GPT-4 Code Interpreter to generate verification code for its answers and automatically correct solutions when verification fails, leveraging the model's self-debugging capabilities without external verifiers.\n\n\nBefore returning, check that:\n- Every paper title appears exactly once.\n- Only leaf nodes have a \"papers\" field.\n- All intermediate nodes have a \"subtopics\" field.\n- The JSON is strictly valid and parsable.\n\n### Output:", "hierarchy_response": "{\n  \"name\": \"Large Language Model based Intelligent Agents\",\n  \"subtopics\": [\n    {\n      \"name\": \"Tool Augmentation and API Integration\",\n      \"subtopics\": [\n        {\n          \"name\": \"Tool Creation and Customization\",\n          \"papers\": [\n            \"CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets\",\n            \"Large Language Models as Tool Makers\"\n          ]\n        },\n        {\n          \"name\": \"API Integration and Execution\",\n          \"papers\": [\n            \"RestGPT: Connecting Large Language Models with Real-World RESTful APIs\",\n            \"TALM: Tool Augmented Language Models\",\n            \"Gorilla: Large Language Model Connected with Massive APIs\"\n          ]\n        },\n        {\n          \"name\": \"Multi-Modal Tool Integration\",\n          \"papers\": [\n            \"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face\",\n            \"ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models\",\n            \"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models\"\n          ]\n        },\n        {\n          \"name\": \"Tool-Use Frameworks and Benchmarks\",\n          \"papers\": [\n            \"Toolformer: Language Models Can Teach Themselves to Use Tools\",\n            \"ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs\",\n            \"TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Agent Architectures and Frameworks\",\n      \"subtopics\": [\n        {\n          \"name\": \"Multi-Agent Systems\",\n          \"papers\": [\n            \"Gentopia: A Collaborative Platform for Tool-Augmented LLMs\",\n            \"ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate\",\n            \"Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate\",\n            \"Building Cooperative Embodied Agents Modularly with Large Language Models\",\n            \"BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents\",\n            \"AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\",\n            \"AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\"\n          ]\n        },\n        {\n          \"name\": \"Cognitive Architectures\",\n          \"papers\": [\n            \"Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis\",\n            \"CGMI: Configurable General Multi-Agent Interaction Framework\",\n            \"SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks\"\n          ]\n        },\n        {\n          \"name\": \"Neuro-Symbolic Integration\",\n          \"papers\": [\n            \"MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning\",\n            \"LLM As DBA\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Planning and Reasoning\",\n      \"subtopics\": [\n        {\n          \"name\": \"Planning with World Models\",\n          \"papers\": [\n            \"Dynamic Planning with a LLM\",\n            \"Reasoning with Language Model is Planning with World Model\",\n            \"LLM+P: Empowering Large Language Models with Optimal Planning Proficiency\"\n          ]\n        },\n        {\n          \"name\": \"Task Decomposition and Planning\",\n          \"papers\": [\n            \"Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents\",\n            \"Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents\"\n          ]\n        },\n        {\n          \"name\": \"Context-Aware Planning\",\n          \"papers\": [\n            \"Context-Aware Composition of Agent Policies by Markov Decision Process Entity Embeddings and Agent Ensembles\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Prompting Strategies for Reasoning\",\n      \"subtopics\": [\n        {\n          \"name\": \"Chain-of-Thought Methods\",\n          \"papers\": [\n            \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\",\n            \"Large Language Models are Zero-Shot Reasoners\",\n            \"Chain-of-Thought in Large Language Models: Decoding, Projection, and Activation\"\n          ]\n        },\n        {\n          \"name\": \"Enhanced Chain-of-Thought Techniques\",\n          \"papers\": [\n            \"Self-Consistency Improves Chain of Thought Reasoning in Language Models\",\n            \"Complexity-Based Prompting for Multi-Step Reasoning\",\n            \"Automatic Chain of Thought Prompting in Large Language Models\",\n            \"Progressive-Hint Prompting Improves Reasoning in Large Language Models\"\n          ]\n        },\n        {\n          \"name\": \"Advanced Reasoning Structures\",\n          \"papers\": [\n            \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\",\n            \"Graph of Thoughts: Solving Elaborate Problems with Large Language Models\",\n            \"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Learning and Adaptation\",\n      \"subtopics\": [\n        {\n          \"name\": \"Self-Refinement and Reflection\",\n          \"papers\": [\n            \"Self-Refine: Iterative Refinement with Self-Feedback\",\n            \"Reflexion: Language Agents with Verbal Reinforcement Learning\",\n            \"Introspective Tips: Large Language Model for In-Context Decision Making\"\n          ]\n        },\n        {\n          \"name\": \"Reinforcement Learning Integration\",\n          \"papers\": [\n            \"Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling\",\n            \"Large Language Models can Implement Policy Iteration\",\n            \"Large Language Models Are Semi-Parametric Reinforcement Learning Agents\",\n            \"Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization\",\n            \"REX: Rapid Exploration and eXploitation for AI Agents\"\n          ]\n        },\n        {\n          \"name\": \"Learning from Feedback\",\n          \"papers\": [\n            \"Let's Verify Step by Step\",\n            \"Chain of Hindsight Aligns Language Models with Feedback\",\n            \"Making Large Language Models Better Reasoners with Alignment\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Embodied and Interactive Agents\",\n      \"subtopics\": [\n        {\n          \"name\": \"Embodied Task Execution\",\n          \"papers\": [\n            \"Collaborating with language models for embodied reasoning\"\n          ]\n        },\n        {\n          \"name\": \"Game-Playing Agents\",\n          \"papers\": [\n            \"AvalonBench: Evaluating LLMs Playing the Game of Avalon\",\n            \"Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf\",\n            \"Dialogue Shaping: Empowering Agents through NPC Interaction\"\n          ]\n        },\n        {\n          \"name\": \"Code-Based Problem Solving\",\n          \"papers\": [\n            \"Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification\"\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"Knowledge Representation\",\n      \"papers\": [\n        \"A Survey of Knowledge Graph Embedding and Their Applications\"\n      ]\n    },\n    {\n      \"name\": \"Action and Interaction Paradigms\",\n      \"papers\": [\n        \"ReAct: Synergizing Reasoning and Acting in Language Models\"\n      ]\n    }\n  ]\n}\n", "hierarchy_tree": {"name": "Large Language Model based Intelligent Agents", "subtopics": [{"name": "Tool Augmentation and API Integration", "subtopics": [{"name": "Tool Creation and Customization", "papers": ["CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets", "Large Language Models as Tool Makers"]}, {"name": "API Integration and Execution", "papers": ["RestGPT: Connecting Large Language Models with Real-World RESTful APIs", "TALM: Tool Augmented Language Models", "Gorilla: Large Language Model Connected with Massive APIs"]}, {"name": "Multi-Modal Tool Integration", "papers": ["HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face", "ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models", "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"]}, {"name": "Tool-Use Frameworks and Benchmarks", "papers": ["Toolformer: Language Models Can Teach Themselves to Use Tools", "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs", "TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage"]}]}, {"name": "Agent Architectures and Frameworks", "subtopics": [{"name": "Multi-Agent Systems", "papers": ["Gentopia: A Collaborative Platform for Tool-Augmented LLMs", "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate", "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate", "Building Cooperative Embodied Agents Modularly with Large Language Models", "BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents", "AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors", "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation"]}, {"name": "Cognitive Architectures", "papers": ["Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis", "CGMI: Configurable General Multi-Agent Interaction Framework", "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks"]}, {"name": "Neuro-Symbolic Integration", "papers": ["MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning", "LLM As DBA"]}]}, {"name": "Planning and Reasoning", "subtopics": [{"name": "Planning with World Models", "papers": ["Dynamic Planning with a LLM", "Reasoning with Language Model is Planning with World Model", "LLM+P: Empowering Large Language Models with Optimal Planning Proficiency"]}, {"name": "Task Decomposition and Planning", "papers": ["Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents", "Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents"]}, {"name": "Context-Aware Planning", "papers": ["Context-Aware Composition of Agent Policies by Markov Decision Process Entity Embeddings and Agent Ensembles"]}]}, {"name": "Prompting Strategies for Reasoning", "subtopics": [{"name": "Chain-of-Thought Methods", "papers": ["Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "Large Language Models are Zero-Shot Reasoners", "Chain-of-Thought in Large Language Models: Decoding, Projection, and Activation"]}, {"name": "Enhanced Chain-of-Thought Techniques", "papers": ["Self-Consistency Improves Chain of Thought Reasoning in Language Models", "Complexity-Based Prompting for Multi-Step Reasoning", "Automatic Chain of Thought Prompting in Large Language Models", "Progressive-Hint Prompting Improves Reasoning in Large Language Models"]}, {"name": "Advanced Reasoning Structures", "papers": ["Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "Graph of Thoughts: Solving Elaborate Problems with Large Language Models", "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"]}]}, {"name": "Learning and Adaptation", "subtopics": [{"name": "Self-Refinement and Reflection", "papers": ["Self-Refine: Iterative Refinement with Self-Feedback", "Reflexion: Language Agents with Verbal Reinforcement Learning", "Introspective Tips: Large Language Model for In-Context Decision Making"]}, {"name": "Reinforcement Learning Integration", "papers": ["Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling", "Large Language Models can Implement Policy Iteration", "Large Language Models Are Semi-Parametric Reinforcement Learning Agents", "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization", "REX: Rapid Exploration and eXploitation for AI Agents"]}, {"name": "Learning from Feedback", "papers": ["Let's Verify Step by Step", "Chain of Hindsight Aligns Language Models with Feedback", "Making Large Language Models Better Reasoners with Alignment"]}]}, {"name": "Embodied and Interactive Agents", "subtopics": [{"name": "Embodied Task Execution", "papers": ["Collaborating with language models for embodied reasoning"]}, {"name": "Game-Playing Agents", "papers": ["AvalonBench: Evaluating LLMs Playing the Game of Avalon", "Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf", "Dialogue Shaping: Empowering Agents through NPC Interaction"]}, {"name": "Code-Based Problem Solving", "papers": ["Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification"]}]}, {"name": "Knowledge Representation", "papers": ["A Survey of Knowledge Graph Embedding and Their Applications"]}, {"name": "Action and Interaction Paradigms", "papers": ["ReAct: Synergizing Reasoning and Acting in Language Models"]}]}}
