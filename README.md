# TaxoBench: Hierarchical Taxonomy Generation and Evaluation Benchmark for Scientific Literature

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Paper](https://img.shields.io/badge/Paper-Arxiv-red.svg)](https://arxiv.org/abs/2601.xxxxx)

---

## ğŸ“– Project Overview

**TaxoBench** is the first benchmark framework that systematically evaluates **Deep Research Agents** and **Large Language Models (LLMs)** on their ability to **organize, summarize, and construct hierarchical knowledge structures** from scientific literature, from the perspective of **human expert cognitive structures**.

This project is based on the paper from Fudan University NLP Lab:

> *Can Deep Research Agents Find and Organize?  
> Evaluating the Synthesis Gap with Expert Taxonomies*

### ğŸ“š Data Sources

- **72 highly-cited computer science survey papers** (Survey Topics)
- **Expert-constructed Taxonomy Trees**
- **3,815 precisely classified cited papers** as Ground Truth

### ğŸ¯ Evaluation Modes

TaxoBench implements the two core evaluation paradigms defined in the paper:

1. **Deep Research Mode**  
   End-to-end evaluation: Retrieval â†’ Filtering â†’ Organization â†’ Structured Summarization

2. **Bottom-Up Mode** (focus of this repository)  
   Given a fixed collection of papers, evaluate the model's ability to **build hierarchical knowledge structures (taxonomies) bottom-up**

---

## ğŸŒŸ Key Features

- **ğŸ§ª Two-level Evaluation Architecture**
  - **Leaf-Level**: Retrieval & clustering quality
  - **Hierarchy-Level**: Reasonableness of the classification tree structure

- **âš¡ High-throughput concurrent inference**
  - Based on Python `multiprocessing`
  - Supports large-scale parallelism

- **ğŸ§  Native support for Thinking / Reasoning modes**
  - Well adapted to reasoning-enhanced models:
    - DeepSeek-R1 / V3
    - Claude 4.5 Sonnet
    - Kimi-k2-Thinking, etc.

- **ğŸ”Œ Unified interface for multiple models**
  - OpenAI (GPT-5)
  - Anthropic (Claude 4.5)
  - Google (Gemini 3)
  - DeepSeek / Qwen / Moonshot (Kimi)

---

## ğŸ“‚ Repository Structure

```text
TaxoBench/
â”œâ”€â”€ dataset/                  # Input data (72 Survey Topics + 3815 papers)
â”œâ”€â”€ script/                   # Experiment launch scripts (Bottom-Up Mode)
â”‚   â”œâ”€â”€ eval_setting1.sh      # Setting 1: Title + Abstract
â”‚   â”œâ”€â”€ eval_setting2.sh      # Setting 2: Title + Abstract + Summary
â”‚   â””â”€â”€ eval_setting3.sh      # Setting 3: Title + Abstract + Core-task & Contributions
â”œâ”€â”€ setting_pipeline/         # Core inference logic (Python)
â”‚   â”œâ”€â”€ eval_setting1.py
â”‚   â”œâ”€â”€ eval_setting2.py
â”‚   â””â”€â”€ eval_setting3.py
â”œâ”€â”€ metric/                   # Evaluation metrics
â”‚   â”œâ”€â”€ get_clustering_metric.py  # Leaf-Level Metrics
â”‚   â”œâ”€â”€ get_taxonomy_metric.py    # Hierarchy-Level (LLM-as-a-Judge)
â”‚   â”œâ”€â”€ ted.py                    # Tree Edit Distance
â”‚   â””â”€â”€ soft_f1.py                # Soft F1 (NSR / NSP)
â””â”€â”€ results/                  # Experiment results output
```
## ğŸ§ª Evaluation Settings

This repository focuses on the **Bottom-Up Mode** described in the paper, examining model organization capability across three progressively richer input granularities.

### ğŸ”¹ Setting 1: Basic Metadata

- **Input**: Title + Abstract
- **Launch command**:
  ```bash
  bash script/eval_setting1.sh
  ```
  **Description**:Most basic setting â€” only surface semantic information, tests preliminary organization ability.
### ğŸ”¹ Setting 2: Enhanced Semantic Context

- **Input**: Title + Abstract + Summary
- **Launch command**:
  ```bash
  bash script/eval_setting2.sh
  ```
  **Description**:Summary generated by LLM (containing research problem, motivation, method, etc.). Tests whether richer semantics improves classification quality.

### Setting 3: Core Elements (Recommended)

- **Input**: Title + Abstract + Core-task & Contributions
- **Launch command**:
  ```bash
  bash script/eval_setting3.sh
  ```
  **Description**:
  ```
  Uses expert-extracted core tasks and contributions
  Removes redundant descriptions, focuses on innovative essence
  Supports Thinking mode & self-correction
â†’ Closest to how human experts cognitively organize knowledge
```
## ğŸš€ Quick Start

### Clone & Install Dependencies

```bash
git clone [https://github.com/KongLongGeFDU/TaxoBench.git](https://github.com/KongLongGeFDU/TaxoBench.git)
cd TaxoBench
pip install openai anthropic tqdm numpy pandas scikit-learn
```
### Configure API Keys
Edit the Python scripts under `setting_pipeline/`:

```python
from openai import OpenAI

client = OpenAI(
    base_url="[https://api.openai.com/v1](https://api.openai.com/v1)",
    api_key="sk-..."
)
```
### Run Experiments
Modify these fields in `script/eval_setting*.sh`:
* `MODEL_PAIRS`: list of models
* `NUM_WORKERS`: number of parallel processes

Then execute:

```bash
chmod +x script/eval_setting3.sh
./script/eval_setting3.sh
```
## ğŸ“Š Evaluation Metrics
Complete evaluation tools from the paper are provided in the `metric/` directory.

### ğŸ§© Leaf-Level Metrics (Paper / Clustering Level)

| Metric | Description | Script |
| :--- | :--- | :--- |
| **Recall** | (Deep Research Mode only) [cite_start]Coverage of expert-selected papers [cite: 172-174] | `get_recall.py` |
| **ARI** | [cite_start]Adjusted Rand Index â€” agreement between clustering & ground truth [cite: 179-181] | `get_clustering_metric.py` |
| **V-Measure** | [cite_start]Weighted average of Homogeneity and Completeness [cite: 184-194] | `get_clustering_metric.py` |

### ğŸŒ³ Hierarchy-Level Metrics (Taxonomy Tree Structure)

| Metric | Description | Script |
| :--- | :--- | :--- |
| **TED** | [cite_start]Tree Edit Distance â€” minimum cost to transform model tree to expert tree [cite: 200-205] | `ted.py` |
| **Soft F1** | [cite_start]Soft F1 based on NSR (Node Soft Recall) and NSP (Node Soft Precision) [cite: 208-215] | `soft_f1.py` |
| **LLM-as-a-Judge** | [cite_start]GPT-4o scoring on 4 dimensions: Semantic Coverage, Sibling Organization, Hierarchical Logic, Structural Topology [cite: 217-221] | `get_taxonomy_metric.py` |

## ğŸ“ Citation
If you use this code or dataset in your research, please cite our paper:

```bibtex
loading~
```
## ğŸ“„ License
This project is licensed under the MIT License. See the LICENSE file for details.

